{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CalMS21 State-Annotation Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0\n",
    "os.environ[\"JAX_CAPTURED_CONSTANTS_REPORT_FRAMES\"]=\"-1\"\n",
    "from pathlib import Path\n",
    "import jax \n",
    "jax.config.update(\"jax_compilation_cache_dir\", (Path.cwd() / \"tmp/jax_cache\").as_posix())\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)\n",
    "try: \n",
    "    jax.config.update(\"jax_persistent_cache_enable_xla_caches\", \"xla_gpu_per_fusion_autotune_cache_dir\")\n",
    "except AttributeError:\n",
    "    pass  # Skip if not available in this JAX version\n",
    "\n",
    "try:\n",
    "    import blackjax\n",
    "except ModuleNotFoundError:\n",
    "    print('installing blackjax')\n",
    "    %pip install -qq blackjax\n",
    "    import blackjax\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "# from fastprogress.fastprogress import progress_bar\n",
    "from functools import partial\n",
    "\n",
    "# jax.config.update('jax_platform_name', 'cpu')\n",
    "from jax import random as  jr\n",
    "from jax import numpy as jnp\n",
    "from jax import jit, vmap\n",
    "from itertools import count\n",
    "from flax import nnx\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# device = 'gpu' if jax.lib.xla_bridge.get_backend().platform == 'gpu' else 'cpu'\n",
    "device = 'gpu' if jax.extend.backend.get_backend().platform == 'gpu' else 'cpu'\n",
    "n_gpus = jax.device_count(backend=device)\n",
    "print(f\"Using {n_gpus} device(s) on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add TiDHy to path if needed\n",
    "# sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import TiDHy utilities\n",
    "from TiDHy.utils.state_annotation_comparison import (\n",
    "    match_states_to_annotations,\n",
    "    compute_clustering_metrics,\n",
    "    compute_state_purity,\n",
    "    compute_per_behavior_metrics,\n",
    "    plot_confusion_matrix,\n",
    "    analyze_state_annotation_correspondence\n",
    ")\n",
    "from TiDHy.utils.slds_analysis import load_slds_results\n",
    "from TiDHy.models.TiDHy_nnx_vmap import TiDHy\n",
    "from TiDHy.models.TiDHy_nnx_vmap_training import train_model, evaluate_record, load_model, get_latest_checkpoint_epoch, list_checkpoints\n",
    "from TiDHy.datasets.datasets_dynamax import *\n",
    "from TiDHy.datasets.load_data import load_data, stack_data\n",
    "from TiDHy.utils import io_dict_to_hdf5 as ioh5\n",
    "from TiDHy.utils.path_utils import *\n",
    "\n",
    "# Plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##### Plotting settings ######\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size':          10,\n",
    "                     'axes.linewidth':     2,\n",
    "                     'xtick.major.size':   5,\n",
    "                     'ytick.major.size':   5,\n",
    "                     'xtick.major.width':  2,\n",
    "                     'ytick.major.width':  2,\n",
    "                     'axes.spines.right':  False,\n",
    "                     'axes.spines.top':    False,\n",
    "                     'pdf.fonttype':       42,\n",
    "                     'ps.fonttype':        42,\n",
    "                     'xtick.labelsize':    10,\n",
    "                     'ytick.labelsize':    10,\n",
    "                     'figure.facecolor':   'white',\n",
    "                     'pdf.use14corefonts': False,  # Changed to False - we're embedding TrueType fonts\n",
    "                     'font.family':        'sans-serif',\n",
    "                     'font.sans-serif':    'Arial',  # Uncommented - Arial is now available\n",
    "                     'axes.unicode_minus': True,  # Ensures proper minus sign rendering in PDFs\n",
    "                    })\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "clrs = ['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000']\n",
    "cmap = ListedColormap(clrs)\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def map_discrete_cbar(cmap,N):\n",
    "    cmap = plt.get_cmap(cmap,N+1)\n",
    "    bounds = np.arange(-.5,N+1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    return cmap, norm\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load CalMS21 Data with Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CalMS21'\n",
    "# version = 'HierarchicalMultiTimescale'\n",
    "version = 'TiDHy'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 0\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd() / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg.dataset.name}: {cfg.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")\n",
    "\n",
    "\n",
    "data_dict = load_data(cfg)\n",
    "inputs_test = data_dict['inputs_test'][None]\n",
    "max_seq_len = inputs_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNG\n",
    "rngs = nnx.Rngs(42)\n",
    "\n",
    "# Get model params as dict and unpack directly\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "# model_params.pop('batch_converge')\n",
    "model_params['input_dim'] = inputs_test.shape[-1]\n",
    "\n",
    "model = TiDHy(**model_params, rngs=rngs)\n",
    "# model.l0 = nnx.data(jnp.zeros(3))\n",
    "# model.loss_weights = nnx.data(jnp.ones(3))\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"input_dim: {model.input_dim}, r_dim: {model.r_dim}, r2_dim: {model.r2_dim}, mix_dim: {model.mix_dim}\")\n",
    "jit_model = jax.jit(model)\n",
    "# out = jit_model(inputs_train)\n",
    "epoch=get_latest_checkpoint_epoch(cfg.paths.ckpt_dir)\n",
    "loaded_model = load_model(model,cfg.paths.ckpt_dir/f'epoch_{epoch:04d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = ioh5.load(cfg.paths.log_dir/'evaluation_results.h5')\n",
    "seq_len = natsorted(list(result_dict.keys()))\n",
    "# W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "# I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "# Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "# Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "# R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "# R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "# R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])\n",
    "# Ut = jnp.stack([result_dict[str(seq)]['Ut'].reshape((-1,)+ result_dict[str(seq)]['Ut'].shape[2:]) for seq in seq_len])\n",
    "ts=-1\n",
    "W = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "Ut = result_dict['{}'.format(seq_len[ts])]['Ut'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-2],result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-1])\n",
    "annotations = data_dict['annotations_test'][:R2_hat.shape[0]]\n",
    "W.shape, R2_hat.shape, R_hat.shape, Ut.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "ts = -1\n",
    "exp_path = cfg.paths.save_dir\n",
    "run_id = next(part.split('=')[1] for part in exp_path.parts if part.startswith('run_id='))\n",
    "results_path_list = natsorted(list((cfg.paths.base_dir / f'run_id={run_id}').rglob(f'evaluation_results.h5')))\n",
    "best_pred_all = []\n",
    "for n, results_path in enumerate(results_path_list):\n",
    "\tprint(n, results_path)\n",
    "\tresult_dict = ioh5.load(results_path)\n",
    "\tseq_len = natsorted(list(result_dict.keys()))\n",
    "\tW = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "\tI = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\tIhat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "\tR_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\tR_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "\tR2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "\tUt = result_dict['{}'.format(seq_len[ts])]['Ut'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-2],result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-1])\n",
    "\tannotations = data_dict['annotations_test'][:R2_hat.shape[0]]\n",
    "\treg_variables = [np.concatenate([R2_hat,W,R_hat],axis=-1),R2_hat,W,R_hat,R_bar]\n",
    "\tlabels = ['all','R2_hat','W','R_hat','R_bar']\n",
    "\n",
    "\tmax_acc = 0\n",
    "\ttr_batch_size = 40\n",
    "\tbatch_size = 10\n",
    "\tfor k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "\t\tX_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), annotations.reshape(-1), test_size=0.25, random_state=42)\n",
    "\t\tneigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "\t\t# neigh = RidgeClassifierCV()\n",
    "\t\tneigh.fit(X_train, y_train)\n",
    "\t\tscores = neigh.score(X_test, y_test)\n",
    "\t\tprint(labels[k],scores)\n",
    "\t\tif scores > max_acc:\n",
    "\t\t\ty_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "\t\t\tmax_acc = scores\n",
    "\t\t\tbest_reg_vars = reg_vars\n",
    "\t\t\tbest_label = labels[k]\n",
    "\t\t\tbest_pred = y_pred\n",
    "\tbest_pred_all.append(best_pred)\n",
    "states_tidhy = best_pred_all\n",
    "behavior_names = list(data_dict['vocabulary'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SLDS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CalMS21'\n",
    "# version = 'HierarchicalMultiTimescale'\n",
    "version = 'SLDS'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 3\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg_ssm = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd().parent / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg_ssm.dataset.name}: {cfg_ssm.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg_ssm.paths = convert_dict_to_path(cfg_ssm.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natsorted(list((cfg_ssm.paths.base_dir / f'run_id={run_id}').rglob(f'ssm_slds_*.h5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_ssm.paths.save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = next(part.split('=')[1] for part in cfg_ssm.paths.save_dir.parts if part.startswith('run_id='))\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rslds_dict = ioh5.load(list(cfg_ssm.paths.log_dir.glob('ssm_rslds_*.h5'))[0])\n",
    "run_id = next(part.split('=')[1] for part in cfg_ssm.paths.save_dir.parts if part.startswith('run_id='))\n",
    "\n",
    "results_path_list = natsorted(list((cfg_ssm.paths.base_dir / f'run_id={run_id}').rglob(f'ssm_slds_*.h5')))\n",
    "slds_states = []\n",
    "for n, results_path in enumerate(results_path_list):\n",
    "\tprint(n, results_path)\n",
    "\tslds_dict = ioh5.load(list(cfg_ssm.paths.log_dir.glob('ssm_slds_*.h5'))[0])\n",
    "\temission = slds_dict['SLDS_emission']\n",
    "\tlatents = slds_dict['SLDS_latents']\n",
    "\tslds_states.append(slds_dict['SLDS_states'])\n",
    "slds_states = jnp.stack(slds_states)\n",
    "annotations = data_dict['annotations_test']\n",
    "behavior_names = list(data_dict['vocabulary'].keys())\n",
    "slds_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when you have loaded data\n",
    "\n",
    "print(f\"States shape: {states.shape}\")\n",
    "print(f\"Annotations shape: {annotations.shape}\")\n",
    "print(f\"Number of unique states: {len(np.unique(states))}\")\n",
    "print(f\"Number of unique behaviors: {len(np.unique(annotations))}\")\n",
    "print(f\"\\nBehavior vocabulary: {behavior_names}\")\n",
    "print(f\"\\nState distribution:\")\n",
    "for s in np.unique(states):\n",
    "    count = np.sum(states == s)\n",
    "    print(f\"  State {s}: {count} timesteps ({count/len(states)*100:.1f}%)\")\n",
    "print(f\"\\nAnnotation distribution:\")\n",
    "for a in np.unique(annotations):\n",
    "    count = np.sum(annotations == a)\n",
    "    behavior = behavior_names[a] if a < len(behavior_names) else f'Unknown_{a}'\n",
    "    print(f\"  {behavior}: {count} timesteps ({count/len(annotations)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive State-Annotation Analysis\n",
    "\n",
    "Run the full analysis pipeline using the main analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive analysis\n",
    "# Uncomment when data is loaded\n",
    "matched_states, f1_tidhy = [], []\n",
    "for n in range(len(states)):\n",
    "\tprint(f\"\\nAnalyzing TiDHy run {n+1}/{len(states)}\")\n",
    "\tresults_tidhy = analyze_state_annotation_correspondence(\n",
    "\t\tstates=states[n],\n",
    "\t\tannotations=annotations,\n",
    "\t\tbehavior_names=behavior_names,\n",
    "\t\tverbose=True\n",
    "\t)\n",
    "\tmatched_states.append(results_tidhy['matching']['matched_states'])\n",
    "\tf1_tidhy.append(results_tidhy['per_behavior_metrics']['per_behavior_f1'])\n",
    "matched_states = jnp.stack(matched_states)\n",
    "f1_tidhy = jnp.stack(f1_tidhy)\n",
    "\n",
    "slds_matched_states, f1_slds = [], []\n",
    "for n in range(len(slds_states)):\n",
    "\tprint(f\"\\nAnalyzing SLDS run {n+1}/{len(slds_states)}\")\n",
    "\tresults_slds = analyze_state_annotation_correspondence(\n",
    "\t\tstates=slds_states[n],\n",
    "\t\tannotations=annotations,\n",
    "\t\tbehavior_names=behavior_names,\n",
    "\t\tverbose=True\n",
    "\t)\n",
    "\tslds_matched_states.append(results_slds['matching']['matched_states'])\n",
    "\tf1_slds.append(results_slds['per_behavior_metrics']['per_behavior_f1'])\n",
    "slds_matched_states = jnp.stack(slds_matched_states)\n",
    "f1_slds = jnp.stack(f1_slds)\n",
    "\n",
    "print(\"This will print a comprehensive analysis summary including:\")\n",
    "print(\"  - Data summary (timesteps, num states, num behaviors)\")\n",
    "print(\"  - Clustering quality metrics (ARI, NMI, V-measure, accuracy)\")\n",
    "print(\"  - Purity metrics (state purity, annotation purity)\")\n",
    "print(\"  - Per-behavior performance (precision, recall, F1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Confusion Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from TiDHy.utils.state_annotation_comparison import analyze_state_annotation_correspondence\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "cmap_b = ListedColormap(clrs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=13\n",
    "fig = plt.figure(constrained_layout=True, figsize=(8.75,5))\n",
    "gs  = gridspec.GridSpec(nrows=4, ncols=6,hspace=5,wspace=.1) \n",
    "gs0 = gridspec.GridSpecFromSubplotSpec(1, 6, subplot_spec=gs[:2,:], wspace=.1,hspace=.1)\n",
    "gs1 = gridspec.GridSpecFromSubplotSpec(1, 8, subplot_spec=gs[2:4,:], wspace=10,hspace=1.5)\n",
    "# gs2 = gridspec.GridSpecFromSubplotSpec(3, 2, subplot_spec=gs[4:,:], wspace=.1,hspace=.1)\n",
    "\n",
    "ax = fig.add_subplot(gs0[:, :3])\n",
    "fig1 = plot_confusion_matrix(\n",
    "    matched_states=matched_states[-1],\n",
    "    annotations=annotations,\n",
    "    behavior_names=behavior_names,\n",
    "    normalize='true',  # Row-normalize (shows recall)\n",
    "    model='TiDHy',\n",
    "    ax=ax\n",
    ")\n",
    "fig1.delaxes(fig1.axes[1])\n",
    "ax = fig.add_subplot(gs0[:, 3:])\n",
    "fig2 = plot_confusion_matrix(\n",
    "    matched_states=slds_matched_states[-1],\n",
    "    annotations=annotations,\n",
    "    behavior_names=behavior_names,\n",
    "    normalize='true',  # Row-normalize (shows recall)\n",
    "    model='SLDS',\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_ylabel('')\n",
    "ax.set_yticklabels([])\n",
    "ax = fig.add_subplot(gs1[:, :4])\n",
    "ax.bar(x=np.arange(len(results_tidhy['per_behavior_metrics']['per_behavior_f1'])), height=np.mean(f1_tidhy, axis=0),color='k')\n",
    "ax.errorbar(x=np.arange(len(results_tidhy['per_behavior_metrics']['per_behavior_f1'])), y=np.mean(f1_tidhy, axis=0), yerr=np.std(f1_tidhy, axis=0), capsize=5, color='gray', fmt='none')\n",
    "ax.set_xticks(np.arange(len(behavior_names)))\n",
    "ax.set_xticklabels(behavior_names,rotation=45,ha='right',fontsize=fontsize-2)\n",
    "ax.set_ylabel('F1 Score',fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(gs1[:, 4:8])\n",
    "ax.bar(x=np.arange(len(results_slds['per_behavior_metrics']['per_behavior_f1'])), height=np.mean(f1_slds, axis=0),color='k')\n",
    "ax.errorbar(x=np.arange(len(results_slds['per_behavior_metrics']['per_behavior_f1'])), y=np.mean(f1_slds, axis=0), yerr=np.std(f1_slds, axis=0), capsize=5, color='gray', fmt='none')\n",
    "ax.set_xticks(np.arange(len(behavior_names)))\n",
    "ax.set_xticklabels(behavior_names,rotation=45,ha='right',fontsize=fontsize-2)\n",
    "ax.set_ylabel('F1 Score',fontsize=fontsize-2)\n",
    "ax.set_ylim(0,1)\n",
    "fig.savefig(cfg.paths.fig_dir / f'state_annotation_analysis_{cfg.dataset.name}_{cfg.version}_vs_{cfg_ssm.version}.pdf', bbox_inches='tight', dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import cuml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = ioh5.load(cfg.paths.log_dir/'evaluation_results.h5')\n",
    "seq_len = natsorted(list(result_dict.keys()))\n",
    "W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])\n",
    "Ut = jnp.stack([result_dict[str(seq)]['Ut'].reshape((-1,)+ result_dict[str(seq)]['Ut'].shape[2:]) for seq in seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = -1\n",
    "t = 0\n",
    "dt = 50000\n",
    "feature = 10\n",
    "fig, axs = plt.subplots(3,1,figsize=(6,6))\n",
    "ax = axs[0]\n",
    "ax.plot(I[ts,t:t+dt,feature],label='I1')\n",
    "ax.plot(Ihat[ts,t:t+dt,feature],label='Ihat1')\n",
    "ax = axs[1]\n",
    "ax.plot(R_hat[ts,t:t+dt,:],label='R_hat')\n",
    "ax = axs[2]\n",
    "ax.plot(R2_hat[ts,t:t+dt,:],label='R_2hat')\n",
    "# ax.plot(Ihat[ts,:10000,2],label='Ihat3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts = -1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(R_hat[ts])\n",
    "umap = cuml.manifold.UMAP(n_components=10, n_neighbors=500, min_dist=0.0, metric='euclidean', random_state=42)\n",
    "reduced_data = umap.fit_transform(X_scaled)\n",
    "# reduced_data = umap.fit_transform(R2_hat[ts])\n",
    "# reduced_data = PCA(n_components=8).fit_transform(R_hat[ts])\n",
    "\n",
    "# clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=100, cluster_selection_epsilon=0.075, prediction_data=True)\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=5000, min_samples=5, cluster_selection_epsilon=0.05, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "labels = clusterer.labels_\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n",
    "\n",
    "soft_label = jnp.argmax(soft_clusters[:,1:].get() if isinstance(soft_clusters, cupy.ndarray) else soft_clusters[:,1:], axis=1)\n",
    "reduced_data = reduced_data.get() if isinstance(reduced_data, cupy.ndarray) else reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "# full_state_z = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000',\n",
    "                 '#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0'])\n",
    "clrs_1 = clrs[[0,2,5,8]]\n",
    "clrs_2 = clrs[:len(np.unique(soft_label))]\n",
    "def map_discrete_cbar(colors):\n",
    "\tN = len(colors)  # Number of discrete colors\n",
    "\tcmap = mpl.colors.ListedColormap(colors)\n",
    "\tbounds = np.arange(-0.5, N + .5, 1)  # [-0.5, 0.5, 1.5, ..., N-0.5]\n",
    "\tnorm = mpl.colors.BoundaryNorm(bounds, N)\n",
    "\treturn cmap, norm\n",
    "\n",
    "cmap1, norm1 = map_discrete_cbar(clrs_1)\n",
    "cmap2, norm2 = map_discrete_cbar(clrs_2)\n",
    "\n",
    "fig = plt.figure(figsize=(8.5,3), dpi=300)\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=annotations, cmap=cmap1, alpha=.05, rasterized=True)\n",
    "# ax.scatter(reduced_data[:1000,0], reduced_data[:1000,1], reduced_data[:1000,2], c=np.arange(1000), cmap='turbo', alpha=0.1)\n",
    "cbar = fig.colorbar(im, boundaries=norm1.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(annotations)),1))\n",
    "cbar.set_ticklabels([key for key in behavior_names],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('UMAP 1',fontsize=fontsize)\n",
    "ax.set_ylabel('UMAP 2',fontsize=fontsize)\n",
    "ax.set_zlabel('UMAP 3',fontsize=fontsize)\n",
    "ax.set_xlim([-5.5,5.5])\n",
    "ax.set_ylim([-5.5,5.5])\n",
    "ax.set_zlim([-5.5,5.5])\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=soft_label, cmap=cmap2, alpha=.01, rasterized=True)\n",
    "cbar = fig.colorbar(im, boundaries=norm2.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(soft_label)),1))\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('UMAP 1',fontsize=fontsize)\n",
    "ax.set_ylabel('UMAP 2',fontsize=fontsize)\n",
    "ax.set_zlabel('UMAP 3',fontsize=fontsize)\n",
    "ax.set_xlim([-5.5,5.5])\n",
    "ax.set_ylim([-5.5,5.5])\n",
    "ax.set_zlim([-5.5,5.5])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(cfg.paths.fig_dir/'UMAP_Clustering.pdf',dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts = -1\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(R_hat[ts])\n",
    "# umap = cuml.manifold.UMAP(n_components=10, n_neighbors=500, min_dist=0.0, metric='euclidean', random_state=42)\n",
    "# reduced_data = umap.fit_transform(X_scaled)\n",
    "# reduced_data = umap.fit_transform(R2_hat[ts])\n",
    "reduced_data = PCA(n_components=8).fit_transform(R_hat[ts])\n",
    "\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=100, cluster_selection_epsilon=0.075, prediction_data=True)\n",
    "# clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=5000, min_samples=5, cluster_selection_epsilon=0.05, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "labels = clusterer.labels_\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n",
    "\n",
    "soft_label = jnp.argmax(soft_clusters[:,1:].get() if isinstance(soft_clusters, cupy.ndarray) else soft_clusters[:,1:], axis=1)\n",
    "reduced_data = reduced_data.get() if isinstance(reduced_data, cupy.ndarray) else reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "# full_state_z = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000',\n",
    "                 '#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0'])\n",
    "clrs_1 = clrs[[0,2,5,8]]\n",
    "clrs_2 = clrs[:len(np.unique(soft_label))]\n",
    "def map_discrete_cbar(colors):\n",
    "\tN = len(colors)  # Number of discrete colors\n",
    "\tcmap = mpl.colors.ListedColormap(colors)\n",
    "\tbounds = np.arange(-0.5, N + .5, 1)  # [-0.5, 0.5, 1.5, ..., N-0.5]\n",
    "\tnorm = mpl.colors.BoundaryNorm(bounds, N)\n",
    "\treturn cmap, norm\n",
    "\n",
    "cmap1, norm1 = map_discrete_cbar(clrs_1)\n",
    "cmap2, norm2 = map_discrete_cbar(clrs_2)\n",
    "\n",
    "fig = plt.figure(figsize=(8.5,3), dpi=300)\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=annotations, cmap=cmap1, alpha=.05, rasterized=True)\n",
    "# ax.scatter(reduced_data[:1000,0], reduced_data[:1000,1], reduced_data[:1000,2], c=np.arange(1000), cmap='turbo', alpha=0.1)\n",
    "cbar = fig.colorbar(im, boundaries=norm1.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(annotations)),1))\n",
    "cbar.set_ticklabels([key for key in behavior_names],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('PCA 1',fontsize=fontsize)\n",
    "ax.set_ylabel('PCA 2',fontsize=fontsize)\n",
    "ax.set_zlabel('PCA 3',fontsize=fontsize)\n",
    "# ax.set_xlim([-5.5,5.5])\n",
    "# ax.set_ylim([-5.5,5.5])\n",
    "# ax.set_zlim([-5.5,5.5])\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=soft_label, cmap=cmap2, alpha=.01, rasterized=True)\n",
    "cbar = fig.colorbar(im, boundaries=norm2.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(soft_label)),1))\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('PCA 1',fontsize=fontsize)\n",
    "ax.set_ylabel('PCA 2',fontsize=fontsize)\n",
    "ax.set_zlabel('PCA 3',fontsize=fontsize)\n",
    "# ax.set_xlim([-5.5,5.5])\n",
    "# ax.set_ylim([-5.5,5.5])\n",
    "# ax.set_zlim([-5.5,5.5])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(cfg.paths.fig_dir/'PCA_Clustering.pdf',dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidhy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
