{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0e1841-1962-46c0-a629-11e884c6c99c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014eaebf-52c1-4b95-a6d9-34a1b1525b1b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os.path as op\n",
    "import ssm\n",
    "import itertools\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as pltx\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import TiDHy\n",
    "from TiDHy.utils.utils import *\n",
    "from TiDHy.datasets import load_dataset\n",
    "import TiDHy.utils.io_dict_to_hdf5 as ioh5\n",
    "from TiDHy.Evaluate_TiDHy import run_seq_len_model\n",
    "\n",
    "##### Plotting settings ######\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size':          10,\n",
    "                     'axes.linewidth':     2,\n",
    "                     'xtick.major.size':   5,\n",
    "                     'ytick.major.size':   5,\n",
    "                     'xtick.major.width':  2,\n",
    "                     'ytick.major.width':  2,\n",
    "                     'axes.spines.right':  False,\n",
    "                     'axes.spines.top':    False,\n",
    "                     'pdf.fonttype':       42,\n",
    "                     'ps.fonttype':        42,\n",
    "                     'xtick.labelsize':    10,\n",
    "                     'ytick.labelsize':    10,\n",
    "                     'figure.facecolor':   'white',\n",
    "                     'pdf.use14corefonts': True,\n",
    "                     'font.family':        'sans-serif',\n",
    "                    #  'font.family':        'Arial',\n",
    "                    #  'font.sans-serif':    'Arial',\n",
    "                     'font.serif':         'Arial',\n",
    "                    })\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "clrs = ['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000']\n",
    "cmap = ListedColormap(clrs)\n",
    "\n",
    "device = torch.device(1) #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df481e9",
   "metadata": {},
   "source": [
    "# Load SLDS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b7da2f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=10,dataset.train.num_epochs=1500/config.yaml\n",
      "1 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=11,dataset.train.num_epochs=1500/config.yaml\n",
      "2 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=12,dataset.train.num_epochs=1500/config.yaml\n",
      "3 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=13,dataset.train.num_epochs=1500/config.yaml\n",
      "4 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=14,dataset.train.num_epochs=1500/config.yaml\n",
      "5 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=15,dataset.train.num_epochs=1500/config.yaml\n",
      "6 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=16,dataset.train.num_epochs=1500/config.yaml\n",
      "7 /data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Orth_alpha_r2=5,dataset.ssm_params.seed=17,dataset.train.num_epochs=1500/config.yaml\n"
     ]
    }
   ],
   "source": [
    "dataset = 'SLDS'\n",
    "version = 'r2_orth_long'\n",
    "# version = 'partial_obs'\n",
    "base_dir = Path('/data/users/eabe/hypernets/{}/TiDHy_{}/{}'.format(dataset,dataset,version))\n",
    "configs = sorted(list(base_dir.rglob('*config.yaml'))[::2])\n",
    "fig_path = Path('/data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Paper_Figs')\n",
    "for n,conf in enumerate(configs):\n",
    "    print(n,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97fe884c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "cfg_path =configs[n]# \n",
    "default_model_config = '/home/eabe/Research/MyRepos/TiDHy/conf/dataset/model/default_model.yaml'\n",
    "cfg = load_cfg(cfg_path, default_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280b7c65",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataset: SLDS\n",
      "Setting seed: 10\n",
      "Setting seed: 10\n",
      "Setting seed: 10\n",
      "x Timescales: {'0': -0.025, '1': -0.25, '2': -0.1, '3': -0.5, '4': -0.01, '5': -0.75}, z Timescales: [0.975, 0.975, 0.975]\n",
      "Our inputs have shape: torch.Size([250, 200, 20])\n"
     ]
    }
   ],
   "source": [
    "data_dict, cfg = load_dataset(cfg)\n",
    "\n",
    "# ##### Convert to float tensors #####\n",
    "train_inputs = torch.tensor(data_dict['inputs_train']).float()\n",
    "test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "val_inputs = torch.tensor(data_dict['inputs_val']).float()\n",
    "\n",
    "input_dim = test_inputs.shape[-1]\n",
    "# test_inputs = test_inputs.unsqueeze(0)\n",
    "# test_inputs = stack_data(test_inputs,cfg.train.sequence_length,overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "test_inputs = test_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "val_inputs = val_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "# train_inputs = train_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "# cfg.model.input_dim = input_dim\n",
    "\n",
    "\n",
    "\n",
    "print(f'Our inputs have shape: {test_inputs.shape}')\n",
    "cfg.model.input_dim = input_dim\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_inputs)\n",
    "# dataloader_train = torch.utils.data.DataLoader(train_dataset,batch_size=train_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=test_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "# dataloader_val = torch.utils.data.DataLoader(val_dataset,batch_size=val_inputs.shape[0],shuffle=False,pin_memory=True,drop_last=True)\n",
    "# device = torch.device(\"cuda:{}\".formt(cfg.train['gpu']) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd10df18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['inputs_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce1a91",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "##### Load weights and params #####\n",
    "rerecord = False\n",
    "cfg.model.batch_converge=False\n",
    "epoch = 1000\n",
    "model = TiDHy.TiDHy(cfg.model, device, show_progress=cfg.train.show_progress).to(device)\n",
    "# set_seed(42)\n",
    "load_path = cfg.paths.ckpt_dir/'best.pth.tar'\n",
    "# load_path = sorted(list(cfg.paths.ckpt_dir.rglob('last_{}.pth.tar'.format(epoch))))[0]\n",
    "data_load = torch.load(load_path,map_location=device)\n",
    "model.load_state_dict(data_load['state_dict'])\n",
    "model.to(device)\n",
    "print('Loaded from {} epoch'.format(data_load['epoch']))\n",
    "epoch = data_load['epoch']-1\n",
    "\n",
    "\n",
    "# result_dict = load_results(model, dataloader_test, data_dict, device, cfg, epoch, rerecord=rerecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[8,8,1,1,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "# cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5434c",
   "metadata": {},
   "source": [
    "# Save Multi Seq Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cae1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "cfg, result_dict = run_seq_len_model(seq_len, model, data_dict, device, 1500, cfg, rerun=False)\n",
    "\n",
    "data_dict, cfg = load_dataset(cfg)\n",
    "seq_len = sorted([int(el) for el in list(result_dict.keys())])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2664017",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fa3122",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "states_x_test = data_dict['states_x_test']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "# states_z_test = data_dict['states_z']\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "##### Set up combinatorics of timescales #####\n",
    "lst = list(itertools.product([1, 0], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros(ssm_params['time_bins_test'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,states_z_test)] = n\n",
    "\n",
    "\n",
    "save_figs = False\n",
    "p=-1\n",
    "W = result_dict['{}'.format(seq_len[p])]['W'].reshape(-1,result_dict['{}'.format(seq_len[p])]['W'].shape[-1])\n",
    "# b = result_dict['b'].reshape(-1,result_dict['b'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[p])]['I'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[p])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1])\n",
    "Ibar = result_dict['{}'.format(seq_len[p])]['I_bar'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[p])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[p])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[p])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1])\n",
    "# Ut = result_dict['Ut'].reshape(-1,result_dict['Ut'].shape[-2],result_dict['Ut'].shape[-1])\n",
    "# ##### Plot dynamic matrices #####\n",
    "if cfg.model.low_rank_temp:\n",
    "    Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "else:\n",
    "    Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "    U_t = torch.matmul(torch.Tensor(W[:,None,:]), model.temporal.unsqueeze(0).cpu().detach()).reshape(-1, model.r_dim, model.r_dim).numpy()\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "bs = np.stack([v for v in data_dict['bs'].values()])\n",
    "nfig = 0\n",
    "fontsize=13\n",
    "# t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770f503d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ssm.util import random_rotation, find_permutation\n",
    "inputs_train_SLDS=data_dict['inputs_train']\n",
    "inputs_test_SLDS=data_dict['inputs_test']\n",
    "seed = cfg.dataset.ssm_params['seed']\n",
    "# SLDS_path = Path('/data/users/eabe/hypernets/SLDS/TiDHy_SLDS/L1ShortWin/L1_alpha=0.001,dataset.ssm_params.seed=10,dataset.train.sequence_length=200')\n",
    "SLDS_path = cfg.paths.log_dir#/'SSM/DPC_SSM/Benchmark/dataset.ssm_params.seed={}/'.format(seed)\n",
    "N = inputs_train_SLDS.shape[-1]\n",
    "K = len(np.unique(full_state_z))\n",
    "D = data_dict['states_x_test'].shape[-1]\n",
    "with open(SLDS_path/'ssm_slds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "# with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "    slds = pickle.load(handle)\n",
    "    posterior = pickle.load(handle)\n",
    "    SLDS_latents = pickle.load(handle)\n",
    "    SLDS_states = pickle.load(handle)\n",
    "    SLDS_emission = pickle.load(handle)\n",
    "\n",
    "with open(SLDS_path/'ssm_rslds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "# with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "    slds = pickle.load(handle)\n",
    "    posterior = pickle.load(handle)\n",
    "    rSLDS_latents = pickle.load(handle)\n",
    "    rSLDS_states = pickle.load(handle)\n",
    "    rSLDS_emission = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537481f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=-1\n",
    "reg_variables = [\n",
    "    result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['W'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_bar'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['I'],\n",
    "    SLDS_latents,]\n",
    "\n",
    "labels = ['R2_hat','W','R_bar','R_hat','I','SLDS']\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "\n",
    "state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_comps=ssm_params['latent_dim']\n",
    "t=1500; dt=200\n",
    "    \n",
    "fig = plt.figure(constrained_layout=True, figsize=(7,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=5,ncols=4, figure=fig, wspace=.45,hspace=.2)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=6,ncols=1,subplot_spec=gs0[:,:2],wspace=.1,hspace=.2)\n",
    "# gs01 = gridspec.GridSpecFromSubplotSpec(3,1, subplot_spec=gs0[0:,:1],wspace=.05,hspace=.8)\n",
    "axs = np.array([fig.add_subplot(gs00[:-3,0]),\n",
    "                fig.add_subplot(gs00[-3:-2,0]),\n",
    "                fig.add_subplot(gs00[-2:,0]),\n",
    "                fig.add_subplot(gs0[:3,2:]),\n",
    "                fig.add_subplot(gs0[3:,2:])])\n",
    "axs[4].sharex(axs[3])\n",
    "ts = -1\n",
    "##### Pannel a #####\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "ax = axs[0]\n",
    "spacing = 1\n",
    "xrange = 100\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z)))\n",
    "linestyle =  (0, (5, 1))#(0, (5, 5))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=2,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing, linestyle=linestyle, color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "# ax.set_yticks(hlines_I)\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "##### Pannel c #####\n",
    "ax = axs[3]\n",
    "count=0\n",
    "acc_slds = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_rslds = accuracy_score(full_state_z, rSLDS_states)\n",
    "acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "spacing = .5\n",
    "\n",
    "hlines_x,hlines_Rhat = [],[]\n",
    "for p in range(ssm_params['Nlds']):\n",
    "    # states_x_cca = states_x_train[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    states_x_cca = states_x_test[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    # states_x_cca = states_x_test[:,p:p+1]\n",
    "    cca = CCA(n_components=ssm_params['latent_dim'],max_iter=1000)\n",
    "    X_c,Y_c = cca.fit_transform(states_x_cca,R_hat)\n",
    "    cca_coefficient = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=n_comps)\n",
    "    x_w = cca.x_weights_\n",
    "    y_w = cca.y_weights_\n",
    "    cca_angles = [np.rad2deg(angle_between(X_c[:,n],Y_c[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_x = [np.rad2deg(angle_between(X_c[:,n],states_x_cca[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_r = [np.rad2deg(angle_between(Y_c[:,n],R_hat[:,n])) for n in range(n_comps)]\n",
    "    for n in range(n_comps):\n",
    "        print('comp {}, cc: {:.03}, ang: {:.03}, ang_x:{:.03}, ang_r:{:.03}'.format(n,cca_coefficient[n],cca_angles[n],cca_angles_x[n],cca_angles_r[n]))\n",
    "\n",
    "    for i in range(X_c.shape[-1]):\n",
    "        mean_centered_x = X_c[t:t+dt,i] - np.mean(X_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_x=mean_centered_x/(np.max(np.abs(mean_centered_x)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_x + count/spacing,color='k', lw=2,zorder=1)\n",
    "        hlines_x.append(np.mean(mean_centered_x + count/spacing,axis=0))\n",
    "        mean_centered_Rhat = Y_c[t:t+dt,i] - np.mean(Y_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_Rhat=mean_centered_Rhat/(np.max(np.abs(mean_centered_Rhat)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_Rhat + count/spacing,linestyle=linestyle,color='r', lw=1.5,zorder=2,label='$\\hat{{r}}_{{{}}}$={:.02}'.format(n,cca_coefficient[n]),alpha=1)\n",
    "        hlines_Rhat.append(np.mean(mean_centered_Rhat + count/spacing,axis=0))\n",
    "        ax.text(x=dt+15,y=hlines_x[count],s='cc = {:.02}'.format(cca_coefficient[i]),fontsize=fontsize-2)\n",
    "        count += 1\n",
    "    ax.set_yticks(hlines_x) \n",
    "    ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "    # ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "    ax.set_ylabel('latent variables',fontsize=fontsize)\n",
    "# ax.set_xticks([])\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps', fontsize=fontsize)\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*6))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*6+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "##### Pannel b #####\n",
    "ax = axs[1]\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "# timescales_As = 1/np.abs(np.log(np.real(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "min_error_idx = np.array([np.argmin(np.abs(timescales.reshape(-1) - timescales_As.reshape(-1)[n])) for n in range(timescales_As.shape[0])])\n",
    "min_error_timescales = timescales[min_error_idx]\n",
    "# timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "ax.scatter(x=min_error_timescales,y=np.abs(evals_Uhat_all.reshape(-1)[min_error_idx]), marker='x',\n",
    "            c='k',s=50,alpha=1,edgecolor='None',zorder=3)\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlim(0.25,2.5e2)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n",
    "\n",
    "######### Accuracy and reconstruction error #########\n",
    "ax = axs[2]\n",
    "Input_Errors_SLDS = np.abs(inputs_test_SLDS-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(inputs_test_SLDS-rSLDS_emission)\n",
    "Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs[1],label='TiDHy')\n",
    "ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs[1],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.legend(frameon=False,fontsize=fontsize-3,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "##### Pannel d #####\n",
    "ax = axs[4]\n",
    "ylabels_states = ['rSLDS \\n {:02}%'.format(int(np.round(acc_rslds*100))),\n",
    "                  'SLDS \\n {:02}%'.format(int(np.round(acc_slds*100))),\n",
    "                  'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),\n",
    "                  'True']\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,4,1))\n",
    "ax.set_yticklabels(ylabels_states,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=-.2)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Benchmark_Compare3.pdf',dpi=300)\n",
    "fig.savefig(fig_path/'{}_Fig2_V2.pdf'.format(nfig),dpi=300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=1\n",
    "best_pred_all = []\n",
    "acc_ssm = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_TiDHy_all = []\n",
    "for ts in range(len(seq_len)):\n",
    "    reg_variables = [\n",
    "        # np.concatenate([result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['W']],axis=-1)\n",
    "        result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "        # q_lem_x,\n",
    "        ]\n",
    "\n",
    "    labels = ['R2_hat']\n",
    "    # reg_variables = W\n",
    "    max_acc = 0\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        print(labels[k],scores)\n",
    "        acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "        acc_TiDHy_all.append(acc_TiDHy)\n",
    "        best_pred_all.append(y_pred)\n",
    "\n",
    "state_compare = np.concatenate([SLDS_states[None,:],np.stack(best_pred_all),full_state_z[None,:]],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbac2ec",
   "metadata": {},
   "source": [
    "## Load multi-seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d49857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "result_dict_all = {}\n",
    "for n in range(len(configs)):\n",
    "    cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "    default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "    cfg = load_cfg(cfg_path, default_model_config)\n",
    "    cfg, result_dict = run_seq_len_model(seq_len, model, data_dict, device, 1500, cfg, rerun=False)\n",
    "    result_dict_all['seed_{}'.format(cfg.dataset.ssm_params.seed)] = result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values\n",
    "seeds = np.arange(10,10+len(configs))\n",
    "seed = 10\n",
    "ts = 200\n",
    "\n",
    "##### Set up combinatorics of timescales #####\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "lst = list(itertools.product([0, 1], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros((len(seeds),ssm_params['time_bins_test']),dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for i, seed in enumerate(seeds):\n",
    "    for n in range(len(lst)):\n",
    "        full_state_z[i,np.apply_along_axis(lambda x: np.all(x == lst[n]),0,result_dict_all[f'seed_{seed}'][f'{ts}']['states_z_test'])] = n\n",
    "    \n",
    "I_all = np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['I'] for seed in seeds])\n",
    "I_hat_all = np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['I_hat'] for seed in seeds])\n",
    "R2_hat_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R2_hat'] for ts in seq_len])for seed in seeds])  # (n_seeds, Time, Z_dim)\n",
    "R_bar_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R_bar'] for ts in seq_len]) for seed in seeds])   # (n_seeds, TempWinN, Time, r2_dim)\n",
    "R_hat_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R_hat'] for ts in seq_len]) for seed in seeds])   # (n_seeds, TempWinN, Time, r_dim)\n",
    "W_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['W'] for ts in seq_len]) for seed in seeds])           # (n_seeds, TempWinN, Time, r_dim)\n",
    "As_all = np.stack([np.stack([v for v in result_dict_all[f'seed_{seed}'][f'{ts}']['As'].values()]) for seed in seeds])\n",
    "result_dict_all[f'seed_{seed}'][f'{ts}'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fac0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ssm.util import random_rotation, find_permutation\n",
    "inputs_train_SLDS=data_dict['inputs_train']\n",
    "inputs_test_SLDS=data_dict['inputs_test']\n",
    "seed = cfg.dataset.ssm_params['seed']\n",
    "TotalTime = cfg.dataset.ssm_params.time_bins_test\n",
    "# SLDS_path = Path('/data/users/eabe/hypernets/SLDS/TiDHy_SLDS/L1ShortWin/L1_alpha=0.001,dataset.ssm_params.seed=10,dataset.train.sequence_length=200')\n",
    "N = inputs_train_SLDS.shape[-1]\n",
    "K = len(np.unique(full_state_z))\n",
    "D = data_dict['states_x_test'].shape[-1]\n",
    "SLDS_latents = np.zeros((len(seeds),TotalTime,D))\n",
    "SLDS_states = np.zeros((len(seeds),TotalTime))\n",
    "SLDS_emission = np.zeros((len(seeds),TotalTime,N))\n",
    "rSLDS_latents = np.zeros((len(seeds),TotalTime,D))\n",
    "rSLDS_states = np.zeros((len(seeds),TotalTime))\n",
    "rSLDS_emission = np.zeros((len(seeds),TotalTime,N))\n",
    "for k, seed in enumerate(seeds):\n",
    "    SLDS_path = configs[k].parent#/'SSM/DPC_SSM/Benchmark/dataset.ssm_params.seed={}/'.format(seed)\n",
    "    with open(SLDS_path/'ssm_slds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "    # with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "        slds = pickle.load(handle)\n",
    "        posterior = pickle.load(handle)\n",
    "        SLDS_latents[k] = pickle.load(handle)\n",
    "        SLDS_states[k] = pickle.load(handle)\n",
    "        SLDS_emission[k] = pickle.load(handle)\n",
    "\n",
    "    with open(SLDS_path/'ssm_rslds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "    # with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "        slds = pickle.load(handle)\n",
    "        posterior = pickle.load(handle)\n",
    "        rSLDS_latents[k] = pickle.load(handle)\n",
    "        rSLDS_states[k] = pickle.load(handle)\n",
    "        rSLDS_emission[k] = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=-1\n",
    "\n",
    "labels = seeds\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "scores_all = []\n",
    "pred_all = np.zeros((len(seeds),len(seq_len),cfg.dataset.ssm_params.time_bins_test))\n",
    "for k in range(len(seeds)):\n",
    "    for ts in range(len(seq_len)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(R2_hat_all[k,ts], full_state_z[k].reshape(-1), test_size=0.25, random_state=42)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        y_pred = neigh.predict(R2_hat_all[k,ts])\n",
    "        pred_all[k,ts] = y_pred\n",
    "        scores_all.append(accuracy_score(full_state_z[k].reshape(-1),y_pred))\n",
    "        print(labels[k],seq_len[ts],scores)\n",
    "        if (scores > max_acc) & (labels[k] != 'I'):\n",
    "            max_acc = scores\n",
    "            best_label = k\n",
    "            best_pred = y_pred\n",
    "\n",
    "# state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z[best_label]],axis=0)\n",
    "\n",
    "scores_all = np.stack(scores_all).reshape(len(seeds),len(seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f817574",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(8,6))\n",
    "gs0 = gridspec.GridSpec(nrows=4,ncols=2, figure=fig, wspace=.15,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs0[:2,0]),fig.add_subplot(gs0[2:,0]),fig.add_subplot(gs0[:2,1]),fig.add_subplot(gs0[2:,1])])\n",
    "t=1500; dt=1000\n",
    "# seq_len=[100,200,500]\n",
    "ts = 1\n",
    "Input_Errors_SLDS = (I_all[k]-SLDS_emission[k])\n",
    "ax = axs[-1]\n",
    "Input_Errors_SLDS = np.mean([np.abs(I_all[k]-SLDS_emission[k]) for k in range(len(seeds))],axis=-1)\n",
    "Input_Errors_rSLDS = np.mean([np.abs(I_all[k]-rSLDS_emission[k]) for k in range(len(seeds))],axis=-1)\n",
    "err_slds  = np.mean([100*(1-accuracy_score(full_state_z[k], SLDS_states[k])) for k in range(len(seeds))])\n",
    "err_rslds = np.mean([100*(1-accuracy_score(full_state_z[k], rSLDS_states[k])) for k in range(len(seeds))])\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors =np.stack([np.abs(I_all[seed,ts]-I_hat_all[seed,ts]) for seed in range(len(seeds))])\n",
    "    err_TiDHy = np.mean([100*(1-accuracy_score(full_state_z[k], pred_all[k,ts])) for k in range(len(seeds))])\n",
    "    ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs_b[ts],label='TiDHy')\n",
    "    ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs_b[ts],xerr=np.mean(np.std(Input_Errors,axis=-1)),yerr=np.mean(np.std(Input_Errors,axis=0)))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=np.mean(err_rslds), s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=np.mean(err_rslds), ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.tick_params(axis='y', labelsize=fontsize-2)\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "tts+=1\n",
    "ax.annotate('SLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[2],fontsize=fontsize-2)\n",
    "tts += 1\n",
    "ax.annotate('rSLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[6],fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs[2]\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "# timescales_As = 1/np.abs(np.log(np.real(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    for seed in seeds: \n",
    "        R_bar = result_dict_all['seed_{}'.format(seed)]['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "        R_hat = result_dict_all['seed_{}'.format(seed)]['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "        ##### Plottign Eigenvalues #####\n",
    "        Uhat_all = []\n",
    "        for state in range(len(np.unique(full_state_z[0]))):\n",
    "            # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "            # model = VAR(rhat2)\n",
    "            # results = model.fit(maxlags=20,ic='aic')\n",
    "            # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "            # Uhat_all.append(eig_Ut.reshape(-1))\n",
    "        # for p in range(ssm_params.Nlds):\n",
    "        #     for state in range(ssm_params.n_disc_states):\n",
    "                # rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "            rhat2 = R_hat[np.where(full_state_z[0]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "\n",
    "        Uhat_all = np.stack(Uhat_all)\n",
    "        # evals_Uhat_all = np.hstack(Uhat_all)\n",
    "        # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "        # timescales = np.real(np.log(Uhat_all))\n",
    "        evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "        timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "        # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "        min_error_idx = np.array([np.argmin(np.abs(timescales.reshape(-1) - timescales_As.reshape(-1)[n])) for n in range(timescales_As.shape[0])])\n",
    "        min_error_timescales = timescales[min_error_idx]\n",
    "        ax.scatter(x=min_error_timescales,y=np.abs(evals_Uhat_all.reshape(-1)[min_error_idx])+ts,\n",
    "                c=clrs_b[ts],alpha=.75,edgecolor='None',marker='x',s=25,zorder=3)\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "        ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    ax.set_xscale('symlog',linthresh=.1)\n",
    "    ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "    ax.set_ylabel('T',fontsize=fontsize,labelpad=-2)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=.015)\n",
    "y = .99\n",
    "dy = 0.075\n",
    "ax.annotate('System 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-2)\n",
    "ax.annotate('System 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-2)\n",
    "ax.annotate('System 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-2)\n",
    "\n",
    "ax=axs[0]\n",
    "spacing= 5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xlim([0,dt])\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(1.05,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "\n",
    "# legend = ax.legend(['T={}'.format(seq_len[p]) for p in range(len(seq_len))],frameon=False,fontsize=fontsize,loc='upper right',\n",
    "#           bbox_to_anchor=(1.25,1),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1)\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "state_compare = np.concatenate([SLDS_states[:1,:],pred_all[0],full_state_z[:1,:]],axis=0)\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z[0])))\n",
    "acc_slds = accuracy_score(full_state_z[0], SLDS_states[0])\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(scores_all[0,n]*100))) for n in range(len(scores_all[0]))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_slds*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+200,200))\n",
    "# ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "fig.savefig(fig_path/'{}_Fig3.pdf'.format(0),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1\n",
    "Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "np.mean(Input_Errors),np.std(Input_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(8,6))\n",
    "gs0 = gridspec.GridSpec(nrows=4,ncols=2, figure=fig, wspace=.15,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs0[:2,0]),fig.add_subplot(gs0[2:,0]),fig.add_subplot(gs0[:2,1]),fig.add_subplot(gs0[2:,1])])\n",
    "t=1500; dt=1000\n",
    "# seq_len=[100,200,500]\n",
    "\n",
    "Input_Errors_SLDS = (inputs_test_SLDS-SLDS_emission)\n",
    "ax = axs[0]\n",
    "Input_Errors_SLDS = np.abs(inputs_test_SLDS-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(inputs_test_SLDS-rSLDS_emission)\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "    err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "    ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs_b[ts],label='TiDHy')\n",
    "    ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs_b[ts],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.tick_params(axis='y', labelsize=fontsize-2)\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "tts+=1\n",
    "ax.annotate('SLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[2],fontsize=fontsize-2)\n",
    "tts += 1\n",
    "ax.annotate('rSLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[6],fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs[1]\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        # model = VAR(rhat2)\n",
    "        # results = model.fit(maxlags=20,ic='aic')\n",
    "        # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "        # Uhat_all.append(eig_Ut.reshape(-1))\n",
    "    # for p in range(ssm_params.Nlds):\n",
    "    #     for state in range(ssm_params.n_disc_states):\n",
    "            # rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    # evals_Uhat_all = np.hstack(Uhat_all)\n",
    "    # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "    # timescales = np.real(np.log(Uhat_all))\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "    \n",
    "    ax.scatter(x=timescales,y=np.abs(evals_Uhat_all)+ts,\n",
    "               c=clrs_b[ts],alpha=.75,edgecolor='None',s=25)\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "        ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    ax.set_xscale('symlog',linthresh=.1)\n",
    "    ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "    ax.set_ylabel('T',fontsize=fontsize,labelpad=-2)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=.015)\n",
    "y = .99\n",
    "dy = 0.075\n",
    "ax.annotate('System 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-2)\n",
    "ax.annotate('System 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-2)\n",
    "ax.annotate('System 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-2)\n",
    "\n",
    "ax=axs[2]\n",
    "spacing= 5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xlim([0,dt])\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(1.05,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "\n",
    "# legend = ax.legend(['T={}'.format(seq_len[p]) for p in range(len(seq_len))],frameon=False,fontsize=fontsize,loc='upper right',\n",
    "#           bbox_to_anchor=(1.25,1),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1)\n",
    "\n",
    "\n",
    "ax = axs[3]\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(acc_TiDHy_all[n]*100))) for n in range(len(acc_TiDHy_all))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_ssm*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+200,200))\n",
    "# ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "fig.savefig(cfg.paths.fig_dir/'{}_Fig3.pdf'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60973a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade off plot between accuracy of reconstruction and correct identification of unique timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(5,3))\n",
    "ax = axs\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "ax.scatter(x=timescales,y=np.abs(evals_Uhat_all),\n",
    "            c='k',s=40,alpha=.5,edgecolor='None')\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd0f76",
   "metadata": {},
   "source": [
    "# Load Anymal Terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffe840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'AnymalTerrain'\n",
    "base_dir = Path('/data/users/eabe/hypernets/{}/TiDHy_{}/'.format(dataset,dataset))\n",
    "configs = sorted(list(base_dir.rglob('*config.yaml'))[::2])\n",
    "\n",
    "for n,conf in enumerate(configs):\n",
    "    print(n,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3dfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "params = OmegaConf.load(default_model_config)\n",
    "cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "for k in cfg.paths.keys():\n",
    "    cfg.paths[k] = Path(cfg.paths[k])\n",
    "    cfg.paths[k].mkdir(parents=True, exist_ok=True)\n",
    "params_curr = cfg.dataset.model\n",
    "cfg.dataset.model = OmegaConf.merge(params, params_curr)\n",
    "cfg.dataset.train.normalize_obs = False\n",
    "cfg.model.batch_converge=False\n",
    "cfg.model.Orth_alpha_spat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238228b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict, cfg = load_dataset(cfg)\n",
    "# ##### Convert to float tensors #####\n",
    "train_inputs = torch.tensor(data_dict['inputs_train']).float()\n",
    "test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "\n",
    "input_dim = train_inputs.shape[-1]\n",
    "\n",
    "train_inputs = train_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "test_inputs = test_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "cfg.model.input_dim = input_dim\n",
    "\n",
    "if cfg.train.batch_size_input:\n",
    "    batch_size_train = train_inputs.shape[0]\n",
    "    batch_size_test = test_inputs.shape[0]\n",
    "else:\n",
    "    batch_size_train = cfg.train.batch_size\n",
    "    # batch_size_test = cfg.train.batch_size\n",
    "    batch_size_test = test_inputs.shape[0]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "dataloader_train = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size_train,pin_memory=True,shuffle=True,drop_last=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size_test,pin_memory=True,drop_last=True)\n",
    "device = torch.device(\"cuda:{}\".format(cfg.train['gpu']) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Load weights and params #####\n",
    "rerecord = False\n",
    "epoch = 1000\n",
    "model = TiDHy.TiDHy(cfg.model, device, show_progress=cfg.train.show_progress).to(device)\n",
    "# set_seed(42)\n",
    "load_path = cfg.paths.ckpt_dir/'best.pth.tar'\n",
    "load_path = sorted(list(cfg.paths.ckpt_dir.rglob('last_{}.pth.tar'.format(epoch))))[0]\n",
    "data_load = torch.load(load_path,map_location=device)\n",
    "model.load_state_dict(data_load['state_dict'])\n",
    "model.to(device)\n",
    "print('Loaded from {} epoch'.format(data_load['epoch']))\n",
    "epoch = data_load['epoch']\n",
    "\n",
    "\n",
    "# result_dict = load_results(model, dataloader_test, data_dict, device, cfg, epoch, rerecord=rerecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "# cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b51ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "cfg, result_dict = run_seq_len_model(seq_len, model, data_dict, device, epoch, cfg, rerun=False)\n",
    "\n",
    "data_dict, cfg = load_dataset(cfg)\n",
    "seq_len = sorted([int(el) for el in list(result_dict.keys())])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_figs = False\n",
    "ts=-2\n",
    "W = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "# b = result_dict['b'].reshape(-1,result_dict['b'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "Ibar = result_dict['{}'.format(seq_len[ts])]['I_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "# Ut = result_dict['Ut'].reshape(-1,result_dict['Ut'].shape[-2],result_dict['Ut'].shape[-1])\n",
    "# ##### Plot dynamic matrices #####\n",
    "if cfg.model.low_rank_temp:\n",
    "    Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "else:\n",
    "    Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "    U_t = torch.matmul(torch.Tensor(W[:,None,:]), model.temporal.unsqueeze(0).cpu().detach()).reshape(-1, model.r_dim, model.r_dim).numpy()\n",
    "\n",
    "nfig = 0\n",
    "t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50825e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "t=0; dt=1000\n",
    "spacing= .1\n",
    "ts=-2\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "I_shuff  = deepcopy(result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1]))\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,5))\n",
    "ax = axs[0]\n",
    "# cmap,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=1,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing,ls='-',color='#4e7eb3ff', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*I.shape[-1]))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1]+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "# # im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1],1)),cmap=cmap,norm=norm,alpha=.5)\n",
    "# cbar = fig.colorbar(im,ax=axs,aspect=30)\n",
    "# cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "# cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "# cbar.outline.set_linewidth(1)\n",
    "# cbar.minorticks_off()\n",
    "# cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "ax.set_yticks(hlines_I)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(0,1200,200),fontsize=fontsize)\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "ax.set_ylabel('Observation #',fontsize=fontsize)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.025,1.05),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "# ax.set_title('Learned Observations',fontsize=fontsize)\n",
    "lim0 = 0\n",
    "lim1 = 20\n",
    "hbins = 1\n",
    "Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# Input_Errors_SLDS = np.mean((inputs_test_SLDS-q_lem_y)**2,axis=0)\n",
    "I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "ax = axs[1]\n",
    "count,edges = np.histogram(Input_Errors,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='#757575ff',width=hbins, alpha=1,zorder=1,label='data')\n",
    "count,edges = np.histogram(Input_Errors_shuff,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='r',width=hbins, alpha=1,zorder=1,label='shuffle') \n",
    "# count,edges = np.histogram(Input_Errors_SLDS,bins=np.arange(lim0,lim1,hbins))\n",
    "# edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "# ax.bar(edges_mid, count/len(Input_Errors_SLDS),color='g',width=hbins, alpha=.5,zorder=1,label='SLDS') \n",
    "ax.set_ylabel('Proportion',fontsize=fontsize)\n",
    "ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize)\n",
    "# ax = axs[1]\n",
    "# fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "# Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "# Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "# xs = np.arange(I.shape[-1])\n",
    "# heights = Input_Errors\n",
    "# ax.barh(y=hlines_I,width=heights,color='k',)\n",
    "# ax.errorbar(heights,hlines_I,xerr=np.std((I-Ihat),axis=0)/np.sqrt((I-Ihat).shape[0]),ls='none',color='tab:gray',capsize=3)\n",
    "# ax.axvline(x=np.mean(Input_Errors_shuff),c='k',ls='--',lw=1,label='shuffle error')\n",
    "# # ax.set_xticks(np.arange(0,.004,.001))\n",
    "# ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "# ax.set_title('Average Error',fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(heights.shape[-1])+1)\n",
    "# ax.legend(bbox_to_anchor=(.3, .95), loc='lower left', borderaxespad=0.,frameon=False,fontsize=fontsize)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_observations_pred.png'.format(nfig),dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "t=0; dt = 1000\n",
    "# cmap2,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig, axs = plt.subplots(2,2,figsize=(20,12))\n",
    "axs = axs.flatten()\n",
    "fontsize=13\n",
    "ax = axs[0]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R2_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-.5,.75,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('$\\hat{r}^h$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax.legend(['T={}'.format(Ln) for Ln in seq_len],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.15,1.05),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "ax = axs[1]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['W'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['W'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(0,2.25,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('W',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax = axs[2]\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_bar'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "\n",
    "ax = axs[3]\n",
    "spacing= 1\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['I_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[p])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=.5)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('$\\hat{Z}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_TempWindExpansion.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1073bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind = [2,8,9]\n",
    "clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "terrain_names = ['Flat','Slope','Inv. Slope','Stairs','Inv. Stairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain_type_train = data_dict['terrain_train'].reshape(-1)\n",
    "terrain_type_test = data_dict['terrain_test']\n",
    "terrain_difficulty_train = data_dict['terrain_difficulty_train'].reshape(-1)\n",
    "terrain_difficulty_test = data_dict['terrain_difficulty_test'].reshape(-1)\n",
    "terrain_slope_train = data_dict['terrain_slope_train'].reshape(-1)\n",
    "terrain_slope_test = data_dict['terrain_slope_test'].reshape(-1)\n",
    "command_vel_train = data_dict['command_vel_train'].reshape(-1)\n",
    "command_vel_test = data_dict['command_vel_test'].reshape(-1)\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "terrain_robot_test = np.stack([terrain_type_test[robot_type_test==0].reshape(-1),terrain_type_test[robot_type_test==1].reshape(-1)])\n",
    "terrain_type_test = data_dict['terrain_test'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59828be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "seq_len=[100,200,500,1000]\n",
    "ax = axs\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.random.uniform(y_ranges[ts,0],y_ranges[ts,1],timescales.shape[0]),\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None')\n",
    "\n",
    "# for n in range(len(timescales_M)):\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize)\n",
    "ax.set_ylabel('T',fontsize=fontsize)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize)\n",
    "ax.set_xticks([0,10e0,10e1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ts=-2\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([result_dict['R2_hat'],result_dict['W'],result_dict['R_bar']],axis=-1),result_dict['R2_hat'],result_dict['W'],result_dict['R_hat'],result_dict['I']]\n",
    "reg_variables = [result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['I']]\n",
    "labels = ['R2_hat','I']\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "command_vel_test = np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    # X_train = reg_vars[:tr_batch_size].reshape(-1,reg_vars.shape[-1])\n",
    "    # X_test = reg_vars[-batch_size:].reshape(-1,reg_vars.shape[-1])\n",
    "    # y_train = terrain[:tr_batch_size].reshape(-1)\n",
    "    # y_test = terrain[-batch_size:].reshape(-1)\n",
    "    # y_train = robot_type_test[:tr_batch_size].reshape(-1)\n",
    "    # y_test = robot_type_test[-batch_size:].reshape(-1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), robot_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    # neigh = RidgeClassifierCV()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    # y_pred = neigh.predict(X_test)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred_terrain = y_pred\n",
    "    else:\n",
    "        I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        \n",
    "state_compare = np.stack([I_pred,best_pred_terrain,terrain_type_test])\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded530d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_type_test.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([R2_hat,W,R_bar],axis=-1),R2_hat,W,R_hat,I]\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(I.reshape(-1,I.shape[-1]))\n",
    "reg_variables = [R2_hat,I] #,X_pca]\n",
    "labels = ['R2_hat','I'] #,'X_pca']\n",
    "label_type = ['terrain_difficulty_test','terrain_slope_test','command_vel_test']#,'robot_type_test']\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0).reshape(-1)\n",
    "command_vel_test = np.tile(data_dict['command_vel_test'],(9000,1)).transpose(1,0)\n",
    "slope_cat = np.unique(terrain_slope_test,return_inverse=True)[1]\n",
    "difficulty_cat =  np.unique(terrain_difficulty_test,return_inverse=True)[1]\n",
    "vel_cat = np.unique(command_vel_test,return_inverse=True)[1]\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "class_results = {}\n",
    "state_compare_best = {}\n",
    "for m, test_labels in enumerate([difficulty_cat,slope_cat,vel_cat]): #,robot_type_test\n",
    "\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), test_labels.reshape(-1), test_size=0.25, random_state=42)\n",
    "\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        print(label_type[m],labels[k],scores)\n",
    "        if (labels[k] != 'I'):\n",
    "            y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = y_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "            max_acc = scores\n",
    "            best_reg_vars = reg_vars\n",
    "            best_label = labels[k]\n",
    "            best_pred = y_pred\n",
    "        else:\n",
    "            I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = I_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "    state_compare_best['{}'.format(label_type[m])] = np.stack([I_pred,best_pred,test_labels],axis=0)\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n",
    "from sklearn import metrics\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bafde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_compare_data = {'state_compare_best':state_compare_best,'state_compare':state_compare,'class_results':class_results}\n",
    "ioh5.save(cfg.paths.log_dir/'state_compare.h5',state_compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e59514",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_compare_data = ioh5.load(cfg.paths.log_dir/'state_compare.h5')\n",
    "state_compare = state_compare_data['state_compare']\n",
    "state_compare_best = state_compare_data['state_compare_best']\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=2500; dt=1000\n",
    "fontsize=13\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=6,ncols=3, figure=fig, wspace=.1,hspace=.1)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5, subplot_spec=gs0[:3,:],wspace=.1,hspace=.1)\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5,subplot_spec=gs0[4:,:],wspace=.1,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs00[:,:2]),\n",
    "                fig.add_subplot(gs00[:,2:4]),\n",
    "                fig.add_subplot(gs00[:,4:]),\n",
    "                fig.add_subplot(gs01[:,:2]),\n",
    "                fig.add_subplot(gs01[:,2:])])\n",
    "ax = axs[1]\n",
    "spacing= .5\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=1)\n",
    "ts=1\n",
    "hlines_x = []\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I_hat'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c='r',alpha=.75)\n",
    "    hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "# ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "# ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "# ax.set_yticks(hlines_x[1::2])\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(2,len(hlines_x)+1,2),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "# ax.set_ylim(-1,(1/spacing)*len(hlines_x)+.5)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xticks([0,10e0,10e1,10e2])\n",
    "ax.set_xlim(0,1e2)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=-1)\n",
    "\n",
    "\n",
    "ax=axs[3]\n",
    "ax.bar(np.arange(len(f1_TiDHy)),f1_TiDHy,width=.4,color=clrs2[0])\n",
    "ax.bar(np.arange(len(f1_Inputs))+.4,f1_Inputs,width=.4,color=clrs2[4])\n",
    "ax.set_ylabel('F1 Score',fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(.2,len(f1_TiDHy)+.2))\n",
    "ax.set_xticklabels(['difficulty','slope','velocity'],fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(0,1.1,.25))\n",
    "ax.set_yticklabels(np.arange(0,1.1,.25),fontsize=fontsize-2)\n",
    "ax.legend(['TiDHy','Obs.'],fontsize=fontsize,frameon=False,loc='upper right',bbox_to_anchor=(1.025,1.25),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "ax = axs[-1]\n",
    "acc_obs = accuracy_score(terrain_type_test.reshape(-1),state_compare[0])\n",
    "acc_TiDHy = accuracy_score(terrain_type_test.reshape(-1),state_compare[1])\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(terrain_type_test)))\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+3000,3000))\n",
    "# ax.set_xticklabels(np.arange(0,dt+3000,3000),fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(.5,3,1))\n",
    "ax.set_yticklabels(['Obs. \\n {:02}%'.format(int(np.round(acc_obs*100))),'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=.05)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(terrain_type_test)),1))\n",
    "cbar.set_ticklabels([''.join(terrain_names[n]) for n in range(len(terrain_names))],fontsize=fontsize-2)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "fig.savefig(cfg.paths.fig_dir/'{}_Fig4.pdf'.format(nfig),dpi=300,transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8eed0f",
   "metadata": {},
   "source": [
    "# Load CalMS21 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "cmap_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CalMS21'\n",
    "base_dir = Path('/data/users/eabe/hypernets/{}/TiDHy_{}/'.format(dataset,dataset))\n",
    "configs = sorted(list(base_dir.rglob('*config.yaml'))[::2])\n",
    "\n",
    "for n,conf in enumerate(configs):\n",
    "    print(n,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5bd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "params = OmegaConf.load(default_model_config)\n",
    "cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "for k in cfg.paths.keys():\n",
    "    cfg.paths[k] = Path(cfg.paths[k])\n",
    "    cfg.paths[k].mkdir(parents=True, exist_ok=True)\n",
    "params_curr = cfg.dataset.model\n",
    "cfg.dataset.model = OmegaConf.merge(params, params_curr)\n",
    "# cfg.dataset.model['L1_inf_r2'] = 0\n",
    "# cfg.dataset.model['L1_inf_r'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ce630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.dataset.train.add_basic_features=False\n",
    "cfg.dataset.train.normalize=False\n",
    "data_dict, cfg = load_dataset(cfg)\n",
    "# ##### Convert to float tensors #####\n",
    "# test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "\n",
    "input_dim = test_inputs.shape[-1]\n",
    "test_inputs = test_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "cfg.model.input_dim = input_dim\n",
    "\n",
    "print(f'Our inputs have shape: {test_inputs.shape}')\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=test_inputs.shape[0],pin_memory=True)\n",
    "device = torch.device(\"cuda:{}\".format(cfg.train['gpu']) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de418971",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load weights and params #####\n",
    "rerecord = False\n",
    "epoch = 1000\n",
    "model = TiDHy.TiDHy(cfg.model, device, show_progress=cfg.train.show_progress).to(device)\n",
    "# set_seed(42)\n",
    "load_path = cfg.paths.ckpt_dir/'best.pth.tar'\n",
    "# load_path = sorted(list(cfg.paths.ckpt_dir.rglob('last_{}.pth.tar'.format(epoch))))[0]\n",
    "data_load = torch.load(load_path,map_location=device)\n",
    "model.load_state_dict(data_load['state_dict'])\n",
    "model.to(device)\n",
    "print('Loaded from {} epoch'.format(data_load['epoch']))\n",
    "epoch = data_load['epoch']\n",
    "\n",
    "\n",
    "# result_dict = load_results(model, dataloader_test, data_dict, device, cfg, epoch, rerecord=rerecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b20afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "cfg, result_dict = run_seq_len_model(seq_len, model, data_dict, device, epoch, cfg, rerun=False)\n",
    "\n",
    "data_dict, cfg = load_dataset(cfg)\n",
    "seq_len = sorted([int(el) for el in list(result_dict.keys())])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "p = -1\n",
    "pca = PCA(n_components=3)\n",
    "full_state_z = data_dict['annotations_test']\n",
    "# reg_all = np.concatenate([R2_hat,W,R_bar],axis=1)\n",
    "X_pca = pca.fit_transform(result_dict['{}'.format(seq_len[p])]['R_hat'])\n",
    "n = 0\n",
    "t = 0; dt = len(X_pca)\n",
    "X_pca = X_pca[t:t+dt]\n",
    "comps = X_pca[full_state_z==n][t:t+dt]\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "# ax = Axes3D(fig)\n",
    "ax.scatter(comps[:,0],comps[:,1],comps[:,2],c='r',alpha=.75)\n",
    "ax.scatter(X_pca[:,0],X_pca[:,1],X_pca[:,2],c=clrs_b[p],alpha=.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b18a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = -1\n",
    "pca = PCA(n_components=3)\n",
    "full_state_z = data_dict['annotations_test']\n",
    "# reg_all = np.concatenate([R2_hat,W,R_bar],axis=1)\n",
    "# X_pca = pca.fit_transform(result_dict['{}'.format(seq_len[p])]['R_hat'])\n",
    "X_pca = pca.fit_transform(result_dict['{}'.format(seq_len[p])]['R2_hat'])\n",
    "# X_pca = pca.fit_transform(R_hat)\n",
    "# X_pca = pca.fit_transform(R_bar)\n",
    "# X_pca = pca.fit_transform(reg_all)\n",
    "# X_pca = pca.fit_transform(W[:,np.sum(W,axis=0)!=0])\n",
    "# X_pca = pca.fit_transform(inputs_train)\n",
    "# X_pca = pca.fit_transform(b)\n",
    "# X_pca = pca.fit_transform(Vt.reshape(Vt.shape[0],-1))\n",
    "n = 3\n",
    "t = 0; dt = len(X_pca)\n",
    "fig = go.Figure()\n",
    "comps = X_pca[full_state_z==n][t:t+dt]\n",
    "# labels = full_state_z[full_state_z==n]\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=X_pca[:,0], y=X_pca[:,1], z=X_pca[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color='black',                # set color to an array/list of desired values\n",
    "            colorscale='Viridis',   # choose a colorscale\n",
    "            opacity=.1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=comps[:,0], y=comps[:,1], z=comps[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color='red',                # set color to an array/list of desired values\n",
    "            colorscale='Viridis',   # choose a colorscale\n",
    "            opacity=.01\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=500,\n",
    "    height=500,\n",
    "    autosize=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ab3cf",
   "metadata": {},
   "source": [
    "## Record Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88393e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False\n",
    "ts=1\n",
    "W = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "# b = result_dict['{}'.format(seq_len[ts])]['b'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['b'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "Ut = result_dict['{}'.format(seq_len[ts])]['Ut'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-2],result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-1])\n",
    "##### Plot dynamic matrices #####\n",
    "if cfg.model.low_rank_temp:\n",
    "    Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "else:\n",
    "    Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "full_state_z = data_dict['annotations_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25508806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "reg_variables = [np.concatenate([R2_hat,W,R_hat],axis=-1),R2_hat,W,R_hat,R_bar,I]\n",
    "labels = ['all','R2_hat','W','R_hat','R_bar','I']\n",
    "\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "    else:\n",
    "        I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, best_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d15570",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=13\n",
    "fps = 30\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.5))\n",
    "gs  = gridspec.GridSpec(nrows=8, ncols=4,hspace=10,wspace=.5) \n",
    "gs0 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[:3,:],  wspace=.5,hspace=.2)\n",
    "# gsb = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:3,2:3], wspace=.1,hspace=.5)\n",
    "\n",
    "# gsc = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:5,3:], wspace=.5,hspace=.1)\n",
    "gs1 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[3:6,:], wspace=.2,hspace=.1)\n",
    "\n",
    "gs2 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[6:,:-1],   wspace=0, hspace=.10)\n",
    "\n",
    "##### plotting video frame ######\n",
    "t = 50; dt = 1\n",
    "skeleton = np.array([[0,1],[0,2],[2,3],[1,3],[3,4],[3,5],[4,6],[5,6]])\n",
    "ani1_x = data_dict['inputs_train'][t:t+dt,:7]\n",
    "ani1_y = data_dict['inputs_train'][t:t+dt,7:14]\n",
    "ani2_x = data_dict['inputs_train'][t:t+dt,14:21]\n",
    "ani2_y = data_dict['inputs_train'][t:t+dt,21:28]\n",
    "I1_x = Ihat[t:t+dt,:7]\n",
    "I1_y = Ihat[t:t+dt,7:14]\n",
    "I2_x = Ihat[t:t+dt,14:21]\n",
    "I2_y = Ihat[t:t+dt,21:28]\n",
    "labels = data_dict['annotations_train'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "axs = np.array([fig.add_subplot(gs0[:2])])\n",
    "ax = axs[0]\n",
    "# ax.imshow(frames[t]) ##### Load frames from video in cell below\n",
    "for n in range(7):\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=10,c='#ffa600ff')\n",
    "    im = ax.scatter(ani2_x[:,n],ani2_y[:,n],s=10,c='#4daf50ff')\n",
    "tt=0\n",
    "for n in range(len(skeleton)):\n",
    "    ax.plot(ani1_x[:,skeleton[n]].squeeze(),ani1_y[:,skeleton[n]].squeeze(),'#ffa600ff',lw=2,zorder=1)\n",
    "    ax.plot(ani2_x[:,skeleton[n]].squeeze(),ani2_y[:,skeleton[n]].squeeze(),'#4daf50ff',lw=2,zorder=1)\n",
    "ax.axis('off')\n",
    "\n",
    "###### Plotting Observations #####\n",
    "axs = np.array([fig.add_subplot(gs0[2])])\n",
    "ax = axs[0]\n",
    "spacing = .75; fontsize=13\n",
    "t = 0; dt = 10000\n",
    "hlines,hlines_Ihat= [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    mean_centered = mean_centered\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered + n/spacing,color='k', lw=1)\n",
    "    hlines.append(np.mean(mean_centered + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered_Ihat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=0)\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "ax.set_ylabel(\"observations\",fontsize=fontsize)\n",
    "ax.set_yticks([])\n",
    "\n",
    "##### Timescales #####\n",
    "axs = np.array([fig.add_subplot(gs0[3])])\n",
    "ax = axs[0]\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "        \n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))/fps\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xscale('symlog',linthresh=100)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=0)\n",
    "ax.set_xticks([10e0,10e1,10e2,10e3,10e4,10e5])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=0)\n",
    "ax.set_xlim([-1,7e4])\n",
    "\n",
    "###### PCA #####\n",
    "n = 0; m = 1\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(R_bar.reshape(-1,R_hat.shape[-1]))\n",
    "lst = list(itertools.combinations(np.arange(X_pca.shape[-1]), 2))\n",
    "unq = set(lst)\n",
    "axs = np.array([fig.add_subplot(gs1[n]) for n in range(4)])\n",
    "for state in range(4):\n",
    "    ax = axs[state]\n",
    "    comps = X_pca[full_state_z==state]\n",
    "    comps = comps/np.max(np.abs(comps),axis=0)\n",
    "    # comps = X_pca[(full_state_z==0) | (full_state_z==1) | (full_state_z==2)]\n",
    "    h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=50,range=[[-1,1],[-1,1]],density=True)\n",
    "    im = ax.imshow((h).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_title('{}'.format(list(data_dict['vocabulary'].keys())[state]),fontsize=fontsize,y=.95)\n",
    "    ax.set_xlabel('PC{}'.format(n+1),fontsize=fontsize)\n",
    "    if (state==0):\n",
    "        ax.set_ylabel('PC{}'.format(m+1),fontsize=fontsize)\n",
    "ax = axs[-1]\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10,shrink=.5)\n",
    "cbar.set_ticks([0,np.max(h)])\n",
    "cbar.set_ticklabels(['low','high'],rotation=90)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "\n",
    "##### Behaviroal State ID #####\n",
    "t = 80000; dt = 10000; \n",
    "acc_TiDHy=accuracy_score(best_pred, full_state_z)\n",
    "# acc_TiDHy=neigh.score(I_pred, full_state_z)\n",
    "state_compare = np.stack([y_pred,full_state_z],axis=0)\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "axs = np.array([fig.add_subplot(gs2[n]) for n in range(1)])\n",
    "ax = axs[0]\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_yticks(np.arange(.5,2,1))\n",
    "ax.set_yticklabels(['TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt,60*fps))\n",
    "ax.set_xticklabels((np.arange(0,dt,60*fps)*1/fps).astype(int),fontsize=fontsize-2)\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "\n",
    "# plt.tight_layout()\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10, pad=.01)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()),fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "# fig.savefig(cfg.paths.fig_dir / 'Fig5.pdf',dpi=300,transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e262598",
   "metadata": {},
   "source": [
    "### Load video frames for CalMS21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/data/users/eabe/hypernets/CalMS21/datasets/mouse001_task1_annotator1.mp4')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    " \n",
    "# Read until video is completed\n",
    "frames = []\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    frames.append(frame)   \n",
    "  # Break the loop\n",
    "  else: \n",
    "    break\n",
    " \n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41097339",
   "metadata": {},
   "source": [
    "## Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41097339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import cv2\n",
    "import gc\n",
    "mpl.use('agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_plt_im(t, X_pca,data_dict,full_state_z):   #\n",
    "    trail = 5\n",
    "    labels = full_state_z[t-trail:t+1]\n",
    "    cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "    bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "    ax = axs[0]\n",
    "    im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "    im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1,1])\n",
    "    ax.set_ylim([-1,1])\n",
    "    cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "    cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "    cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "    ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "    ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "    ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "    ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "    ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "    labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "    for n in range(7):\n",
    "        ax = axs[1]\n",
    "        im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 1')\n",
    "        ax = axs[2]\n",
    "        ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    images = np.frombuffer(fig.canvas.tostring_rgb(),\n",
    "                        dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    plt.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = make_plt_im(t, X_pca,data_dict)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# full_state_z = data_dict['annotations_test']\n",
    "full_state_z = cluster_labels\n",
    "X_pca = pca.fit_transform(R_bar)\n",
    "\n",
    "t = 80000; dt = 5000\n",
    "trail = 5\n",
    "labels = full_state_z[t-trail:t+1]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig(cfg.paths.fig_dir/'test.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(cluster_labels==2)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "##### initialize time points for animation and progressbar #####\n",
    "t = 0 ; dt = 10000\n",
    "full_state_z = cluster_labels\n",
    "state = 'I'\n",
    "time_range = np.arange(t,t+dt)#  np.where(cluster_labels==state)[0]#\n",
    "num_ticks = np.size(time_range)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(I)\n",
    "\n",
    "##### Put large arrays into shared memory #####\n",
    "X_pca_r = ray.put(X_pca)\n",
    "data_dict_r = ray.put(data_dict)\n",
    "full_state_z_r = ray.put(full_state_z)\n",
    "##### Loop over parameters appending process ids #####\n",
    "result_ids = []\n",
    "for t in time_range:\n",
    "    result_ids.append(make_plt_im.remote(t, X_pca_r, data_dict_r,full_state_z_r))\n",
    "\n",
    "##### pring progressbar and get results #####\n",
    "results_p = ray.get(result_ids)\n",
    "images = np.stack([results_p[i] for i in range(len(results_p))])\n",
    "\n",
    "##### Make video with opencv #####\n",
    "aniname = 'PCA_Evolve_{}.mp4'.format(state) \n",
    "\n",
    "\n",
    "vid_name = cfg.paths.fig_dir / aniname\n",
    "FPS = 30\n",
    "out = cv2.VideoWriter(vid_name.as_posix(), cv2.VideoWriter_fourcc(*'mp4v'), FPS, (images.shape[-2], images.shape[-3]))\n",
    "\n",
    "for fm in tqdm(range(images.shape[0])):\n",
    "    out.write(cv2.cvtColor(images[fm], cv2.COLOR_BGR2RGB))\n",
    "out.release()\n",
    "print('Making Animation {}: {}'.format(aniname, time.time()-start))\n",
    "del results_p, X_pca_r, data_dict_r, full_state_z_r\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 80000 ; dt = 5000\n",
    "labels = full_state_z[t:t+dt]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[t:t+dt,0],X_pca[t:t+dt,1],s=1,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,4,1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t:t+dt,:7]\n",
    "ani1_y = data_dict['inputs_test'][t:t+dt,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t:t+dt,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t:t+dt,21:28]\n",
    "labels = data_dict['annotations_test'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.axis('square')\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TiDHy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
