# @package _global_

dataset:
  name: Rossler
  version: HierarchicalMultiTimescale
  rossler_params:
    # Time parameters
    time_bins_train: 200000
    time_bins_test: 50000
    dt: 0.01
    transient_steps: 1000

    # Slow Rossler system parameters (slower dynamics)
    a_slow: 0.2
    b_slow: 0.2
    c_slow: 3.0  # Lower c value for slower oscillations

    # Fast Rossler system base parameters (faster dynamics)
    a_fast: 0.2
    b_base: 0.2
    c_base: 5.7  # Standard Rossler c for chaotic behavior

    # Coupling parameters (how strongly slow modulates fast)
    # Note: Modulated parameters are clipped to stable ranges:
    # - b_fast is clipped to [0.01, 1.0]
    # - c_fast is clipped to [2.0, 10.0]
    # This prevents numerical instability while preserving multi-timescale dynamics
    coupling_b: 0.5  # Modulation strength for b parameter
    coupling_c: 1.0  # Modulation strength for c parameter

    # Observation parameters
    # State ordering: [xs, ys, zs, xf, yf, zf] = [0, 1, 2, 3, 4, 5]
    # Default: observe [xf, yf, zs] = [3, 4, 2] (fast x,y + slow z)
    observed_states: [0,1,2,3,4,5]
    rand_proj: true  # Use random projection for observations
    rand_proj_dim: 16  # Dim of random projection if used
    # Data processing
    noise_level: 0.1  # Observation noise std dev (0 = no noise)
    normalize: true
    seed: 42

# model:
#   # Override specific parameters for Rossler dataset
#   grad_norm_inf: false
#   grad_alpha_inf: 1.0
#   cos_eta: 5.0
#   # Regularization parameters
#   L1_alpha: ${resolve_default:0.0,${L1_alpha}}
#   L1_alpha_inf: ${resolve_default:0.0,${L1_alpha_inf}}
#   L1_alpha_r2: 0.0
#   L1_inf_r2: 0.0
#   L1_inf_w: 0.0
#   L1_inf_r: 0.0
#   lmda_r: 0.0
#   lmda_r2: 0.0
#   # Combined sparsity regularization settings
#   sparsity_r_l1: 0.01          # Mild L1 for r
#   sparsity_r2_adaptive: 0.0     # DISABLED - was causing exponential loss growth when model overshoots target sparsity
#   sparsity_r2_l1: 0.005         # Static L1 regularization for r2 (safer than adaptive)
#   sparsity_w_safe: 0.008        # SAFE hypernetwork sparsity
#   target_sparsity_r2: 0.85
#   sparsity_temperature: 1.5     # Temperature for adaptive sparsity (reduced from 1.5 to prevent loss spikes)
#   w_min_active_ratio: 1.0
#   weight_decay: 0.0001
#   # Training continuity (cross-window overlap)
#   r2_continuity_weight: 0.05    # Enforce r2 continuity in overlapping regions during training (reduced from 0.2 to prevent conflicts with sparsity)


train:
  wandb_project: "TiDHy_eabe"
  device_id: null  # null = use all GPUs, int = specific GPU ID
  num_epochs: 1000
  sequence_length: 500
  overlap_factor: 5
  stack_inputs: true
  validation_cooldown: 25
  show_progress: false
  show_inf_progress: false
  test: false
  batch_size_input: true
  batch_size: null
  ##### Learning rates for different components #####
  # Multi-LR training
  use_schedule: true  # Enable learning rate scheduling
  schedule_transition_steps: 200  # Steps before applying decay
  schedule_decay: 0.96  # Learning rate decay factor (increased from 0.99 for faster decay)
  learning_rate_s: 0.002  # Spatial decoder learning rate (reduced from 0.01)
  learning_rate_t: 0.002  # Temporal parameters learning rate (reduced from 0.01)
  learning_rate_h: 0.0005  # Hypernetwork learning rate (reduced from 0.001) 
  # Single LR fallback (if multi-LR not specified)
  learning_rate: 1e-3
  lr_weights: 0.01  # GradNorm settings
  grad_norm: false  # Use GradNorm for loss balancing
  grad_alpha: 0.5
  weight_decay: 0.00001
  ##### Logging settings #####
  log_params_every: 10
  log_sparsity_every: 5

delay:
  delay_embed: false
  delay_tau: 3
  delay_pcs: true
  skipt: 15
