# @package _global_

defaults:
  - model/sparsity@model
  - _self_

dataset:
  name: AnymalTerrain
  version: Debug
  single_ani: false
  train_size: 500
  test_size: 50

nenvs: 50

model:
  # Override specific parameters for AnymalTerrain dataset
  grad_norm_inf: false
  grad_alpha_inf: 1.0
  cos_eta: 5.0
  # Regularization parameters
  L1_alpha: ${resolve_default:0.0,${L1_alpha}}
  L1_alpha_inf: ${resolve_default:0.0,${L1_alpha_inf}}
  L1_alpha_r2: 0.0
  L1_inf_r2: 0.0
  L1_inf_w: 0.0
  L1_inf_r: 0.0
  lmda_r: 0.0
  lmda_r2: 0.0
  # Combined sparsity regularization settings
  sparsity_r_l1: 0.005          # Mild L1 for r
  sparsity_r2_adaptive: 0.015   # Adaptive for precise r2 control  
  sparsity_w_safe: 0.008        # SAFE hypernetwork sparsity
  target_sparsity_r2: 0.8
  sparsity_temperature: 1.5
  w_min_active_ratio: 0.3
  weight_decay: 0.00001


train:
  wandb_project: "TiDHy_eabe"
  num_epochs: 1000
  sequence_length: 200
  overlap_factor: 2
  stack_inputs: true
  save_summary_steps: 100
  show_progress: true
  show_inf_progress: false
  test: false
  batch_size_input: true
  batch_size: null
  normalize_obs: true
  ##### Learning rates for different components #####
  # Multi-LR training
  use_schedule: true  # Enable learning rate scheduling
  schedule_transition_steps: 100  # Steps before applying decay
  schedule_decay: 0.96  # Learning rate decay factor
  learning_rate_s: 0.01  # Spatial decoder learning rate
  learning_rate_t: 0.01  # Temporal parameters learning rate  
  learning_rate_h: 0.001  # Hypernetwork learning rate
  # Single LR fallback (if multi-LR not specified)
  learning_rate: 1e-3
  lr_weights: 0.01  # GradNorm settings
  grad_norm: false  # Use GradNorm for loss balancing
  grad_alpha: 0.5
  weight_decay: 0.00001
  ##### Logging settings #####
  log_params_every: 10
  log_sparsity_every: 5

delay:
  delay_embed: false
  delay_tau: 3
  delay_pcs: true
  skipt: 15