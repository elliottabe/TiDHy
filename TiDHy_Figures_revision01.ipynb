{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0e1841-1962-46c0-a629-11e884c6c99c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd955540",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Use GPU 0\n",
    "os.environ[\"JAX_CAPTURED_CONSTANTS_REPORT_FRAMES\"]=\"-1\"\n",
    "from pathlib import Path\n",
    "import jax \n",
    "jax.config.update(\"jax_compilation_cache_dir\", (Path.cwd() / \"tmp/jax_cache\").as_posix())\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)\n",
    "try:\n",
    "    jax.config.update(\"jax_persistent_cache_enable_xla_caches\", \"xla_gpu_per_fusion_autotune_cache_dir\")\n",
    "except AttributeError:\n",
    "    pass  # Skip if not available in this JAX version\n",
    "\n",
    "try:\n",
    "    import blackjax\n",
    "except ModuleNotFoundError:\n",
    "    print('installing blackjax')\n",
    "    %pip install -qq blackjax\n",
    "    import blackjax\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "# from fastprogress.fastprogress import progress_bar\n",
    "from functools import partial\n",
    "\n",
    "# jax.config.update('jax_platform_name', 'cpu')\n",
    "from jax import random as  jr\n",
    "from jax import numpy as jnp\n",
    "from jax import jit, vmap\n",
    "from itertools import count\n",
    "from flax import nnx\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# device = 'gpu' if jax.lib.xla_bridge.get_backend().platform == 'gpu' else 'cpu'\n",
    "device = 'gpu' if jax.extend.backend.get_backend().platform == 'gpu' else 'cpu'\n",
    "n_gpus = jax.device_count(backend=device)\n",
    "print(f\"Using {n_gpus} device(s) on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eaebf-52c1-4b95-a6d9-34a1b1525b1b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from TiDHy.models.TiDHy_nnx_vmap import TiDHy\n",
    "from TiDHy.models.TiDHy_nnx_vmap_training import train_model, evaluate_record, load_model, get_latest_checkpoint_epoch\n",
    "from TiDHy.datasets.datasets_dynamax import *\n",
    "from TiDHy.datasets.load_data import load_data, stack_data\n",
    "from TiDHy.utils import io_dict_to_hdf5 as ioh5\n",
    "from TiDHy.utils.path_utils import *\n",
    "\n",
    "##### Plotting settings ######\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size':          10,\n",
    "                     'axes.linewidth':     2,\n",
    "                     'xtick.major.size':   5,\n",
    "                     'ytick.major.size':   5,\n",
    "                     'xtick.major.width':  2,\n",
    "                     'ytick.major.width':  2,\n",
    "                     'axes.spines.right':  False,\n",
    "                     'axes.spines.top':    False,\n",
    "                     'pdf.fonttype':       42,\n",
    "                     'ps.fonttype':        42,\n",
    "                     'xtick.labelsize':    10,\n",
    "                     'ytick.labelsize':    10,\n",
    "                     'figure.facecolor':   'white',\n",
    "                     'pdf.use14corefonts': True,\n",
    "                     'font.family':        'sans-serif',\n",
    "                    #  'font.family':        'Arial',\n",
    "                    #  'font.sans-serif':    'Arial',\n",
    "                     'font.serif':         'Arial',\n",
    "                    })\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "clrs = ['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000']\n",
    "cmap = ListedColormap(clrs)\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def map_discrete_cbar(cmap,N):\n",
    "    cmap = plt.get_cmap(cmap,N+1)\n",
    "    bounds = np.arange(-.5,N+1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    return cmap, norm\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24dc44",
   "metadata": {},
   "source": [
    "# Rossler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c186d3",
   "metadata": {},
   "source": [
    "## Load and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Rossler'\n",
    "version = 'HierarchicalMultiTimescale'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 2#7 #22 #2 #8\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd().parent / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg.dataset.name}: {cfg.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data(cfg)\n",
    "inputs_train = stack_data(data_dict['inputs_train'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "inputs_test = stack_data(data_dict['inputs_test'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ca81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNG\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Get model params as dict and unpack directly\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "# model_params.pop('batch_converge')\n",
    "model_params['input_dim'] = inputs_train.shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "model = TiDHy(**model_params, rngs=rngs)\n",
    "# model.l0 = nnx.data(jnp.zeros(3))\n",
    "# model.loss_weights = nnx.data(jnp.ones(3))\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"input_dim: {model.input_dim}, r_dim: {model.r_dim}, r2_dim: {model.r2_dim}, mix_dim: {model.mix_dim}\")\n",
    "jit_model = jax.jit(model)\n",
    "# out = jit_model(inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=get_latest_checkpoint_epoch(cfg.paths.ckpt_dir)\n",
    "loaded_model = load_model(model,cfg.paths.ckpt_dir/f'epoch_{epoch:04d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6cfb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "# seq_len = [100,200,500,1000,5000,10000]\n",
    "seq_len = [50000]\n",
    "for seq in tqdm(seq_len):\n",
    "    inputs_test = stack_data(data_dict['inputs_test'], sequence_length=seq, overlap=seq)\n",
    "    rng_key = model.rngs()\n",
    "    spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict_single = evaluate_record(loaded_model, inputs_test, rng_key)\n",
    "    result_dict['{}'.format(seq)] = result_dict_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])\n",
    "Ut = jnp.stack([result_dict[str(seq)]['Ut'].reshape((-1,)+ result_dict[str(seq)]['Ut'].shape[2:]) for seq in seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cuml.accel\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca = CCA(n_components=6)\n",
    "Y = data_dict['states_x_test']\n",
    "X = R_hat[0]\n",
    "pred_x,pred_y = cca.fit_transform(X,Y)\n",
    "\n",
    "n_comps=3\n",
    "cca = CCA(n_components=3,max_iter=1000)\n",
    "X_c,Y_c = cca.fit_transform(data_dict['states_x_test'][:,:3],  R_hat[0])\n",
    "cca_coefficient = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=n_comps)\n",
    "x_w = cca.x_weights_\n",
    "y_w = cca.y_weights_\n",
    "cca_angles = [np.rad2deg(angle_between(X_c[:,n],Y_c[:,n])) for n in range(n_comps)]\n",
    "cca_angles_x = [np.rad2deg(angle_between(X_c[:,n],data_dict['states_x_test'][:,n])) for n in range(n_comps)]\n",
    "cca_angles_r = [np.rad2deg(angle_between(Y_c[:,n],R_hat[0,:,n])) for n in range(n_comps)]\n",
    "for n in range(n_comps):\n",
    "    print('comp {}, cc: {:.03}, ang: {:.03}, ang_x:{:.03}, ang_r:{:.03}'.format(n,cca_coefficient[n],cca_angles[n],cca_angles_x[n],cca_angles_r[n]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd85e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot(data_dict['states_x_test'][:,0],\n",
    "        data_dict['states_x_test'][:,1],\n",
    "        data_dict['states_x_test'][:,2],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot(data_dict['states_x_test'][:,3],\n",
    "        data_dict['states_x_test'][:,4],\n",
    "        data_dict['states_x_test'][:,5],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6c386",
   "metadata": {},
   "source": [
    "## Explore plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.utils import (\n",
    "    compute_rossler_ground_truth_timescales,\n",
    "    compare_discovered_to_ground_truth,\n",
    "    analyze_hierarchical_rossler_recovery,\n",
    "    plot_discovered_vs_ground_truth,\n",
    "    analyze_effective_timescales\n",
    ")\n",
    "\n",
    "# Option 2: All-in-one with all three methods\n",
    "# ============================================\n",
    "rossler_params = cfg.dataset.rossler_params\n",
    "dt = cfg.dataset.rossler_params.dt\n",
    "recovery_analysis = analyze_hierarchical_rossler_recovery(\n",
    "    model=model,\n",
    "    test_data=inputs_test,\n",
    "    rossler_trajectory=I[-1],\n",
    "    rossler_params=rossler_params,\n",
    "    dt=dt,\n",
    "    rng_key=jax.random.PRNGKey(42),\n",
    "    tolerance=0.2\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(\"\\nBasis method:\")\n",
    "print(f\"  Recovery: {recovery_analysis['basis_comparison']['recovery_rate']*100:.1f}%\")\n",
    "\n",
    "print(\"\\nEffective method:\")\n",
    "if recovery_analysis['effective_comparison']:\n",
    "    print(f\"  Recovery: {recovery_analysis['effective_comparison']['recovery_rate']*100:.1f}%\")\n",
    "\n",
    "print(\"\\nWeighted method:\")\n",
    "if recovery_analysis['weighted_comparison']:\n",
    "    print(f\"  Recovery: {recovery_analysis['weighted_comparison']['recovery_rate']*100:.1f}%\")\n",
    "\n",
    "# Compare all methods in one plot\n",
    "comparisons_dict = {\n",
    "    'Basis': recovery_analysis['basis_comparison'],\n",
    "    'Effective': recovery_analysis['effective_comparison'],\n",
    "    'Weighted': recovery_analysis['weighted_comparison']\n",
    "}\n",
    "fig, axes = plot_discovered_vs_ground_truth(comparisons_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.utils.analysis import analyze_latent_dimension\n",
    "from TiDHy.utils.analysis_plotting import plot_dimensionality_analysis\n",
    "\n",
    "R_hat = result_dict_single['R_hat'].reshape(-1, result_dict_single['R_hat'].shape[-1])\n",
    "\n",
    "# Analyze effective dimensionality\n",
    "dim_analysis = analyze_latent_dimension(R_hat, method='pca')\n",
    "\n",
    "print(f\"Total dimensions: {dim_analysis['total_dims']}\")\n",
    "print(f\"Effective dim (95%): {dim_analysis['effective_dim_95']}\")\n",
    "print(f\"Effective dim (99%): {dim_analysis['effective_dim_99']}\")\n",
    "print(f\"Participation ratio: {dim_analysis['participation_ratio']:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plot_dimensionality_analysis(dim_analysis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.utils.analysis import analyze_mixture_timescales\n",
    "from TiDHy.utils.analysis_plotting import plot_timescale_distribution\n",
    "\n",
    "# Analyze timescales from trained model\n",
    "timescale_analysis = analyze_mixture_timescales(model, dt=0.01)\n",
    "\n",
    "print(f\"Discovered timescale range: {timescale_analysis['timescale_range']}\")\n",
    "print(f\"Unique timescales: {len(timescale_analysis['unique_timescales'])}\")\n",
    "print(f\"Stable modes per component: {timescale_analysis['n_stable_modes']}\")\n",
    "\n",
    "# Visualize timescale distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_timescale_distribution(timescale_analysis, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Plot eigenvalues in complex plane for each component\n",
    "from TiDHy.utils import plot_complex_eigenvalues\n",
    "eigenvalues_list = [spec['eigenvalues'] for spec in timescale_analysis['spectral_results']]\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_complex_eigenvalues(eigenvalues_list, ax=ax,\n",
    "                        component_labels=[f'V_{i}' for i in range(model.mix_dim)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec83c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 3: Analyze Hypernetwork Usage\n",
    "\n",
    "from TiDHy.utils import analyze_hypernetwork_usage, plot_hypernetwork_usage\n",
    "\n",
    "W = result_dict_single['W'].reshape(-1, result_dict_single['W'].shape[-1])\n",
    "\n",
    "# Analyze which components are used\n",
    "usage_analysis = analyze_hypernetwork_usage(W, threshold=0.01)\n",
    "\n",
    "print(f\"Active components: {usage_analysis['active_components']}/{usage_analysis['mix_dim']}\")\n",
    "print(f\"Entropy (normalized): {usage_analysis['entropy']:.3f}\")\n",
    "print(f\"Sparsity: {usage_analysis['sparsity']:.2%}\")\n",
    "print(f\"Dominant component: {usage_analysis['dominant_component']}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "plot_hypernetwork_usage(W, ax=axes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b221eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 4: Combined Model Selection Workflow\n",
    "\n",
    "from TiDHy.utils import (\n",
    "    analyze_latent_dimension,\n",
    "    analyze_mixture_timescales,\n",
    "    analyze_hypernetwork_usage\n",
    ")\n",
    "\n",
    "# 1. Determine effective r_dim\n",
    "R_analysis = analyze_latent_dimension(result_dict_single['R_hat'])\n",
    "effective_r_dim = R_analysis['effective_dim_95']\n",
    "\n",
    "# 2. Determine effective r2_dim  \n",
    "R2_analysis = analyze_latent_dimension(result_dict_single['R2_hat'])\n",
    "effective_r2_dim = R2_analysis['effective_dim_95']\n",
    "\n",
    "# 3. Discover number of timescales (effective mix_dim)\n",
    "timescale_analysis = analyze_mixture_timescales(model, dt=0.01)\n",
    "n_unique_timescales = len(timescale_analysis['unique_timescales'])\n",
    "\n",
    "# 4. Verify with hypernetwork usage\n",
    "usage_analysis = analyze_hypernetwork_usage(result_dict_single['W'])\n",
    "active_components = usage_analysis['active_components']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL SELECTION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Recommended r_dim:   {effective_r_dim} (current: {model.r_dim})\")\n",
    "print(f\"Recommended r2_dim:  {effective_r2_dim} (current: {model.r2_dim})\")\n",
    "print(f\"Recommended mix_dim: {active_components} (current: {model.mix_dim})\")\n",
    "print(f\"Discovered timescales: {n_unique_timescales}\")\n",
    "print(f\"Timescale range: {timescale_analysis['timescale_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0abb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = -1\n",
    "# plt.plot(R2_hat[:,:10000,0].T)\n",
    "# plt.plot(R_hat[:10000])\n",
    "plt.plot(R2_hat[ts,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(W[:10000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5584b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1,figsize=(6,4))\n",
    "ax = axs[0]\n",
    "ax.plot(I[ts,:10000,0],label='I1')\n",
    "ax.plot(Ihat[ts,:10000,0],label='Ihat1')\n",
    "ax = axs[1]\n",
    "ax.plot(R_hat[:10000,:],label='Ihat3')\n",
    "# ax.plot(Ihat[ts,:10000,2],label='Ihat3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(R_hat[:,1],\n",
    "        R_hat[:,4],\n",
    "        R_hat[:,5],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(data_dict['inputs_train'][:,0],\n",
    "        data_dict['inputs_train'][:,1],\n",
    "        data_dict['inputs_train'][:,2],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(data_dict['states_x_train'][:,0],\n",
    "        data_dict['states_x_train'][:,1],\n",
    "        data_dict['states_x_train'][:,2],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(data_dict['states_x_train'][:,3],\n",
    "        data_dict['states_x_train'][:,4],\n",
    "        data_dict['states_x_train'][:,5],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df481e9",
   "metadata": {},
   "source": [
    "# Load SLDS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34475724",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'SLDS'\n",
    "version = 'Debug'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 10#7 #22 #2 #8\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd().parent / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg.dataset.name}: {cfg.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda16cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_params = cfg.dataset.ssm_params\n",
    "lds_dict, data_dict = partial_superposition_SLDS(cfg, ssm_params)\n",
    "inputs_train = stack_data(data_dict['inputs_train'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "inputs_val = stack_data(data_dict['inputs_val'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)\n",
    "inputs_test = stack_data(data_dict['inputs_test'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNG\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Get model params as dict and unpack directly\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "# model_params.pop('batch_converge')\n",
    "model_params['input_dim'] = inputs_train.shape[-1]\n",
    "# model_params['show_inf_progress'] = False\n",
    "# model_params['max_iter'] = 1000\n",
    "# model_params['lr_r'] = 7.5e-3\n",
    "# model_params['lr_r2'] = 7.5e-3\n",
    "\n",
    "\n",
    "model = TiDHy(**model_params, rngs=rngs)\n",
    "# model.l0 = nnx.data(jnp.zeros(3))\n",
    "# model.loss_weights = nnx.data(jnp.ones(3))\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"input_dim: {model.input_dim}, r_dim: {model.r_dim}, r2_dim: {model.r2_dim}, mix_dim: {model.mix_dim}\")\n",
    "jit_model = jax.jit(model)\n",
    "# out = jit_model(inputs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb351ace",
   "metadata": {},
   "source": [
    "## Test Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_single(seq):\n",
    "    return model(seq, return_internals=True)\n",
    "\n",
    "# in_axes=0 means vmap over first dimension (batch)\n",
    "vmapped_forward = jax.vmap(forward_single, in_axes=0)\n",
    "# batch_losses = vmapped_forward(inputs_train)\n",
    "out, internals = forward_single(inputs_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14829f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, history = train_model(\n",
    "    model,\n",
    "    inputs_train,\n",
    "    n_epochs=250,\n",
    "    learning_rate_s=0.01,  # Spatial decoder learning rate\n",
    "    learning_rate_t=0.01,  # Temporal parameters learning rate  \n",
    "    learning_rate_h=0.001,  # Hypernetwork learning rate\n",
    "    schedule_transition_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=None,\n",
    "    use_gradnorm=False,\n",
    "    val_data=inputs_val,\n",
    "    val_every_n_epochs=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.ylim(0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict = evaluate_record(model, inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = inputs_test.reshape(-1, model.input_dim)\n",
    "Ihat = result_dict['I_hat'].reshape(-1, model.input_dim)\n",
    "Rhat = result_dict['R_hat'].reshape(-1, model.r_dim)\n",
    "Rbar = result_dict['R_bar'].reshape(-1, model.r_dim)\n",
    "R2hat = result_dict['R2_hat'].reshape(-1, model.r2_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70abe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_state = nnx.split(model)\n",
    "model_dict = nnx.to_pure_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5192656",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws = model_dict['temporal'].reshape(model.mix_dim, model.r_dim, model.r_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e347ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5, figsize=(10,6))\n",
    "axs = axs.flatten()\n",
    "for n in range(model.mix_dim):\n",
    "    axs[n].imshow(Ws[n], cmap='bwr', vmin=-jnp.max(jnp.abs(Ws[n])), vmax=jnp.max(jnp.abs(Ws[n])))\n",
    "    axs[n].set_title(f'W[{n}]')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e626aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model_dict['spatial_decoder']['dense']['kernel'], cmap='bwr', aspect='auto', vmin=-0.5, vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ee27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(I[:200,5], label='I', c='k')\n",
    "# plt.plot(Ihat[:200,5], label='Ihat', c='r')\n",
    "# plt.plot(Rhat[:200,])\n",
    "# plt.plot(Rbar[:200,])\n",
    "plt.plot(R2hat[:1000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4585883",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws = result_dict['W'].reshape(-1,model.mix_dim)\n",
    "plt.plot(Ws[:1000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = nnx.state(model, nnx.Param)\n",
    "\n",
    "nnx.to_pure_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = nnx.state(model, nnx.Param)\n",
    "model_state = nnx.to_pure_dict(model_state)\n",
    "# Group parameters by component\n",
    "spatial_params = {k: v for k, v in model_state.items() if 'spatial_decoder' in str(k)}\n",
    "temporal_params = {k: v for k, v in model_state.items() if 'temporal' in str(k)}\n",
    "hyper_params = {k: v for k, v in model_state.items() if 'hypernet' in str(k)}\n",
    "\n",
    "spatial_norm = float(jnp.sqrt(sum(jnp.sum(v ** 2) for v in spatial_params.values())))\n",
    "spatial_mean = float(jnp.mean(jnp.concatenate([v.flatten() for v in spatial_params.values()])))\n",
    "spatial_std = float(jnp.std(jnp.concatenate([v.flatten() for v in spatial_params.values()])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237cc8ac",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=get_latest_checkpoint_epoch(cfg.paths.ckpt_dir)\n",
    "loaded_model = load_model(model,cfg.paths.ckpt_dir/f'epoch_{epoch:04d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a927a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "# seq_len = [100,200,500,1000]\n",
    "seq_len = [50000]\n",
    "for seq in tqdm(seq_len):\n",
    "    inputs_test = stack_data(data_dict['inputs_test'], sequence_length=seq, overlap=seq)\n",
    "    rng_key = model.rngs()\n",
    "    spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict_single = evaluate_record(loaded_model, inputs_test, rng_key)\n",
    "    result_dict['{}'.format(seq)] = result_dict_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a74590",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df885c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[8,8,1,1,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "# cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b\n",
    "import itertools\n",
    "states_x_test = data_dict['states_x_test']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "# states_z_test = data_dict['states_z']\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "##### Set up combinatorics of timescales #####\n",
    "lst = list(itertools.product([1, 0], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros(ssm_params['time_bins_test'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,states_z_test)] = n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = -1\n",
    "feature = 0\n",
    "T_max = 1000\n",
    "plt.plot(I[ts,:T_max, feature], color=clr2b[0], label='Input current I')\n",
    "# plt.plot(Ihat[ts,:T_max, feature], color=clr2b[1], label='Reconstructed current Î')\n",
    "# plt.plot(Ibar[ts,:T_max, feature], color=clr2b[2], label='Smoothed current Ī')\n",
    "plt.xlabel('Time steps', fontsize=fontsize)\n",
    "plt.ylabel('Input current', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "ts = 1\n",
    "feature = 0\n",
    "T_max = 1000\n",
    "fig,axs = plt.subplots(1,1, figsize=(6,4))\n",
    "ax = axs\n",
    "# ax.plot(R_bar[ts,:T_max, feature], color=clr2b[0])\n",
    "# ax.plot(R_hat[ts,:T_max, feature], color=clr2b[1])\n",
    "ax.plot(R2_hat[ts,:T_max, :], color=clr2b[2])\n",
    "ax.set_xlabel('Time steps', fontsize=fontsize)\n",
    "ax.set_ylabel('Input current', fontsize=fontsize)\n",
    "# plt.legend(fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_results = analyze_effective_timescales(result_dict_single, dt=dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ea70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "from sklearn.cluster import HDBSCAN, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0cc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(R_hat[0])\n",
    "umap = cuml.manifold.UMAP(n_components=6, n_neighbors=50, min_dist=0.75, metric='euclidean')\n",
    "reduced_data = umap.fit_transform(X_scaled)\n",
    "\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=100, min_samples=20, cluster_selection_epsilon=0.25, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterer.labels_\n",
    "\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_label = jnp.argmax(soft_clusters[:,1:], axis=1)\n",
    "r_data = reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data.min(), r_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(r_data[:, 0], r_data[:, 1], s=2,  c=full_state_z, cmap=cmap_b, alpha=0.05)\n",
    "plt.axis((-10,10,-10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf87fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33651383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "comp = pca.fit_transform(R2_hat[0])\n",
    "subset = comp[labels!=-1,:]\n",
    "fig, axs = plt.subplots(2,2,figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "for n in range(len(axs)):\n",
    "    ax = axs[n]\n",
    "    ax.scatter(subset[:,n], subset[:,n+1], s=2, c=labels[labels!=-1], alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7da2f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'SLDS'\n",
    "version = 'r2_orth_long'\n",
    "# version = 'partial_obs'\n",
    "base_dir = Path('/data/users/eabe/hypernets/{}/TiDHy_{}/{}'.format(dataset,dataset,version))\n",
    "configs = sorted(list(base_dir.rglob('*config.yaml'))[::2])\n",
    "fig_path = Path('/data/users/eabe/hypernets/SLDS/TiDHy_SLDS/r2_orth_long/Paper_Figs')\n",
    "for n,conf in enumerate(configs):\n",
    "    print(n,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe884c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "cfg_path =configs[n]# \n",
    "default_model_config = '/home/eabe/Research/MyRepos/TiDHy/conf/dataset/model/default_model.yaml'\n",
    "cfg = load_cfg(cfg_path, default_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b7c65",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "data_dict, cfg = load_dataset(cfg)\n",
    "\n",
    "# ##### Convert to float tensors #####\n",
    "train_inputs = torch.tensor(data_dict['inputs_train']).float()\n",
    "test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "val_inputs = torch.tensor(data_dict['inputs_val']).float()\n",
    "\n",
    "input_dim = test_inputs.shape[-1]\n",
    "# test_inputs = test_inputs.unsqueeze(0)\n",
    "# test_inputs = stack_data(test_inputs,cfg.train.sequence_length,overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "test_inputs = test_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "val_inputs = val_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "# train_inputs = train_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "# cfg.model.input_dim = input_dim\n",
    "\n",
    "\n",
    "\n",
    "print(f'Our inputs have shape: {test_inputs.shape}')\n",
    "cfg.model.input_dim = input_dim\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_inputs)\n",
    "# dataloader_train = torch.utils.data.DataLoader(train_dataset,batch_size=train_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=test_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "# dataloader_val = torch.utils.data.DataLoader(val_dataset,batch_size=val_inputs.shape[0],shuffle=False,pin_memory=True,drop_last=True)\n",
    "# device = torch.device(\"cuda:{}\".formt(cfg.train['gpu']) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['inputs_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce1a91",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "##### Load weights and params #####\n",
    "rerecord = False\n",
    "cfg.model.batch_converge=False\n",
    "epoch = 1000\n",
    "model = TiDHy.TiDHy(cfg.model, device, show_progress=cfg.train.show_progress).to(device)\n",
    "# set_seed(42)\n",
    "load_path = cfg.paths.ckpt_dir/'best.pth.tar'\n",
    "# load_path = sorted(list(cfg.paths.ckpt_dir.rglob('last_{}.pth.tar'.format(epoch))))[0]\n",
    "data_load = torch.load(load_path,map_location=device)\n",
    "model.load_state_dict(data_load['state_dict'])\n",
    "model.to(device)\n",
    "print('Loaded from {} epoch'.format(data_load['epoch']))\n",
    "epoch = data_load['epoch']-1\n",
    "\n",
    "\n",
    "# result_dict = load_results(model, dataloader_test, data_dict, device, cfg, epoch, rerecord=rerecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5434c",
   "metadata": {},
   "source": [
    "# Save Multi Seq Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cae1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "cfg, result_dict = run_seq_len_model(seq_len, model, data_dict, device, 1500, cfg, rerun=False)\n",
    "\n",
    "data_dict, cfg = load_dataset(cfg)\n",
    "seq_len = sorted([int(el) for el in list(result_dict.keys())])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2664017",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa3122",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "states_x_test = data_dict['states_x_test']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "# states_z_test = data_dict['states_z']\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "##### Set up combinatorics of timescales #####\n",
    "lst = list(itertools.product([1, 0], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros(ssm_params['time_bins_test'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,states_z_test)] = n\n",
    "\n",
    "\n",
    "save_figs = False\n",
    "p=-1\n",
    "W = jnp.stack([result_dict_seq[str(seq_len)]['W'] for seq_len in seq_lens])\n",
    "I = jnp.stack([result_dict_seq[str(seq_len)]['I'] for seq_len in seq_lens])\n",
    "Ihat = jnp.stack([result_dict_seq[str(seq_len)]['I_hat'] for seq_len in seq_lens])\n",
    "Ibar = jnp.stack([result_dict_seq[str(seq_len)]['I_bar'] for seq_len in seq_lens])\n",
    "R_hat = jnp.stack([result_dict_seq[str(seq_len)]['R_hat'] for seq_len in seq_lens])\n",
    "R_bar = jnp.stack([result_dict_seq[str(seq_len)]['R_bar'] for seq_len in seq_lens])\n",
    "R2_hat = jnp.stack([result_dict_seq[str(seq_len)]['R2_hat'] for seq_len in seq_lens])\n",
    "Ut = jnp.stack([result_dict_seq[str(seq_len)]['Ut'] for seq_len in seq_lens])\n",
    "As = data_dict['As']\n",
    "bs = data_dict['bs']\n",
    "nfig = 0\n",
    "fontsize=13\n",
    "# t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f503d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# from ssm.util import random_rotation, find_permutation\n",
    "inputs_train_SLDS=data_dict['inputs_train']\n",
    "inputs_test_SLDS=data_dict['inputs_test']\n",
    "seed = 10#cfg.dataset.ssm_params['seed']\n",
    "# SLDS_path = Path('/data/users/eabe/hypernets/SLDS/TiDHy_SLDS/L1ShortWin/L1_alpha=0.001,dataset.ssm_params.seed=10,dataset.train.sequence_length=200')\n",
    "# SLDS_path = cfg.paths.log_dir#/'SSM/DPC_SSM/Benchmark/dataset.ssm_params.seed={}/'.format(seed)\n",
    "SLDS_path = Path('/data/users/eabe/hypernets/SSM/DPC_SSM/Benchmark/dataset.ssm_params.seed=10')\n",
    "N = inputs_train_SLDS.shape[-1]\n",
    "K = len(np.unique(full_state_z))\n",
    "D = 6#data_dict['states_x_test'].shape[-1]\n",
    "with open(SLDS_path/'ssm_slds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "# with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "    slds = pickle.load(handle)\n",
    "    posterior = pickle.load(handle)\n",
    "    SLDS_latents = pickle.load(handle)\n",
    "    SLDS_states = pickle.load(handle)\n",
    "    SLDS_emission = pickle.load(handle)\n",
    "\n",
    "with open(SLDS_path/'ssm_rslds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "# with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "    slds = pickle.load(handle)\n",
    "    posterior = pickle.load(handle)\n",
    "    rSLDS_latents = pickle.load(handle)\n",
    "    rSLDS_states = pickle.load(handle)\n",
    "    rSLDS_emission = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537481f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=-1\n",
    "reg_variables = [\n",
    "    result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['W'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_bar'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['I'],\n",
    "    # SLDS_latents,\n",
    "    ]\n",
    "\n",
    "labels = ['R2_hat','W','R_bar','R_hat','I','SLDS']\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "\n",
    "# state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z],axis=0)\n",
    "state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ssm_params['latent_dim']*(p)),(p+1)*ssm_params['latent_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_comps=ssm_params['latent_dim']\n",
    "t=1500; dt=200\n",
    "    \n",
    "fig = plt.figure(constrained_layout=True, figsize=(7,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=5,ncols=4, figure=fig, wspace=.45,hspace=.2)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=6,ncols=1,subplot_spec=gs0[:,:2],wspace=.1,hspace=.2)\n",
    "# gs01 = gridspec.GridSpecFromSubplotSpec(3,1, subplot_spec=gs0[0:,:1],wspace=.05,hspace=.8)\n",
    "axs = np.array([fig.add_subplot(gs00[:-3,0]),\n",
    "                fig.add_subplot(gs00[-3:-2,0]),\n",
    "                fig.add_subplot(gs00[-2:,0]),\n",
    "                fig.add_subplot(gs0[:3,2:]),\n",
    "                fig.add_subplot(gs0[3:,2:])])\n",
    "axs[4].sharex(axs[3])\n",
    "ts = -1\n",
    "##### Pannel a #####\n",
    "I = result_dict_seq['{}'.format(seq_len[ts])]['I']#.reshape(-1,result_dict_seq['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict_seq['{}'.format(seq_len[ts])]['I_hat']#.reshape(-1,result_dict_seq['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "ax = axs[0]\n",
    "spacing = 1\n",
    "xrange = 100\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z)))\n",
    "linestyle =  (0, (5, 1))#(0, (5, 5))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=2,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing, linestyle=linestyle, color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "# ax.set_yticks(hlines_I)\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "##### Pannel c #####\n",
    "ax = axs[3]\n",
    "count=0\n",
    "acc_slds = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_rslds = accuracy_score(full_state_z, rSLDS_states)\n",
    "acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "spacing = .5\n",
    "R_bar = result_dict_seq['{}'.format(seq_len[ts])]['R_bar']#.reshape(-1,result_dict_seq['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict_seq['{}'.format(seq_len[ts])]['R_hat']#.reshape(-1,result_dict_seq['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "hlines_x,hlines_Rhat = [],[]\n",
    "for p in range(ssm_params['Nlds']):\n",
    "    # states_x_cca = states_x_train[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    # states_x_cca = states_x_test[(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    states_x_cca = states_x_test[p]\n",
    "    # states_x_cca = states_x_test[:,p:p+1]\n",
    "    cca = CCA(n_components=ssm_params['latent_dim'],max_iter=1000)\n",
    "    X_c,Y_c = cca.fit_transform(states_x_cca, R_hat)\n",
    "    cca_coefficient = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=n_comps)\n",
    "    x_w = cca.x_weights_\n",
    "    y_w = cca.y_weights_\n",
    "    cca_angles = [np.rad2deg(angle_between(X_c[:,n],Y_c[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_x = [np.rad2deg(angle_between(X_c[:,n],states_x_cca[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_r = [np.rad2deg(angle_between(Y_c[:,n],R_hat[:,n])) for n in range(n_comps)]\n",
    "    for n in range(n_comps):\n",
    "        print('comp {}, cc: {:.03}, ang: {:.03}, ang_x:{:.03}, ang_r:{:.03}'.format(n,cca_coefficient[n],cca_angles[n],cca_angles_x[n],cca_angles_r[n]))\n",
    "\n",
    "    for i in range(X_c.shape[-1]):\n",
    "        mean_centered_x = X_c[t:t+dt,i] - np.mean(X_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_x=mean_centered_x/(np.max(np.abs(mean_centered_x)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_x + count/spacing,color='k', lw=2,zorder=1)\n",
    "        hlines_x.append(np.mean(mean_centered_x + count/spacing,axis=0))\n",
    "        mean_centered_Rhat = Y_c[t:t+dt,i] - np.mean(Y_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_Rhat=mean_centered_Rhat/(np.max(np.abs(mean_centered_Rhat)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_Rhat + count/spacing,linestyle=linestyle,color='r', lw=1.5,zorder=2,label='$\\hat{{r}}_{{{}}}$={:.02}'.format(n,cca_coefficient[n]),alpha=1)\n",
    "        hlines_Rhat.append(np.mean(mean_centered_Rhat + count/spacing,axis=0))\n",
    "        ax.text(x=dt+15,y=hlines_x[count],s='cc = {:.02}'.format(cca_coefficient[i]),fontsize=fontsize-2)\n",
    "        count += 1\n",
    "    ax.set_yticks(hlines_x) \n",
    "    ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "    # ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "    ax.set_ylabel('latent variables',fontsize=fontsize)\n",
    "# ax.set_xticks([])\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps', fontsize=fontsize)\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*6))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*6+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "##### Pannel b #####\n",
    "ax = axs[1]\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "# timescales_As = 1/np.abs(np.log(np.real(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict_seq['{}'.format(seq_len[ts])]['R_bar']#.reshape(-1,result_dict_seq['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict_seq['{}'.format(seq_len[ts])]['R_hat']#.reshape(-1,result_dict_seq['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "min_error_idx = np.array([np.argmin(np.abs(timescales.reshape(-1) - timescales_As.reshape(-1)[n])) for n in range(timescales_As.shape[0])])\n",
    "min_error_timescales = timescales[min_error_idx]\n",
    "# timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "ax.scatter(x=min_error_timescales,y=np.abs(evals_Uhat_all.reshape(-1)[min_error_idx]), marker='x',\n",
    "            c='k',s=50,alpha=1,edgecolor='None',zorder=3)\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlim(0.25,2.5e2)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n",
    "\n",
    "######### Accuracy and reconstruction error #########\n",
    "ax = axs[2]\n",
    "Input_Errors_SLDS = np.abs(inputs_test_SLDS-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(inputs_test_SLDS-rSLDS_emission)\n",
    "Input_Errors = np.abs(I-result_dict_seq['{}'.format(seq_len[ts])]['I_hat'])\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs[1],label='TiDHy')\n",
    "ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs[1],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.legend(frameon=False,fontsize=fontsize-3,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "##### Pannel d #####\n",
    "ax = axs[4]\n",
    "ylabels_states = ['rSLDS \\n {:02}%'.format(int(np.round(acc_rslds*100))),\n",
    "                  'SLDS \\n {:02}%'.format(int(np.round(acc_slds*100))),\n",
    "                  'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),\n",
    "                  'True']\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,4,1))\n",
    "ax.set_yticklabels(ylabels_states,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=-.2)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Benchmark_Compare3.pdf',dpi=300)\n",
    "fig.savefig(fig_path/'{}_Fig2_V2.pdf'.format(nfig),dpi=300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=1\n",
    "best_pred_all = []\n",
    "acc_ssm = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_TiDHy_all = []\n",
    "for ts in range(len(seq_len)):\n",
    "    reg_variables = [\n",
    "        # np.concatenate([result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['W']],axis=-1)\n",
    "        result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "        # q_lem_x,\n",
    "        ]\n",
    "\n",
    "    labels = ['R2_hat']\n",
    "    # reg_variables = W\n",
    "    max_acc = 0\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        print(labels[k],scores)\n",
    "        acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "        acc_TiDHy_all.append(acc_TiDHy)\n",
    "        best_pred_all.append(y_pred)\n",
    "\n",
    "state_compare = np.concatenate([SLDS_states[None,:],np.stack(best_pred_all),full_state_z[None,:]],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbac2ec",
   "metadata": {},
   "source": [
    "## Load multi-seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "result_dict_all = {}\n",
    "for n in range(len(configs)):\n",
    "    cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "    default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "    cfg = load_cfg(cfg_path, default_model_config)\n",
    "    cfg, result_dict = run_seq_len_model(seq_len, model, data_dict, device, 1500, cfg, rerun=False)\n",
    "    result_dict_all['seed_{}'.format(cfg.dataset.ssm_params.seed)] = result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values\n",
    "seeds = np.arange(10,10+len(configs))\n",
    "seed = 10\n",
    "ts = 200\n",
    "\n",
    "##### Set up combinatorics of timescales #####\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "lst = list(itertools.product([0, 1], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros((len(seeds),ssm_params['time_bins_test']),dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for i, seed in enumerate(seeds):\n",
    "    for n in range(len(lst)):\n",
    "        full_state_z[i,np.apply_along_axis(lambda x: np.all(x == lst[n]),0,result_dict_all[f'seed_{seed}'][f'{ts}']['states_z_test'])] = n\n",
    "    \n",
    "I_all = np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['I'] for seed in seeds])\n",
    "I_hat_all = np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['I_hat'] for seed in seeds])\n",
    "R2_hat_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R2_hat'] for ts in seq_len])for seed in seeds])  # (n_seeds, Time, Z_dim)\n",
    "R_bar_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R_bar'] for ts in seq_len]) for seed in seeds])   # (n_seeds, TempWinN, Time, r2_dim)\n",
    "R_hat_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R_hat'] for ts in seq_len]) for seed in seeds])   # (n_seeds, TempWinN, Time, r_dim)\n",
    "W_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['W'] for ts in seq_len]) for seed in seeds])           # (n_seeds, TempWinN, Time, r_dim)\n",
    "As_all = np.stack([np.stack([v for v in result_dict_all[f'seed_{seed}'][f'{ts}']['As'].values()]) for seed in seeds])\n",
    "result_dict_all[f'seed_{seed}'][f'{ts}'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ssm.util import random_rotation, find_permutation\n",
    "inputs_train_SLDS=data_dict['inputs_train']\n",
    "inputs_test_SLDS=data_dict['inputs_test']\n",
    "seed = cfg.dataset.ssm_params['seed']\n",
    "TotalTime = cfg.dataset.ssm_params.time_bins_test\n",
    "# SLDS_path = Path('/data/users/eabe/hypernets/SLDS/TiDHy_SLDS/L1ShortWin/L1_alpha=0.001,dataset.ssm_params.seed=10,dataset.train.sequence_length=200')\n",
    "N = inputs_train_SLDS.shape[-1]\n",
    "K = len(np.unique(full_state_z))\n",
    "D = data_dict['states_x_test'].shape[-1]\n",
    "SLDS_latents = np.zeros((len(seeds),TotalTime,D))\n",
    "SLDS_states = np.zeros((len(seeds),TotalTime))\n",
    "SLDS_emission = np.zeros((len(seeds),TotalTime,N))\n",
    "rSLDS_latents = np.zeros((len(seeds),TotalTime,D))\n",
    "rSLDS_states = np.zeros((len(seeds),TotalTime))\n",
    "rSLDS_emission = np.zeros((len(seeds),TotalTime,N))\n",
    "for k, seed in enumerate(seeds):\n",
    "    SLDS_path = configs[k].parent#/'SSM/DPC_SSM/Benchmark/dataset.ssm_params.seed={}/'.format(seed)\n",
    "    with open(SLDS_path/'ssm_slds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "    # with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "        slds = pickle.load(handle)\n",
    "        posterior = pickle.load(handle)\n",
    "        SLDS_latents[k] = pickle.load(handle)\n",
    "        SLDS_states[k] = pickle.load(handle)\n",
    "        SLDS_emission[k] = pickle.load(handle)\n",
    "\n",
    "    with open(SLDS_path/'ssm_rslds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "    # with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "        slds = pickle.load(handle)\n",
    "        posterior = pickle.load(handle)\n",
    "        rSLDS_latents[k] = pickle.load(handle)\n",
    "        rSLDS_states[k] = pickle.load(handle)\n",
    "        rSLDS_emission[k] = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=-1\n",
    "\n",
    "labels = seeds\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "scores_all = []\n",
    "pred_all = np.zeros((len(seeds),len(seq_len),cfg.dataset.ssm_params.time_bins_test))\n",
    "for k in range(len(seeds)):\n",
    "    for ts in range(len(seq_len)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(R2_hat_all[k,ts], full_state_z[k].reshape(-1), test_size=0.25, random_state=42)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        y_pred = neigh.predict(R2_hat_all[k,ts])\n",
    "        pred_all[k,ts] = y_pred\n",
    "        scores_all.append(accuracy_score(full_state_z[k].reshape(-1),y_pred))\n",
    "        print(labels[k],seq_len[ts],scores)\n",
    "        if (scores > max_acc) & (labels[k] != 'I'):\n",
    "            max_acc = scores\n",
    "            best_label = k\n",
    "            best_pred = y_pred\n",
    "\n",
    "# state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z[best_label]],axis=0)\n",
    "\n",
    "scores_all = np.stack(scores_all).reshape(len(seeds),len(seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f817574",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(8,6))\n",
    "gs0 = gridspec.GridSpec(nrows=4,ncols=2, figure=fig, wspace=.15,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs0[:2,0]),fig.add_subplot(gs0[2:,0]),fig.add_subplot(gs0[:2,1]),fig.add_subplot(gs0[2:,1])])\n",
    "t=1500; dt=1000\n",
    "# seq_len=[100,200,500]\n",
    "ts = 1\n",
    "Input_Errors_SLDS = (I_all[k]-SLDS_emission[k])\n",
    "ax = axs[-1]\n",
    "Input_Errors_SLDS = np.mean([np.abs(I_all[k]-SLDS_emission[k]) for k in range(len(seeds))],axis=-1)\n",
    "Input_Errors_rSLDS = np.mean([np.abs(I_all[k]-rSLDS_emission[k]) for k in range(len(seeds))],axis=-1)\n",
    "err_slds  = np.mean([100*(1-accuracy_score(full_state_z[k], SLDS_states[k])) for k in range(len(seeds))])\n",
    "err_rslds = np.mean([100*(1-accuracy_score(full_state_z[k], rSLDS_states[k])) for k in range(len(seeds))])\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors =np.stack([np.abs(I_all[seed,ts]-I_hat_all[seed,ts]) for seed in range(len(seeds))])\n",
    "    err_TiDHy = np.mean([100*(1-accuracy_score(full_state_z[k], pred_all[k,ts])) for k in range(len(seeds))])\n",
    "    ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs_b[ts],label='TiDHy')\n",
    "    ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs_b[ts],xerr=np.mean(np.std(Input_Errors,axis=-1)),yerr=np.mean(np.std(Input_Errors,axis=0)))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=np.mean(err_rslds), s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=np.mean(err_rslds), ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.tick_params(axis='y', labelsize=fontsize-2)\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "tts+=1\n",
    "ax.annotate('SLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[2],fontsize=fontsize-2)\n",
    "tts += 1\n",
    "ax.annotate('rSLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[6],fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs[2]\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "# timescales_As = 1/np.abs(np.log(np.real(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    for seed in seeds: \n",
    "        R_bar = result_dict_all['seed_{}'.format(seed)]['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "        R_hat = result_dict_all['seed_{}'.format(seed)]['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "        ##### Plottign Eigenvalues #####\n",
    "        Uhat_all = []\n",
    "        for state in range(len(np.unique(full_state_z[0]))):\n",
    "            # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "            # model = VAR(rhat2)\n",
    "            # results = model.fit(maxlags=20,ic='aic')\n",
    "            # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "            # Uhat_all.append(eig_Ut.reshape(-1))\n",
    "        # for p in range(ssm_params.Nlds):\n",
    "        #     for state in range(ssm_params.n_disc_states):\n",
    "                # rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "            rhat2 = R_hat[np.where(full_state_z[0]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "\n",
    "        Uhat_all = np.stack(Uhat_all)\n",
    "        # evals_Uhat_all = np.hstack(Uhat_all)\n",
    "        # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "        # timescales = np.real(np.log(Uhat_all))\n",
    "        evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "        timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "        # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "        min_error_idx = np.array([np.argmin(np.abs(timescales.reshape(-1) - timescales_As.reshape(-1)[n])) for n in range(timescales_As.shape[0])])\n",
    "        min_error_timescales = timescales[min_error_idx]\n",
    "        ax.scatter(x=min_error_timescales,y=np.abs(evals_Uhat_all.reshape(-1)[min_error_idx])+ts,\n",
    "                c=clrs_b[ts],alpha=.75,edgecolor='None',marker='x',s=25,zorder=3)\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "        ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    ax.set_xscale('symlog',linthresh=.1)\n",
    "    ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "    ax.set_ylabel('T',fontsize=fontsize,labelpad=-2)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=.015)\n",
    "y = .99\n",
    "dy = 0.075\n",
    "ax.annotate('System 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-2)\n",
    "ax.annotate('System 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-2)\n",
    "ax.annotate('System 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-2)\n",
    "\n",
    "ax=axs[0]\n",
    "spacing= 5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xlim([0,dt])\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(1.05,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "\n",
    "# legend = ax.legend(['T={}'.format(seq_len[p]) for p in range(len(seq_len))],frameon=False,fontsize=fontsize,loc='upper right',\n",
    "#           bbox_to_anchor=(1.25,1),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1)\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "state_compare = np.concatenate([SLDS_states[:1,:],pred_all[0],full_state_z[:1,:]],axis=0)\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z[0])))\n",
    "acc_slds = accuracy_score(full_state_z[0], SLDS_states[0])\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(scores_all[0,n]*100))) for n in range(len(scores_all[0]))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_slds*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+200,200))\n",
    "# ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "fig.savefig(fig_path/'{}_Fig3.pdf'.format(0),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1\n",
    "Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "np.mean(Input_Errors),np.std(Input_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(8,6))\n",
    "gs0 = gridspec.GridSpec(nrows=4,ncols=2, figure=fig, wspace=.15,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs0[:2,0]),fig.add_subplot(gs0[2:,0]),fig.add_subplot(gs0[:2,1]),fig.add_subplot(gs0[2:,1])])\n",
    "t=1500; dt=1000\n",
    "# seq_len=[100,200,500]\n",
    "\n",
    "Input_Errors_SLDS = (inputs_test_SLDS-SLDS_emission)\n",
    "ax = axs[0]\n",
    "Input_Errors_SLDS = np.abs(inputs_test_SLDS-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(inputs_test_SLDS-rSLDS_emission)\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "    err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "    ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs_b[ts],label='TiDHy')\n",
    "    ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs_b[ts],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.tick_params(axis='y', labelsize=fontsize-2)\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "tts+=1\n",
    "ax.annotate('SLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[2],fontsize=fontsize-2)\n",
    "tts += 1\n",
    "ax.annotate('rSLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[6],fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs[1]\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        # model = VAR(rhat2)\n",
    "        # results = model.fit(maxlags=20,ic='aic')\n",
    "        # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "        # Uhat_all.append(eig_Ut.reshape(-1))\n",
    "    # for p in range(ssm_params.Nlds):\n",
    "    #     for state in range(ssm_params.n_disc_states):\n",
    "            # rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    # evals_Uhat_all = np.hstack(Uhat_all)\n",
    "    # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "    # timescales = np.real(np.log(Uhat_all))\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "    \n",
    "    ax.scatter(x=timescales,y=np.abs(evals_Uhat_all)+ts,\n",
    "               c=clrs_b[ts],alpha=.75,edgecolor='None',s=25)\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "        ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    ax.set_xscale('symlog',linthresh=.1)\n",
    "    ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "    ax.set_ylabel('T',fontsize=fontsize,labelpad=-2)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=.015)\n",
    "y = .99\n",
    "dy = 0.075\n",
    "ax.annotate('System 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-2)\n",
    "ax.annotate('System 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-2)\n",
    "ax.annotate('System 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-2)\n",
    "\n",
    "ax=axs[2]\n",
    "spacing= 5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xlim([0,dt])\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(1.05,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "\n",
    "# legend = ax.legend(['T={}'.format(seq_len[p]) for p in range(len(seq_len))],frameon=False,fontsize=fontsize,loc='upper right',\n",
    "#           bbox_to_anchor=(1.25,1),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1)\n",
    "\n",
    "\n",
    "ax = axs[3]\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(acc_TiDHy_all[n]*100))) for n in range(len(acc_TiDHy_all))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_ssm*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+200,200))\n",
    "# ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "fig.savefig(cfg.paths.fig_dir/'{}_Fig3.pdf'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60973a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade off plot between accuracy of reconstruction and correct identification of unique timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(5,3))\n",
    "ax = axs\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "ax.scatter(x=timescales,y=np.abs(evals_Uhat_all),\n",
    "            c='k',s=40,alpha=.5,edgecolor='None')\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd0f76",
   "metadata": {},
   "source": [
    "# Load Anymal Terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29254605",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'AnymalTerrain'\n",
    "version = 'Debug'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 1#7 #22 #2 #8\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd().parent / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg.dataset.name}: {cfg.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6662783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_train = stack_data(data_dict['inputs_train'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "inputs_val = stack_data(data_dict['inputs_val'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)\n",
    "inputs_test = stack_data(data_dict['inputs_test'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create RNG\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Get model params as dict and unpack directly\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "# model_params.pop('batch_converge')\n",
    "model_params['input_dim'] = inputs_test.shape[-1]\n",
    "model_params['show_inf_progress'] = False\n",
    "# model_params['max_iter'] = 1000\n",
    "# model_params['lr_r'] = 7.5e-3\n",
    "# model_params['lr_r2'] = 7.5e-3\n",
    "\n",
    "\n",
    "model = TiDHy(**model_params, rngs=rngs)\n",
    "# model.l0= jnp.zeros(3)\n",
    "# model.loss_weights = jnp.array([1.0, 1.0, 1.0])\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"input_dim: {model.input_dim}, r_dim: {model.r_dim}, r2_dim: {model.r2_dim}, mix_dim: {model.mix_dim}\")\n",
    "jit_model = jax.jit(model)\n",
    "# out = jit_model(inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da250eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_first_two_dims(data_dict):\n",
    "    \"\"\"\n",
    "    Recursively reshape arrays in a dictionary to collapse the first 2 dimensions.\n",
    "    \n",
    "    Arrays with shape (a, b, c, d, ...) will be reshaped to (a*b, c, d, ...).\n",
    "    Arrays with fewer than 2 dimensions are returned unchanged.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary containing arrays (possibly nested)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with reshaped arrays\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for key, value in data_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Recursively handle nested dictionaries\n",
    "            result[key] = collapse_first_two_dims(value)\n",
    "        elif hasattr(value, 'shape') and len(value.shape) >= 2:\n",
    "            # Reshape arrays with at least 2 dimensions\n",
    "            new_shape = (value.shape[0] * value.shape[1],) + value.shape[2:]\n",
    "            result[key] = jnp.reshape(value, new_shape)\n",
    "        else:\n",
    "            # Keep other values unchanged\n",
    "            result[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1000\n",
    "loaded_model = load_model(model,cfg.paths.ckpt_dir/f'epoch_{epoch:04d}')\n",
    "result_dict = {}\n",
    "seq_len = [100,200,500,1000]\n",
    "for seq in tqdm(seq_len):\n",
    "    inputs_test = stack_data(data_dict['inputs_test'], sequence_length=seq, overlap=seq)\n",
    "    spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict_single = evaluate_record(loaded_model, inputs_test)\n",
    "    result_dict['{}'.format(seq)] = collapse_first_two_dims(result_dict_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_single(seq):\n",
    "    return model(seq, return_internals=True)\n",
    "\n",
    "# in_axes=0 means vmap over first dimension (batch)\n",
    "vmapped_forward = jax.vmap(forward_single, in_axes=0)\n",
    "# batch_losses = vmapped_forward(inputs_train)\n",
    "out, internals = forward_single(inputs_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_model, history = train_model(\n",
    "    model,\n",
    "    inputs_train,\n",
    "    n_epochs=250,\n",
    "    learning_rate_s=0.01,  # Spatial decoder learning rate\n",
    "    learning_rate_t=0.01,  # Temporal parameters learning rate  \n",
    "    learning_rate_h=0.001,  # Hypernetwork learning rate\n",
    "    schedule_transition_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=2**16,\n",
    "    use_gradnorm=False,\n",
    "    val_data=inputs_val,\n",
    "    val_every_n_epochs=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "# cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b\n",
    "\n",
    "def shuffle_along_axis(a, axis):\n",
    "    idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "    return np.take_along_axis(a,idx,axis=axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_figs = False\n",
    "W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])\n",
    "nfig = 0\n",
    "t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50825e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "t=0; dt=1000\n",
    "spacing= .1\n",
    "ts=-2\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "I_shuff  = deepcopy(result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1]))\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,5))\n",
    "ax = axs[0]\n",
    "# cmap,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=1,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing,ls='-',color='#4e7eb3ff', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*I.shape[-1]))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1]+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "# # im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1],1)),cmap=cmap,norm=norm,alpha=.5)\n",
    "# cbar = fig.colorbar(im,ax=axs,aspect=30)\n",
    "# cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "# cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "# cbar.outline.set_linewidth(1)\n",
    "# cbar.minorticks_off()\n",
    "# cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "ax.set_yticks(hlines_I)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(0,1200,200),fontsize=fontsize)\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "ax.set_ylabel('Observation #',fontsize=fontsize)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.025,1.05),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "# ax.set_title('Learned Observations',fontsize=fontsize)\n",
    "lim0 = 0\n",
    "lim1 = 20\n",
    "hbins = 1\n",
    "Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# Input_Errors_SLDS = np.mean((inputs_test_SLDS-q_lem_y)**2,axis=0)\n",
    "I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "ax = axs[1]\n",
    "count,edges = np.histogram(Input_Errors,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='#757575ff',width=hbins, alpha=1,zorder=1,label='data')\n",
    "count,edges = np.histogram(Input_Errors_shuff,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='r',width=hbins, alpha=1,zorder=1,label='shuffle') \n",
    "# count,edges = np.histogram(Input_Errors_SLDS,bins=np.arange(lim0,lim1,hbins))\n",
    "# edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "# ax.bar(edges_mid, count/len(Input_Errors_SLDS),color='g',width=hbins, alpha=.5,zorder=1,label='SLDS') \n",
    "ax.set_ylabel('Proportion',fontsize=fontsize)\n",
    "ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize)\n",
    "# ax = axs[1]\n",
    "# fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "# Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "# Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "# xs = np.arange(I.shape[-1])\n",
    "# heights = Input_Errors\n",
    "# ax.barh(y=hlines_I,width=heights,color='k',)\n",
    "# ax.errorbar(heights,hlines_I,xerr=np.std((I-Ihat),axis=0)/np.sqrt((I-Ihat).shape[0]),ls='none',color='tab:gray',capsize=3)\n",
    "# ax.axvline(x=np.mean(Input_Errors_shuff),c='k',ls='--',lw=1,label='shuffle error')\n",
    "# # ax.set_xticks(np.arange(0,.004,.001))\n",
    "# ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "# ax.set_title('Average Error',fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(heights.shape[-1])+1)\n",
    "# ax.legend(bbox_to_anchor=(.3, .95), loc='lower left', borderaxespad=0.,frameon=False,fontsize=fontsize)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_observations_pred.png'.format(nfig),dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d423127",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mean_centered_x + n/spacing,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "t=0; dt = 1000\n",
    "# cmap2,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig, axs = plt.subplots(2,2,figsize=(20,12))\n",
    "axs = axs.flatten()\n",
    "fontsize=13\n",
    "ax = axs[0]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R2_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-.5,.75,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('$\\hat{r}^h$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax.legend(['T={}'.format(Ln) for Ln in seq_len],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.15,1.05),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "ax = axs[1]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['W'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['W'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(0,2.25,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('W',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax = axs[2]\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_bar'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "\n",
    "ax = axs[3]\n",
    "spacing= 1\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['I_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[p])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=.5)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('$\\hat{Z}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_TempWindExpansion.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1073bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind = [2,8,9]\n",
    "clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "terrain_names = ['Flat','Slope','Inv. Slope','Stairs','Inv. Stairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain_type_train = data_dict['terrain_train'].reshape(-1)\n",
    "terrain_type_test = data_dict['terrain_test']\n",
    "terrain_difficulty_train = data_dict['terrain_difficulty_train'].reshape(-1)\n",
    "terrain_difficulty_test = data_dict['terrain_difficulty_test'].reshape(-1)\n",
    "terrain_slope_train = data_dict['terrain_slope_train'].reshape(-1)\n",
    "terrain_slope_test = data_dict['terrain_slope_test'].reshape(-1)\n",
    "command_vel_train = data_dict['command_vel_train'].reshape(-1)\n",
    "command_vel_test = data_dict['command_vel_test'].reshape(-1)\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "terrain_robot_test = np.stack([terrain_type_test[robot_type_test==0].reshape(-1),terrain_type_test[robot_type_test==1].reshape(-1)])\n",
    "terrain_type_test = data_dict['terrain_test'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59828be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "seq_len=[100,200,500,1000]\n",
    "ax = axs\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.random.uniform(y_ranges[ts,0],y_ranges[ts,1],timescales.shape[0]),\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None')\n",
    "\n",
    "# for n in range(len(timescales_M)):\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize)\n",
    "ax.set_ylabel('T',fontsize=fontsize)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize)\n",
    "ax.set_xticks([0,10e0,10e1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ts=-2\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([result_dict['R2_hat'],result_dict['W'],result_dict['R_bar']],axis=-1),result_dict['R2_hat'],result_dict['W'],result_dict['R_hat'],result_dict['I']]\n",
    "reg_variables = [result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['I']]\n",
    "labels = ['R2_hat','I']\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "command_vel_test = np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    # X_train = reg_vars[:tr_batch_size].reshape(-1,reg_vars.shape[-1])\n",
    "    # X_test = reg_vars[-batch_size:].reshape(-1,reg_vars.shape[-1])\n",
    "    # y_train = terrain[:tr_batch_size].reshape(-1)\n",
    "    # y_test = terrain[-batch_size:].reshape(-1)\n",
    "    # y_train = robot_type_test[:tr_batch_size].reshape(-1)\n",
    "    # y_test = robot_type_test[-batch_size:].reshape(-1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), robot_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    # neigh = RidgeClassifierCV()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    # y_pred = neigh.predict(X_test)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred_terrain = y_pred\n",
    "    else:\n",
    "        I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        \n",
    "state_compare = np.stack([I_pred,best_pred_terrain,terrain_type_test])\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([R2_hat,W,R_bar],axis=-1),R2_hat,W,R_hat,I]\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(I.reshape(-1,I.shape[-1]))\n",
    "reg_variables = [R2_hat,I] #,X_pca]\n",
    "labels = ['R2_hat','I'] #,'X_pca']\n",
    "label_type = ['terrain_difficulty_test','terrain_slope_test','command_vel_test']#,'robot_type_test']\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0).reshape(-1)\n",
    "command_vel_test = np.tile(data_dict['command_vel_test'],(9000,1)).transpose(1,0)\n",
    "slope_cat = np.unique(terrain_slope_test,return_inverse=True)[1]\n",
    "difficulty_cat =  np.unique(terrain_difficulty_test,return_inverse=True)[1]\n",
    "vel_cat = np.unique(command_vel_test,return_inverse=True)[1]\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "class_results = {}\n",
    "state_compare_best = {}\n",
    "for m, test_labels in enumerate([difficulty_cat,slope_cat,vel_cat]): #,robot_type_test\n",
    "\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), test_labels.reshape(-1), test_size=0.25, random_state=42)\n",
    "\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        print(label_type[m],labels[k],scores)\n",
    "        if (labels[k] != 'I'):\n",
    "            y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = y_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "            max_acc = scores\n",
    "            best_reg_vars = reg_vars\n",
    "            best_label = labels[k]\n",
    "            best_pred = y_pred\n",
    "        else:\n",
    "            I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = I_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "    state_compare_best['{}'.format(label_type[m])] = np.stack([I_pred,best_pred,test_labels],axis=0)\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n",
    "from sklearn import metrics\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bafde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_compare_data = {'state_compare_best':state_compare_best,'state_compare':state_compare,'class_results':class_results}\n",
    "# ioh5.save(cfg.paths.log_dir/'state_compare.h5',state_compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e59514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_compare_data = ioh5.load(cfg.paths.log_dir/'state_compare.h5')\n",
    "state_compare = state_compare_data['state_compare']\n",
    "state_compare_best = state_compare_data['state_compare_best']\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=2500; dt=1000\n",
    "fontsize=13\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=6,ncols=3, figure=fig, wspace=.1,hspace=.1)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5, subplot_spec=gs0[:3,:],wspace=.1,hspace=.1)\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5,subplot_spec=gs0[4:,:],wspace=.1,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs00[:,:2]),\n",
    "                fig.add_subplot(gs00[:,2:4]),\n",
    "                fig.add_subplot(gs00[:,4:]),\n",
    "                fig.add_subplot(gs01[:,:2]),\n",
    "                fig.add_subplot(gs01[:,2:])])\n",
    "ax = axs[1]\n",
    "spacing= .5\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=1)\n",
    "ts=1\n",
    "hlines_x = []\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I_hat'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c='r',alpha=.75)\n",
    "    hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "# ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "# ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "# ax.set_yticks(hlines_x[1::2])\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(2,len(hlines_x)+1,2),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "# ax.set_ylim(-1,(1/spacing)*len(hlines_x)+.5)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xticks([0,10e0,10e1,10e2])\n",
    "ax.set_xlim(0,1e2)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=-1)\n",
    "\n",
    "\n",
    "ax=axs[3]\n",
    "ax.bar(np.arange(len(f1_TiDHy)),f1_TiDHy,width=.4,color=clrs2[0])\n",
    "ax.bar(np.arange(len(f1_Inputs))+.4,f1_Inputs,width=.4,color=clrs2[4])\n",
    "ax.set_ylabel('F1 Score',fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(.2,len(f1_TiDHy)+.2))\n",
    "ax.set_xticklabels(['difficulty','slope','velocity'],fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(0,1.1,.25))\n",
    "ax.set_yticklabels(np.arange(0,1.1,.25),fontsize=fontsize-2)\n",
    "ax.legend(['TiDHy','Obs.'],fontsize=fontsize,frameon=False,loc='upper right',bbox_to_anchor=(1.025,1.25),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "ax = axs[-1]\n",
    "acc_obs = accuracy_score(terrain_type_test.reshape(-1),state_compare[0])\n",
    "acc_TiDHy = accuracy_score(terrain_type_test.reshape(-1),state_compare[1])\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(terrain_type_test)))\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+3000,3000))\n",
    "# ax.set_xticklabels(np.arange(0,dt+3000,3000),fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(.5,3,1))\n",
    "ax.set_yticklabels(['Obs. \\n {:02}%'.format(int(np.round(acc_obs*100))),'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=.05)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(terrain_type_test)),1))\n",
    "cbar.set_ticklabels([''.join(terrain_names[n]) for n in range(len(terrain_names))],fontsize=fontsize-2)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_Fig4.pdf'.format(nfig),dpi=300,transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "from sklearn.cluster import HDBSCAN, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0579cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.manifold.UMAP(n_components=5, n_neighbors=15, min_dist=0.75, metric='euclidean', init='spectral')\n",
    "reduced_data = umap.fit_transform(R2_hat[-1])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(R_hat[1])\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=50, min_samples=20, cluster_selection_epsilon=0.55, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97241e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterer.labels_.get()\n",
    "\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_label = jnp.argmax(soft_clusters[:,1:].get(), axis=1)\n",
    "r_data = reduced_data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(r_data[:, 0], r_data[:, 1], s=2,  c=terrain_type_test, alpha=0.05)\n",
    "plt.axis((-10,10,-10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = sns.color_palette('Paired', 12)\n",
    "cluster_colors = [color_palette[np.argmax(x)]\n",
    "                  for x in soft_clusters]\n",
    "plt.scatter(*projection.T, s=50, linewidth=0, c=cluster_colors, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA()\n",
    "comp = pca.fit_transform(R2_hat[1])\n",
    "subset = comp[labels!=-1,:]\n",
    "fig, axs = plt.subplots(2,2,figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "for n in range(len(axs)):\n",
    "    ax = axs[n]\n",
    "    ax.scatter(subset[:,n], subset[:,n+1], s=2, c=labels[labels!=-1], alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e6de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc019dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab8eed0f",
   "metadata": {},
   "source": [
    "# Load CalMS21 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20351cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CalMS21'\n",
    "version = 'Debug'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 0#7 #22 #2 #8\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd().parent / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg.dataset.name}: {cfg.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "cmap_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data(cfg)\n",
    "inputs_train = stack_data(data_dict['inputs_train'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "inputs_val = stack_data(data_dict['inputs_val'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)\n",
    "inputs_test = stack_data(data_dict['inputs_test'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNG\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Get model params as dict and unpack directly\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "# model_params.pop('batch_converge')\n",
    "model_params['input_dim'] = inputs_train.shape[-1]\n",
    "# model_params['show_inf_progress'] = False\n",
    "# model_params['max_iter'] = 1000\n",
    "# model_params['lr_r'] = 7.5e-3\n",
    "# model_params['lr_r2'] = 7.5e-3\n",
    "\n",
    "\n",
    "model = TiDHy(**model_params, rngs=rngs)\n",
    "model.l0= jnp.zeros(3)\n",
    "model.loss_weights = jnp.array([1.0, 1.0, 1.0])\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"input_dim: {model.input_dim}, r_dim: {model.r_dim}, r2_dim: {model.r2_dim}, mix_dim: {model.mix_dim}\")\n",
    "jit_model = jax.jit(model)\n",
    "# out = jit_model(inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad31f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1000\n",
    "loaded_model = load_model(model,cfg.paths.ckpt_dir/f'epoch_{epoch:04d}')\n",
    "# Test the unstacking function with the current parameters\n",
    "# Since inputs_val has no overlap (overlap = sequence_length), we can simply concatenate\n",
    "sequence_length = cfg.train.sequence_length\n",
    "overlap = sequence_length\n",
    "# Check current parameters\n",
    "print(f\"sequence_length: {sequence_length}\")\n",
    "print(f\"overlap: {overlap}\")\n",
    "print(f\"inputs_val shape: {inputs_val.shape}\")\n",
    "\n",
    "# Test unstacking with a small example first\n",
    "print(f\"\\nTesting unstacking with no overlap...\")\n",
    "\n",
    "# Now test with the actual results\n",
    "print(f\"\\nTesting unstacking on result_dict...\")\n",
    "try:\n",
    "    spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict = evaluate_record(\n",
    "        loaded_model, inputs_test\n",
    "    )\n",
    "    \n",
    "    print(\"Success! Unstacked shapes:\")\n",
    "    for key, value in result_dict.items():\n",
    "        if isinstance(value, jnp.ndarray):\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error during unstacking: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c660164",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "seq_len = [100,200,500,1000]\n",
    "for seq in tqdm(seq_len):\n",
    "    inputs_test = stack_data(data_dict['inputs_test'], sequence_length=seq, overlap=seq)\n",
    "    spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict_single = evaluate_record(loaded_model, inputs_test)\n",
    "    result_dict['{}'.format(seq)] = result_dict_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1\n",
    "feature = 0\n",
    "T_max = 1000\n",
    "plt.plot(I[ts,:T_max, feature], color=clr2b[0], label='Input current I')\n",
    "plt.plot(Ihat[ts,:T_max, feature], color=clr2b[1], label='Reconstructed current Î')\n",
    "plt.plot(Ibar[ts,:T_max, feature], color=clr2b[2], label='Smoothed current Ī')\n",
    "plt.xlabel('Time steps', fontsize=fontsize)\n",
    "plt.ylabel('Input current', fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "ts = 1\n",
    "feature = 0\n",
    "T_max = 1000\n",
    "fig,axs = plt.subplots(1,1, figsize=(6,4))\n",
    "ax = axs\n",
    "# ax.plot(R_bar[ts,:T_max, feature], color=clr2b[0], label='Input current I')\n",
    "ax.plot(R_hat[ts,:T_max, feature], color=clr2b[1], label='Reconstructed current Î')\n",
    "ax.plot(R2_hat[ts,:T_max, feature], color=clr2b[2], label='Smoothed current Ī')\n",
    "ax.set_xlabel('Time steps', fontsize=fontsize)\n",
    "ax.set_ylabel('Input current', fontsize=fontsize)\n",
    "# plt.legend(fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "p = -1\n",
    "pca = PCA(n_components=3)\n",
    "full_state_z = data_dict['annotations_test']\n",
    "# reg_all = np.concatenate([R2_hat,W,R_bar],axis=1)\n",
    "X_pca = pca.fit_transform(result_dict['{}'.format(seq_len[p])]['R_hat'])\n",
    "n = 0\n",
    "t = 0; dt = len(X_pca)\n",
    "X_pca = X_pca[t:t+dt]\n",
    "comps = X_pca[full_state_z==n][t:t+dt]\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "# ax = Axes3D(fig)\n",
    "ax.scatter(comps[:,0],comps[:,1],comps[:,2],c='r',alpha=.75)\n",
    "ax.scatter(X_pca[:,0],X_pca[:,1],X_pca[:,2],c=clrs_b[p],alpha=.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b18a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = -1\n",
    "pca = PCA(n_components=3)\n",
    "full_state_z = data_dict['annotations_test']\n",
    "# reg_all = np.concatenate([R2_hat,W,R_bar],axis=1)\n",
    "# X_pca = pca.fit_transform(result_dict['{}'.format(seq_len[p])]['R_hat'])\n",
    "X_pca = pca.fit_transform(result_dict['{}'.format(seq_len[p])]['R2_hat'])\n",
    "# X_pca = pca.fit_transform(R_hat)\n",
    "# X_pca = pca.fit_transform(R_bar)\n",
    "# X_pca = pca.fit_transform(reg_all)\n",
    "# X_pca = pca.fit_transform(W[:,np.sum(W,axis=0)!=0])\n",
    "# X_pca = pca.fit_transform(inputs_train)\n",
    "# X_pca = pca.fit_transform(b)\n",
    "# X_pca = pca.fit_transform(Vt.reshape(Vt.shape[0],-1))\n",
    "n = 3\n",
    "t = 0; dt = len(X_pca)\n",
    "fig = go.Figure()\n",
    "comps = X_pca[full_state_z==n][t:t+dt]\n",
    "# labels = full_state_z[full_state_z==n]\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=X_pca[:,0], y=X_pca[:,1], z=X_pca[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color='black',                # set color to an array/list of desired values\n",
    "            colorscale='Viridis',   # choose a colorscale\n",
    "            opacity=.1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=comps[:,0], y=comps[:,1], z=comps[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color='red',                # set color to an array/list of desired values\n",
    "            colorscale='Viridis',   # choose a colorscale\n",
    "            opacity=.01\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=500,\n",
    "    height=500,\n",
    "    autosize=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ab3cf",
   "metadata": {},
   "source": [
    "## Record Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88393e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False\n",
    "ts=1\n",
    "W = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "# b = result_dict['{}'.format(seq_len[ts])]['b'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['b'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "Ut = result_dict['{}'.format(seq_len[ts])]['Ut'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-2],result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-1])\n",
    "##### Plot dynamic matrices #####\n",
    "# if cfg.model.low_rank_temp:\n",
    "#     Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "# else:\n",
    "#     Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "full_state_z = data_dict['annotations_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25508806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "reg_variables = [np.concatenate([R2_hat,W,R_hat],axis=-1),R2_hat,W,R_hat,R_bar,I]\n",
    "labels = ['all','R2_hat','W','R_hat','R_bar','I']\n",
    "\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "    else:\n",
    "        I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, best_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_state_z = data_dict['annotations_test']\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d15570",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=13\n",
    "fps = 30\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.5))\n",
    "gs  = gridspec.GridSpec(nrows=8, ncols=4,hspace=10,wspace=.5) \n",
    "gs0 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[:3,:],  wspace=.5,hspace=.2)\n",
    "# gsb = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:3,2:3], wspace=.1,hspace=.5)\n",
    "\n",
    "# gsc = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:5,3:], wspace=.5,hspace=.1)\n",
    "gs1 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[3:6,:], wspace=.2,hspace=.1)\n",
    "\n",
    "gs2 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[6:,:-1],   wspace=0, hspace=.10)\n",
    "\n",
    "##### plotting video frame ######\n",
    "t = 50; dt = 1\n",
    "skeleton = np.array([[0,1],[0,2],[2,3],[1,3],[3,4],[3,5],[4,6],[5,6]])\n",
    "ani1_x = data_dict['inputs_train'][t:t+dt,:7]\n",
    "ani1_y = data_dict['inputs_train'][t:t+dt,7:14]\n",
    "ani2_x = data_dict['inputs_train'][t:t+dt,14:21]\n",
    "ani2_y = data_dict['inputs_train'][t:t+dt,21:28]\n",
    "I1_x = Ihat[t:t+dt,:7]\n",
    "I1_y = Ihat[t:t+dt,7:14]\n",
    "I2_x = Ihat[t:t+dt,14:21]\n",
    "I2_y = Ihat[t:t+dt,21:28]\n",
    "labels = data_dict['annotations_train'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "axs = np.array([fig.add_subplot(gs0[:2])])\n",
    "ax = axs[0]\n",
    "# ax.imshow(frames[t]) ##### Load frames from video in cell below\n",
    "for n in range(7):\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=10,c='#ffa600ff')\n",
    "    im = ax.scatter(ani2_x[:,n],ani2_y[:,n],s=10,c='#4daf50ff')\n",
    "tt=0\n",
    "for n in range(len(skeleton)):\n",
    "    ax.plot(ani1_x[:,skeleton[n]].squeeze(),ani1_y[:,skeleton[n]].squeeze(),'#ffa600ff',lw=2,zorder=1)\n",
    "    ax.plot(ani2_x[:,skeleton[n]].squeeze(),ani2_y[:,skeleton[n]].squeeze(),'#4daf50ff',lw=2,zorder=1)\n",
    "ax.axis('off')\n",
    "\n",
    "###### Plotting Observations #####\n",
    "axs = np.array([fig.add_subplot(gs0[2])])\n",
    "ax = axs[0]\n",
    "spacing = .75; fontsize=13\n",
    "t = 0; dt = 10000\n",
    "hlines,hlines_Ihat= [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    mean_centered = mean_centered\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered + n/spacing,color='k', lw=1)\n",
    "    hlines.append(np.mean(mean_centered + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered_Ihat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=0)\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "ax.set_ylabel(\"observations\",fontsize=fontsize)\n",
    "ax.set_yticks([])\n",
    "\n",
    "##### Timescales #####\n",
    "axs = np.array([fig.add_subplot(gs0[3])])\n",
    "ax = axs[0]\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "        \n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))/fps\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xscale('symlog',linthresh=100)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=0)\n",
    "ax.set_xticks([10e0,10e1,10e2,10e3,10e4,10e5])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=0)\n",
    "ax.set_xlim([-1,7e4])\n",
    "\n",
    "###### PCA #####\n",
    "n = 0; m = 1\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(R_hat.reshape(-1,R_hat.shape[-1]))\n",
    "lst = list(itertools.combinations(np.arange(X_pca.shape[-1]), 2))\n",
    "unq = set(lst)\n",
    "axs = np.array([fig.add_subplot(gs1[n]) for n in range(4)])\n",
    "for state in range(4):\n",
    "    ax = axs[state]\n",
    "    comps = X_pca[full_state_z==state]\n",
    "    comps = comps/np.max(np.abs(comps),axis=0)\n",
    "    # comps = X_pca[(full_state_z==0) | (full_state_z==1) | (full_state_z==2)]\n",
    "    h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=50,range=[[-1,1],[-1,1]],density=True)\n",
    "    im = ax.imshow((h).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_title('{}'.format(list(data_dict['vocabulary'].keys())[state]),fontsize=fontsize,y=.95)\n",
    "    ax.set_xlabel('PC{}'.format(n+1),fontsize=fontsize)\n",
    "    if (state==0):\n",
    "        ax.set_ylabel('PC{}'.format(m+1),fontsize=fontsize)\n",
    "ax = axs[-1]\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10,shrink=.5)\n",
    "cbar.set_ticks([0,np.max(h)])\n",
    "cbar.set_ticklabels(['low','high'],rotation=90)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "\n",
    "##### Behaviroal State ID #####\n",
    "t = 80000; dt = 10000; \n",
    "acc_TiDHy=accuracy_score(best_pred, full_state_z)\n",
    "# acc_TiDHy=neigh.score(I_pred, full_state_z)\n",
    "state_compare = np.stack([y_pred,full_state_z],axis=0)\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "axs = np.array([fig.add_subplot(gs2[n]) for n in range(1)])\n",
    "ax = axs[0]\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_yticks(np.arange(.5,2,1))\n",
    "ax.set_yticklabels(['TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt,60*fps))\n",
    "ax.set_xticklabels((np.arange(0,dt,60*fps)*1/fps).astype(int),fontsize=fontsize-2)\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "\n",
    "# plt.tight_layout()\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10, pad=.01)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()),fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "# fig.savefig(cfg.paths.fig_dir / 'Fig5.pdf',dpi=300,transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e262598",
   "metadata": {},
   "source": [
    "### Load video frames for CalMS21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/data/users/eabe/hypernets/CalMS21/datasets/mouse001_task1_annotator1.mp4')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    " \n",
    "# Read until video is completed\n",
    "frames = []\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    frames.append(frame)   \n",
    "  # Break the loop\n",
    "  else: \n",
    "    break\n",
    " \n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41097339",
   "metadata": {},
   "source": [
    "## Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41097339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import cv2\n",
    "import gc\n",
    "mpl.use('agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_plt_im(t, X_pca,data_dict,full_state_z):   #\n",
    "    trail = 5\n",
    "    labels = full_state_z[t-trail:t+1]\n",
    "    cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "    bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "    ax = axs[0]\n",
    "    im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "    im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1,1])\n",
    "    ax.set_ylim([-1,1])\n",
    "    cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "    cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "    cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "    ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "    ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "    ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "    ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "    ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "    labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "    for n in range(7):\n",
    "        ax = axs[1]\n",
    "        im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 1')\n",
    "        ax = axs[2]\n",
    "        ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    images = np.frombuffer(fig.canvas.tostring_rgb(),\n",
    "                        dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    plt.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = make_plt_im(t, X_pca,data_dict)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# full_state_z = data_dict['annotations_test']\n",
    "full_state_z = cluster_labels\n",
    "X_pca = pca.fit_transform(R_bar)\n",
    "\n",
    "t = 80000; dt = 5000\n",
    "trail = 5\n",
    "labels = full_state_z[t-trail:t+1]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig(cfg.paths.fig_dir/'test.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(cluster_labels==2)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "##### initialize time points for animation and progressbar #####\n",
    "t = 0 ; dt = 10000\n",
    "full_state_z = cluster_labels\n",
    "state = 'I'\n",
    "time_range = np.arange(t,t+dt)#  np.where(cluster_labels==state)[0]#\n",
    "num_ticks = np.size(time_range)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(I)\n",
    "\n",
    "##### Put large arrays into shared memory #####\n",
    "X_pca_r = ray.put(X_pca)\n",
    "data_dict_r = ray.put(data_dict)\n",
    "full_state_z_r = ray.put(full_state_z)\n",
    "##### Loop over parameters appending process ids #####\n",
    "result_ids = []\n",
    "for t in time_range:\n",
    "    result_ids.append(make_plt_im.remote(t, X_pca_r, data_dict_r,full_state_z_r))\n",
    "\n",
    "##### pring progressbar and get results #####\n",
    "results_p = ray.get(result_ids)\n",
    "images = np.stack([results_p[i] for i in range(len(results_p))])\n",
    "\n",
    "##### Make video with opencv #####\n",
    "aniname = 'PCA_Evolve_{}.mp4'.format(state) \n",
    "\n",
    "\n",
    "vid_name = cfg.paths.fig_dir / aniname\n",
    "FPS = 30\n",
    "out = cv2.VideoWriter(vid_name.as_posix(), cv2.VideoWriter_fourcc(*'mp4v'), FPS, (images.shape[-2], images.shape[-3]))\n",
    "\n",
    "for fm in tqdm(range(images.shape[0])):\n",
    "    out.write(cv2.cvtColor(images[fm], cv2.COLOR_BGR2RGB))\n",
    "out.release()\n",
    "print('Making Animation {}: {}'.format(aniname, time.time()-start))\n",
    "del results_p, X_pca_r, data_dict_r, full_state_z_r\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 80000 ; dt = 5000\n",
    "labels = full_state_z[t:t+dt]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[t:t+dt,0],X_pca[t:t+dt,1],s=1,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,4,1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t:t+dt,:7]\n",
    "ani1_y = data_dict['inputs_test'][t:t+dt,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t:t+dt,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t:t+dt,21:28]\n",
    "labels = data_dict['annotations_test'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.axis('square')\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e9076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b90da290",
   "metadata": {},
   "source": [
    "# SLDS Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_params = cfg.dataset.ssm_params\n",
    "ssm_params['z_timescale'] = [0.98, 0.98, 0.98]\n",
    "seed = 42\n",
    "saved_evals = True\n",
    "normalize = False\n",
    "# Make SLDS with interesting dynamics parameters\n",
    "lds_dict = {}\n",
    "dstates_train_all, cstates_train_all, emissions_train_all = [], [], []\n",
    "dstates_val_all, cstates_val_all, emissions_val_all = [], [], []\n",
    "dstates_test_all, cstates_test_all, emissions_test_all = [], [], []\n",
    "As, bs = [], []\n",
    "timescales_default = [-.025, -.25, -.1, -.5, -.015, -.75]\n",
    "freq = jnp.zeros(len(timescales_default), dtype=complex)\n",
    "freq = freq.at[:].set(freq + 1j * jnp.array([.0075, .05, .025, .075, .01, .075]))\n",
    "set_eigvalues = jnp.array([\n",
    "    [jnp.exp(jnp.array(timescales_default)) + 2*jnp.pi*freq],\n",
    "    [jnp.exp(jnp.array(timescales_default)) - 2*jnp.pi*freq]\n",
    "]).squeeze().T\n",
    "p=0\n",
    "timescales = timescales_default\n",
    "# Create JAX random key\n",
    "key = jax.random.PRNGKey(seed)\n",
    "    # Setup for sampling discrete states\n",
    "num_states = ssm_params['n_disc_states']\n",
    "state_dim = ssm_params['latent_dim']\n",
    "emission_dim = ssm_params['obs_dim']\n",
    "slds = SLDS(num_states, state_dim, emission_dim)\n",
    "jit_sample = jit(slds.sample)\n",
    "for i in range(ssm_params['Nlds']):\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    # Create dynamics matrices for each discrete state\n",
    "    As_lds, bs_lds = [], []\n",
    "    Cs_lds, ds_lds = [], []\n",
    "    Qs_lds, Rs_lds = [], []\n",
    "\n",
    "    for n in range(ssm_params['n_disc_states']):\n",
    "        # Create dynamics matrix for this discrete state\n",
    "        if saved_evals:\n",
    "            key, subkey2 = jax.random.split(key)\n",
    "            A0 = create_dyn_mat(set_eigvalues[p], subkey2)\n",
    "        else:\n",
    "            neg = True if i % 2 == 0 else False\n",
    "            key, subkey2 = jax.random.split(key)\n",
    "            theta = jnp.pi/2 * jax.random.uniform(subkey2)/(timescales[p])\n",
    "            A0 = .95 * random_rotation(ssm_params['latent_dim'], theta=float(theta), neg=neg)\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        b = np.array(jax.random.uniform(subkey, (ssm_params['latent_dim'],), minval=-5.0, maxval=5.0))\n",
    "        As_lds.append(A0)\n",
    "        bs_lds.append(b)\n",
    "\n",
    "        # Create emission matrix for this discrete state\n",
    "        key, subkey = jax.random.split(key)\n",
    "        C = np.array(jax.random.normal(subkey, (ssm_params['obs_dim'], ssm_params['latent_dim'])))\n",
    "        d = jnp.zeros(ssm_params['obs_dim'])\n",
    "        Cs_lds.append(C)\n",
    "        ds_lds.append(np.array(d))\n",
    "\n",
    "        # Noise covariances\n",
    "        Q = jnp.eye(ssm_params['latent_dim']) * 0.1\n",
    "        R = jnp.eye(ssm_params['obs_dim']) * 0.1\n",
    "        Qs_lds.append(Q)\n",
    "        Rs_lds.append(R)\n",
    "\n",
    "        p += 1\n",
    "    # Stack to make (n_disc_states, latent_dim, latent_dim) arrays\n",
    "    As_lds = jnp.stack(As_lds)\n",
    "    bs_lds = jnp.stack(bs_lds)\n",
    "    Cs_lds = jnp.stack(Cs_lds)\n",
    "    ds_lds = jnp.stack(ds_lds)\n",
    "    Qs_lds = jnp.stack(Qs_lds)\n",
    "    Rs_lds = jnp.stack(Rs_lds)\n",
    "    \n",
    "    # Create transition matrix for discrete states\n",
    "    K = ssm_params['n_disc_states']\n",
    "    z_timescale = ssm_params['z_timescale'][i]\n",
    "    Ps = z_timescale * np.eye(K) + (1 - z_timescale)\n",
    "    Ps /= Ps.sum(axis=1, keepdims=True)\n",
    "    print(f\"Transition matrix for system {i}:\")\n",
    "    print(Ps)\n",
    "\n",
    "    # Initial discrete state distribution\n",
    "    pi0 = np.ones(K) / K\n",
    "\n",
    "    discr_params = DiscreteParamsSLDS(\n",
    "        initial_distribution=jnp.ones(num_states)/num_states,\n",
    "        transition_matrix=jnp.array(Ps),\n",
    "        proposal_transition_matrix=jnp.array(Ps)\n",
    "    )\n",
    "\n",
    "    lg_params = LGParamsSLDS(\n",
    "        initial_mean=jnp.ones(state_dim),\n",
    "        initial_cov=jnp.eye(state_dim),\n",
    "        dynamics_weights=As_lds,\n",
    "        dynamics_cov=Qs_lds,\n",
    "        dynamics_bias=bs_lds,\n",
    "        dynamics_input_weights=None,\n",
    "        emission_weights=Cs_lds,\n",
    "        emission_cov=Rs_lds,\n",
    "        emission_bias=None,\n",
    "        emission_input_weights=None\n",
    "    )\n",
    "\n",
    "    pre_params = ParamsSLDS(\n",
    "        discrete=discr_params,\n",
    "        linear_gaussian=lg_params\n",
    "    )\n",
    "\n",
    "    params = pre_params.initialize(num_states, state_dim, emission_dim)\n",
    "\n",
    "    ## Sample states and emissions Train Set\n",
    "    key, sub_key = jr.split(key)\n",
    "\n",
    "    dstates_train, cstates_train, emissions_train = slds.sample(params, sub_key, ssm_params['time_bins_train'])\n",
    "    _, counts = np.unique(dstates_train, return_counts=True)\n",
    "    p_states_x = counts / np.sum(counts)\n",
    "    print(f'Train {0}, state distribution: {p_states_x}')\n",
    "    \n",
    "    ## Sample states and emissions Val Set\n",
    "    key, sub_key = jr.split(key)\n",
    "    dstates_val, cstates_val, emissions_val = slds.sample(params, sub_key, ssm_params['time_bins_test'])\n",
    "    _, counts = np.unique(dstates_val, return_counts=True)\n",
    "    p_states_x = counts / np.sum(counts)\n",
    "    print(f'Val {0}, state distribution: {p_states_x}')\n",
    "    \n",
    "    ## Sample states and emissions Test Set\n",
    "    key, sub_key = jr.split(key)\n",
    "    dstates_test, cstates_test, emissions_test = slds.sample(params, sub_key, ssm_params['time_bins_test'])\n",
    "    _, counts = np.unique(dstates_test, return_counts=True)\n",
    "    p_states_x = counts / np.sum(counts)\n",
    "    print(f'Test {0}, state distribution: {p_states_x}')\n",
    "\n",
    "    if normalize:\n",
    "        cstates_train = cstates_train / np.max(np.abs(cstates_train), axis=0, keepdims=True)\n",
    "        cstates_val = cstates_val / np.max(np.abs(cstates_val), axis=0, keepdims=True)\n",
    "        cstates_test = cstates_test / np.max(np.abs(cstates_test), axis=0, keepdims=True)\n",
    "\n",
    "    cstates_train_all.append(cstates_train)\n",
    "    cstates_val_all.append(cstates_val)\n",
    "    cstates_test_all.append(cstates_test)\n",
    "    dstates_train_all.append(dstates_train)\n",
    "    dstates_val_all.append(dstates_val)\n",
    "    dstates_test_all.append(dstates_test)\n",
    "    emissions_train_all.append(emissions_train)\n",
    "    emissions_val_all.append(emissions_val)\n",
    "    emissions_test_all.append(emissions_test)\n",
    "    As.append(As_lds)\n",
    "    bs.append(bs_lds)\n",
    "\n",
    "    lds_dict['{}'.format(i)] = {\n",
    "        # 'model': hmm,\n",
    "        'params': params._asdict(),\n",
    "        'As': As_lds,\n",
    "        'bs': bs_lds,\n",
    "        'Cs': Cs_lds,\n",
    "        'ds': ds_lds\n",
    "    }\n",
    "As = jnp.stack(As)\n",
    "bs = jnp.stack(bs)\n",
    "cstates_train_all = jnp.stack(cstates_train_all)\n",
    "cstates_val_all = jnp.stack(cstates_val_all)\n",
    "cstates_test_all = jnp.stack(cstates_test_all)\n",
    "dstates_train_all = jnp.stack(dstates_train_all)\n",
    "dstates_val_all = jnp.stack(dstates_val_all)\n",
    "dstates_test_all = jnp.stack(dstates_test_all)\n",
    "emissions_train_all = jnp.stack(emissions_train_all)\n",
    "emissions_val_all = jnp.stack(emissions_val_all)\n",
    "emissions_test_all = jnp.stack(emissions_test_all)\n",
    "\n",
    "data_dict = {\n",
    "    \n",
    "    'timescales': timescales,\n",
    "    'As': As,\n",
    "    'bs': bs,\n",
    "    'cstates_train_all': cstates_train_all,\n",
    "    'cstates_val_all': cstates_val_all,\n",
    "    'cstates_test_all': cstates_test_all,\n",
    "    'dstates_train_all': dstates_train_all,\n",
    "    'dstates_val_all': dstates_val_all,\n",
    "    'dstates_test_all': dstates_test_all,\n",
    "    'emissions_train_all': emissions_train_all,\n",
    "    'emissions_val_all': emissions_val_all,\n",
    "    'emissions_test_all': emissions_test_all,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_dynamics_2d(dynamics_matrix,\n",
    "                     bias_vector,\n",
    "                     mins=(-40,-40),\n",
    "                     maxs=(40,40),\n",
    "                     npts=20,\n",
    "                     axis=None,\n",
    "                     **kwargs):\n",
    "    \"\"\"Utility to visualize the dynamics for a 2 dimensional dynamical system.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "\n",
    "        dynamics_matrix: 2x2 numpy array. \"A\" matrix for the system.\n",
    "        bias_vector: \"b\" vector for the system. Has size (2,).\n",
    "        mins: Tuple of minimums for the quiver plot.\n",
    "        maxs: Tuple of maximums for the quiver plot.\n",
    "        npts: Number of arrows to show.\n",
    "        axis: Axis to use for plotting. Defaults to None, and returns a new axis.\n",
    "        kwargs: keyword args passed to plt.quiver.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "        q: quiver object returned by pyplot\n",
    "    \"\"\"\n",
    "    assert dynamics_matrix.shape == (2, 2), \"Must pass a 2 x 2 dynamics matrix to visualize.\"\n",
    "    assert len(bias_vector) == 2, \"Bias vector must have length 2.\"\n",
    "\n",
    "    x_grid, y_grid = np.meshgrid(np.linspace(mins[0], maxs[0], npts), np.linspace(mins[1], maxs[1], npts))\n",
    "    xy_grid = np.column_stack((x_grid.ravel(), y_grid.ravel(), np.zeros((npts**2,0))))\n",
    "    dx = xy_grid.dot(dynamics_matrix.T) + bias_vector - xy_grid\n",
    "\n",
    "    if axis is not None:\n",
    "        q = axis.quiver(x_grid, y_grid, dx[:, 0], dx[:, 1], **kwargs)\n",
    "    else:\n",
    "        q = plt.quiver(x_grid, y_grid, dx[:, 0], dx[:, 1], **kwargs)\n",
    "    return q\n",
    "\n",
    "# Helper functions for plotting results\n",
    "def plot_trajectory(z, x, ax=None, ls=\"-\"):\n",
    "    import seaborn as sns\n",
    "    color_names = [\"windows blue\", \"red\", \"amber\", \"faded green\"]\n",
    "    colors = sns.xkcd_palette(color_names)\n",
    "    zcps = np.concatenate(([0], np.where(np.diff(z))[0] + 1, [z.size]))\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        ax = fig.gca()\n",
    "    for start, stop in zip(zcps[:-1], zcps[1:]):\n",
    "        ax.plot(x[start:stop + 1, 0],\n",
    "                x[start:stop + 1, 1],\n",
    "                lw=1, ls=ls,\n",
    "                color=colors[z[start] % len(colors)],\n",
    "                alpha=.75)\n",
    "    return ax\n",
    "\n",
    "def plot_observations(z, y, ax=None, ls=\"-\", lw=1,):\n",
    "    import seaborn as sns\n",
    "    colors = plt.get_cmap('turbo', len(np.unique(z)))\n",
    "    zcps = np.concatenate(([0], np.where(np.diff(z))[0] + 1, [z.size]))\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "        ax = fig.gca()\n",
    "    T, N = y.shape\n",
    "    t = np.arange(T)\n",
    "    for n in range(N):\n",
    "        for start, stop in zip(zcps[:-1], zcps[1:]):\n",
    "            ax.plot(t[start:stop + 1], y[start:stop + 1, n],\n",
    "                    lw=lw, ls=ls,\n",
    "                    color=colors(z[start]),\n",
    "                    alpha=1.0)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ssm_params['Nlds'], 2, figsize=(8, 4*ssm_params['Nlds']))\n",
    "for n in range(ssm_params['Nlds']):\n",
    "    plot_dynamics_2d(As[n][0], bs[n][0], axis=axs[n,0],mins=(-100,-100),maxs=(100,100))\n",
    "    plot_dynamics_2d(As[n][1], bs[n][1], axis=axs[n,1],mins=(-100,-100),maxs=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb93db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max = 1000\n",
    "n = 2\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 6))\n",
    "axs[0].plot(dstates_train_all[n,:T_max])\n",
    "axs[0].set_title('Discrete States')\n",
    "axs[1].plot(cstates_train_all[n,:T_max]/jnp.max(jnp.abs(cstates_train_all[n]), axis=0, keepdims=True))\n",
    "axs[1].set_title('Continuous States')\n",
    "axs[2].plot(emissions_train_all[n,:T_max]/jnp.max(jnp.abs(emissions_train_all[n]), axis=0, keepdims=True))\n",
    "axs[2].set_title('Emissions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c9776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c7d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28141cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
