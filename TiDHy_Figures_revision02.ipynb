{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0e1841-1962-46c0-a629-11e884c6c99c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd955540",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0\n",
    "os.environ[\"JAX_CAPTURED_CONSTANTS_REPORT_FRAMES\"]=\"-1\"\n",
    "from pathlib import Path\n",
    "import jax \n",
    "jax.config.update(\"jax_compilation_cache_dir\", (Path.cwd() / \"tmp/jax_cache\").as_posix())\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)\n",
    "try: \n",
    "    jax.config.update(\"jax_persistent_cache_enable_xla_caches\", \"xla_gpu_per_fusion_autotune_cache_dir\")\n",
    "except AttributeError:\n",
    "    pass  # Skip if not available in this JAX version\n",
    "\n",
    "try:\n",
    "    import blackjax\n",
    "except ModuleNotFoundError:\n",
    "    print('installing blackjax')\n",
    "    %pip install -qq blackjax\n",
    "    import blackjax\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "# from fastprogress.fastprogress import progress_bar\n",
    "from functools import partial\n",
    "\n",
    "# jax.config.update('jax_platform_name', 'cpu')\n",
    "from jax import random as  jr\n",
    "from jax import numpy as jnp\n",
    "from jax import jit, vmap\n",
    "from itertools import count\n",
    "from flax import nnx\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# device = 'gpu' if jax.lib.xla_bridge.get_backend().platform == 'gpu' else 'cpu'\n",
    "device = 'gpu' if jax.extend.backend.get_backend().platform == 'gpu' else 'cpu'\n",
    "n_gpus = jax.device_count(backend=device)\n",
    "print(f\"Using {n_gpus} device(s) on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eaebf-52c1-4b95-a6d9-34a1b1525b1b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from TiDHy.models.TiDHy_nnx_vmap import TiDHy\n",
    "from TiDHy.models.TiDHy_nnx_vmap_training import train_model, evaluate_record, load_model, get_latest_checkpoint_epoch, list_checkpoints\n",
    "from TiDHy.datasets.datasets_dynamax import *\n",
    "from TiDHy.datasets.load_data import load_data, stack_data\n",
    "from TiDHy.utils import io_dict_to_hdf5 as ioh5\n",
    "from TiDHy.utils.path_utils import *\n",
    "\n",
    "##### Plotting settings ######\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size':          10,\n",
    "                     'axes.linewidth':     2,\n",
    "                     'xtick.major.size':   5,\n",
    "                     'ytick.major.size':   5,\n",
    "                     'xtick.major.width':  2,\n",
    "                     'ytick.major.width':  2,\n",
    "                     'axes.spines.right':  False,\n",
    "                     'axes.spines.top':    False,\n",
    "                     'pdf.fonttype':       42,\n",
    "                     'ps.fonttype':        42,\n",
    "                     'xtick.labelsize':    10,\n",
    "                     'ytick.labelsize':    10,\n",
    "                     'figure.facecolor':   'white',\n",
    "                     'pdf.use14corefonts': False,  # Changed to False - we're embedding TrueType fonts\n",
    "                     'font.family':        'sans-serif',\n",
    "                    #  'font.family':        'Arial',\n",
    "                     'font.sans-serif':    'Arial',\n",
    "                     'font.serif':         'Arial',\n",
    "                     'axes.unicode_minus': True,  # Ensures proper minus sign rendering in PDFs\n",
    "                    })\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "clrs = ['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000']\n",
    "cmap = ListedColormap(clrs)\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def map_discrete_cbar(cmap,N):\n",
    "    cmap = plt.get_cmap(cmap,N+1)\n",
    "    bounds = np.arange(0,N+1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    return cmap, norm\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7389f",
   "metadata": {},
   "source": [
    "## Load and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'SLDS'\n",
    "# version = 'HierarchicalMultiTimescale'\n",
    "version = 'TiDHy'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 2\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd() / \"configs\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg.dataset.name}: {cfg.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data(cfg)\n",
    "# inputs_train = stack_data(data_dict['inputs_train'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "# inputs_test = stack_data(data_dict['inputs_test'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "inputs_test = data_dict['inputs_test'][None]\n",
    "max_seq_len = inputs_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ca81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNG\n",
    "rngs = nnx.Rngs(42)\n",
    "\n",
    "# Get model params as dict and unpack directly\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "# model_params.pop('batch_converge')\n",
    "model_params['input_dim'] = inputs_test.shape[-1]\n",
    "\n",
    "model = TiDHy(**model_params, rngs=rngs)\n",
    "# model.l0 = nnx.data(jnp.zeros(3))\n",
    "# model.loss_weights = nnx.data(jnp.ones(3))\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"input_dim: {model.input_dim}, r_dim: {model.r_dim}, r2_dim: {model.r2_dim}, mix_dim: {model.mix_dim}\")\n",
    "jit_model = jax.jit(model)\n",
    "# out = jit_model(inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=get_latest_checkpoint_epoch(cfg.paths.ckpt_dir)\n",
    "loaded_model = load_model(model,cfg.paths.ckpt_dir/f'epoch_{epoch:04d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d775ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = ioh5.load(cfg.paths.log_dir/'evaluation_results.h5')\n",
    "seq_len = natsorted(list(result_dict.keys()))\n",
    "W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])\n",
    "Ut = jnp.stack([result_dict[str(seq)]['Ut'].reshape((-1,)+ result_dict[str(seq)]['Ut'].shape[2:]) for seq in seq_len])\n",
    "W.shape, R2_hat.shape, R_hat.shape, Ut.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6cfb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "seq_len = [100,200,500,1000,2000,max_seq_len]\n",
    "# seq_len = [20000]\n",
    "for seq in tqdm(seq_len):\n",
    "    inputs_test = stack_data(data_dict['inputs_test'], sequence_length=seq, all_sequence_lengths=seq_len)\n",
    "    rng_key = loaded_model.rngs()\n",
    "    spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict_single = evaluate_record(loaded_model, inputs_test, rng_key)\n",
    "    result_dict['{}'.format(seq)] = result_dict_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in seq_len:\n",
    "    print(result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5c995",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.models.LSTM_baseline import LSTMBaseline\n",
    "from TiDHy.models.LSTM_training import (\n",
    "    create_optimizer,\n",
    "    train_epoch,\n",
    "    evaluate,\n",
    "    checkpoint_lstm_model,\n",
    "    load_lstm_checkpoint,\n",
    "    discover_lstm_checkpoint,\n",
    "    evaluate_lstm_record\n",
    ")\n",
    "import orbax.checkpoint as ocp\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "input_dim = inputs_test.shape[-1]\n",
    "hidden_dim = model_params['hidden_dim']\n",
    "num_layers = model_params['num_layers']\n",
    "dropout_rate = model_params['dropout_rate']\n",
    "print(input_dim)\n",
    "rngs = nnx.Rngs(0)\n",
    "model = LSTMBaseline(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=input_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout_rate=dropout_rate,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "optimizer_tx = create_optimizer(\n",
    "    learning_rate=cfg.train.learning_rate,\n",
    "    weight_decay=cfg.train.weight_decay,\n",
    "    use_schedule=cfg.train.use_schedule,\n",
    "    schedule_transition_steps=cfg.train.schedule_transition_steps,\n",
    "    schedule_decay_rate=cfg.train.schedule_decay,\n",
    ")\n",
    "optimizer = nnx.Optimizer(model, optimizer_tx, wrt=nnx.Param)\n",
    "latest_ckpt = discover_lstm_checkpoint(str(cfg.paths.ckpt_dir))\n",
    "model, optimizer, start_epoch = load_lstm_checkpoint(\n",
    "    model, optimizer, str(cfg.paths.ckpt_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, result_dict = evaluate_lstm_record(model, inputs_test,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987da442",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = result_dict['I']\n",
    "I_hat = result_dict['I_hat']\n",
    "H = result_dict['H']\n",
    "C = result_dict['C']\n",
    "ts = 0\n",
    "fig, axs = plt.subplots(3,1, figsize=(6,4), sharex=True)\n",
    "ax = axs[0]\n",
    "ax.plot(I[ts,:1000,-1])\n",
    "ax.plot(I_hat[ts,:1000,-1])\n",
    "ax = axs[1]\n",
    "ax.plot(H[ts,:1000,0])\n",
    "ax = axs[2]\n",
    "ax.plot(C[ts,:1000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9576996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2**cfg.dataset.ssm_params['Nlds']\n",
    "file_name = f'arhmm_K{K}_L{cfg.get(\"num_lags\",1)}_seed{cfg.seed}.h5'\n",
    "arhmm_data = ioh5.load(cfg.paths.log_dir / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42536db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arhmm_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd737e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arhmm_data['ground_truth_states_z']\n",
    "arhmm_data['ARHMM_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0aa1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arhmm_data['ARHMM_predictions'][:1000,1])\n",
    "plt.plot(arhmm_data['ground_truth_emissions'][:1000,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6c386",
   "metadata": {},
   "source": [
    "# Explore plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.utils import (\n",
    "    compute_rossler_ground_truth_timescales,\n",
    "    compare_discovered_to_ground_truth,\n",
    "    analyze_hierarchical_rossler_recovery,\n",
    "    analyze_time_varying_timescales,\n",
    "    plot_discovered_vs_ground_truth,\n",
    "    analyze_effective_timescales,\n",
    ")\n",
    "import cuml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ts = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.utils.analysis import analyze_mixture_timescales\n",
    "from TiDHy.utils.analysis_plotting import plot_timescale_distribution\n",
    "\n",
    "# Analyze timescales from trained model\n",
    "# timescale_analysis = analyze_mixture_timescales(model, dt=0.01)\n",
    "timescale_analysis = analyze_effective_timescales(result_dict[f'{seq_len[ts]}'], dt=1/30)\n",
    "\n",
    "print(f\"Discovered timescale range: {timescale_analysis['timescale_range']}\")\n",
    "print(f\"Unique timescales: {len(timescale_analysis['unique_timescales'])}\")\n",
    "# print(f\"Stable modes per component: {timescale_analysis['n_stable_per_timestep']}\")\n",
    "\n",
    "# Visualize timescale distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_timescale_distribution(timescale_analysis, ax=ax, bins=100)\n",
    "plt.show()\n",
    "\n",
    "# Plot eigenvalues in complex plane for each component\n",
    "from TiDHy.utils import plot_complex_eigenvalues\n",
    "eigenvalues_list = [spec['eigenvalues'] for spec in timescale_analysis['spectral_results_sample']]\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_complex_eigenvalues(eigenvalues_list, ax=ax,\n",
    "                        component_labels=[f'V_{i}' for i in range(model.mix_dim)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec83c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 3: Analyze Hypernetwork Usage\n",
    "\n",
    "from TiDHy.utils import analyze_hypernetwork_usage, plot_hypernetwork_usage\n",
    "\n",
    "# W = result_dict_single['W'].reshape(-1, result_dict_single['W'].shape[-1])\n",
    "ts = -1\n",
    "# Analyze which components are used\n",
    "usage_analysis = analyze_hypernetwork_usage(W[ts], threshold=0.01)\n",
    "\n",
    "print(f\"Active components: {usage_analysis['active_components']}/{usage_analysis['mix_dim']}\")\n",
    "print(f\"Entropy (normalized): {usage_analysis['entropy']:.3f}\")\n",
    "print(f\"Sparsity: {usage_analysis['sparsity']:.2%}\")\n",
    "print(f\"Dominant component: {usage_analysis['dominant_component']}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "plot_hypernetwork_usage(W, ax=axes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b221eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 4: Combined Model Selection Workflow\n",
    "\n",
    "from TiDHy.utils import (\n",
    "    analyze_latent_dimension,\n",
    "    analyze_mixture_timescales,\n",
    "    analyze_hypernetwork_usage\n",
    ")\n",
    "\n",
    "# 1. Determine effective r_dim\n",
    "R_analysis = analyze_latent_dimension(R_hat[ts])\n",
    "effective_r_dim = R_analysis['effective_dim_95']\n",
    "\n",
    "# 2. Determine effective r2_dim  \n",
    "R2_analysis = analyze_latent_dimension(R2_hat[ts])\n",
    "effective_r2_dim = R2_analysis['effective_dim_95']\n",
    "\n",
    "# 3. Discover number of timescales (effective mix_dim)\n",
    "timescale_analysis = analyze_mixture_timescales(model, dt=0.01)\n",
    "n_unique_timescales = len(timescale_analysis['unique_timescales'])\n",
    "\n",
    "# 4. Verify with hypernetwork usage\n",
    "usage_analysis = analyze_hypernetwork_usage(W[ts])\n",
    "active_components = usage_analysis['active_components']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL SELECTION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Recommended r_dim:   {effective_r_dim} (current: {model.r_dim})\")\n",
    "print(f\"Recommended r2_dim:  {effective_r2_dim} (current: {model.r2_dim})\")\n",
    "print(f\"Recommended mix_dim: {active_components} (current: {model.mix_dim})\")\n",
    "print(f\"Discovered timescales: {n_unique_timescales}\")\n",
    "print(f\"Timescale range: {timescale_analysis['timescale_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5584b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "dt = 50000\n",
    "feature = 10\n",
    "fig, axs = plt.subplots(3,1,figsize=(6,6))\n",
    "ax = axs[0]\n",
    "ax.plot(I[ts,t:t+dt,feature],label='I1')\n",
    "ax.plot(Ihat[ts,t:t+dt,feature],label='Ihat1')\n",
    "ax = axs[1]\n",
    "ax.plot(R_hat[ts,t:t+dt,:],label='R_hat')\n",
    "ax = axs[2]\n",
    "ax.plot(R2_hat[ts,t:t+dt,:],label='R_2hat')\n",
    "# ax.plot(Ihat[ts,:10000,2],label='Ihat3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(R_hat[ts, :,0],\n",
    "        R_hat[ts, :,1],\n",
    "        R_hat[ts, :,2],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(data_dict['inputs_train'][:,0],\n",
    "        data_dict['inputs_train'][:,1],\n",
    "        data_dict['inputs_train'][:,2],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(data_dict['states_x_train'][:,0],\n",
    "        data_dict['states_x_train'][:,1],\n",
    "        data_dict['states_x_train'][:,2],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(data_dict['states_x_train'][:,3],\n",
    "        data_dict['states_x_train'][:,4],\n",
    "        data_dict['states_x_train'][:,5],\n",
    "        lw=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5aa833",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_results = analyze_effective_timescales(result_dict_single, dt=1/30)\n",
    "effective_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "ts = 0\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(R_hat[ts])\n",
    "umap = cuml.manifold.UMAP(n_components=6, n_neighbors=15, min_dist=0.01, metric='euclidean', random_state=42)\n",
    "reduced_data = umap.fit_transform(X_scaled)\n",
    "# reduced_data = umap.fit_transform(R2_hat[ts])\n",
    "\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=20, cluster_selection_epsilon=0.75, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "labels = clusterer.labels_\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n",
    "\n",
    "soft_label = jnp.argmax(soft_clusters[:,1:].get() if isinstance(soft_clusters, cupy.ndarray) else soft_clusters[:,1:], axis=1)\n",
    "reduced_data = reduced_data.get() if isinstance(reduced_data, cupy.ndarray) else reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_state_z = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8]]\n",
    "def map_discrete_cbar(colors):\n",
    "\tN = len(colors)  # Number of discrete colors\n",
    "\tcmap = mpl.colors.ListedColormap(colors)\n",
    "\tbounds = np.arange(-0.5, N + .5, 1)  # [-0.5, 0.5, 1.5, ..., N-0.5]\n",
    "\tnorm = mpl.colors.BoundaryNorm(bounds, N)\n",
    "\treturn cmap, norm\n",
    "\n",
    "cmap2, norm = map_discrete_cbar(clrs_b)\n",
    "\n",
    "fig = plt.figure(figsize=(4.5,3), dpi=300)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=full_state_z, cmap=cmap2, alpha=.2, rasterized=True)\n",
    "# ax.scatter(reduced_data[:1000,0], reduced_data[:1000,1], reduced_data[:1000,2], c=np.arange(1000), cmap='turbo', alpha=0.1)\n",
    "cbar = fig.colorbar(im, boundaries=norm.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "# cbar.set_ticks(ticks)\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('UMAP 1',fontsize=fontsize)\n",
    "ax.set_ylabel('UMAP 2',fontsize=fontsize)\n",
    "ax.set_zlabel('UMAP 3',fontsize=fontsize)\n",
    "ax.set_xlim([-5.5,5.5])\n",
    "ax.set_ylim([-5.5,5.5])\n",
    "ax.set_zlim([-5.5,5.5])\n",
    "plt.show()\n",
    "fig.savefig(cfg.paths.fig_dir/'UMAP_timescales.pdf',dpi=300, transparent=True)\n",
    "print('✅ Figure saved to ', cfg.paths.fig_dir/'UMAP_timescales.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df481e9",
   "metadata": {},
   "source": [
    "# Load SLDS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c622af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[8,8,1,1,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "# cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b\n",
    "import itertools\n",
    "states_x_test = data_dict['states_x_test']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "# states_z_test = data_dict['states_z']\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "##### Set up combinatorics of timescales #####\n",
    "lst = list(itertools.product([1, 0], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros(ssm_params['time_bins_test'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,states_z_test)] = n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(full_state_z, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a021c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "ts = 0\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(R_hat[ts])\n",
    "umap = cuml.manifold.UMAP(n_components=10, n_neighbors=1000, min_dist=0.0, metric='euclidean', random_state=42)\n",
    "reduced_data = umap.fit_transform(X_scaled)\n",
    "# reduced_data = umap.fit_transform(R2_hat[ts])\n",
    "# reduced_data = PCA(n_components=8).fit_transform(R_hat[ts])\n",
    "\n",
    "# clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=100, cluster_selection_epsilon=0.075, prediction_data=True)\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=5, cluster_selection_epsilon=0.075, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "labels = clusterer.labels_\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n",
    "\n",
    "soft_label = jnp.argmax(soft_clusters[:,1:].get() if isinstance(soft_clusters, cupy.ndarray) else soft_clusters[:,1:], axis=1)\n",
    "reduced_data = reduced_data.get() if isinstance(reduced_data, cupy.ndarray) else reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_state_z = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "def map_discrete_cbar(colors):\n",
    "\tN = len(colors)  # Number of discrete colors\n",
    "\tcmap = mpl.colors.ListedColormap(colors)\n",
    "\tbounds = np.arange(-0.5, N + .5, 1)  # [-0.5, 0.5, 1.5, ..., N-0.5]\n",
    "\tnorm = mpl.colors.BoundaryNorm(bounds, N)\n",
    "\treturn cmap, norm\n",
    "\n",
    "# full_state_z = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000',\n",
    "                 '#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0'])\n",
    "clrs_1 = clrs[[0,1,2,9,4,6,7,8]]\n",
    "clrs_2 = clrs[:len(np.unique(soft_label))]\n",
    "def map_discrete_cbar(colors):\n",
    "\tN = len(colors)  # Number of discrete colors\n",
    "\tcmap = mpl.colors.ListedColormap(colors)\n",
    "\tbounds = np.arange(-0.5, N + .5, 1)  # [-0.5, 0.5, 1.5, ..., N-0.5]\n",
    "\tnorm = mpl.colors.BoundaryNorm(bounds, N)\n",
    "\treturn cmap, norm\n",
    "\n",
    "cmap1, norm1 = map_discrete_cbar(clrs_1)\n",
    "cmap2, norm2 = map_discrete_cbar(clrs_2)\n",
    "\n",
    "fig = plt.figure(figsize=(8.5,3), dpi=300)\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=full_state_z, cmap=cmap1, alpha=.05, rasterized=True)\n",
    "# ax.scatter(reduced_data[:1000,0], reduced_data[:1000,1], reduced_data[:1000,2], c=np.arange(1000), cmap='turbo', alpha=0.1)\n",
    "cbar = fig.colorbar(im, boundaries=norm1.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('UMAP 1',fontsize=fontsize)\n",
    "ax.set_ylabel('UMAP 2',fontsize=fontsize)\n",
    "ax.set_zlabel('UMAP 3',fontsize=fontsize)\n",
    "ax.set_xlim([-5.5,5.5])\n",
    "ax.set_ylim([-5.5,5.5])\n",
    "ax.set_zlim([-5.5,5.5])\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=soft_label, cmap=cmap2, alpha=.01, rasterized=True)\n",
    "cbar = fig.colorbar(im, boundaries=norm2.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(soft_label)),1))\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('UMAP 1',fontsize=fontsize)\n",
    "ax.set_ylabel('UMAP 2',fontsize=fontsize)\n",
    "ax.set_zlabel('UMAP 3',fontsize=fontsize)\n",
    "ax.set_xlim([-5.5,5.5])\n",
    "ax.set_ylim([-5.5,5.5])\n",
    "ax.set_zlim([-5.5,5.5])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(cfg.paths.fig_dir/'UMAP_Clustering.pdf',dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "ts = 0\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(R_hat[ts])\n",
    "# umap = cuml.manifold.UMAP(n_components=10, n_neighbors=1000, min_dist=0.0, metric='euclidean', random_state=42)\n",
    "# reduced_data = umap.fit_transform(X_scaled)\n",
    "# reduced_data = umap.fit_transform(R2_hat[ts])\n",
    "reduced_data = PCA().fit_transform(R_hat[ts])\n",
    "\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=100, cluster_selection_epsilon=0.025, prediction_data=True)\n",
    "# clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=5, cluster_selection_epsilon=0.075, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "labels = clusterer.labels_\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n",
    "\n",
    "soft_label = jnp.argmax(soft_clusters[:,1:].get() if isinstance(soft_clusters, cupy.ndarray) else soft_clusters[:,1:], axis=1)\n",
    "reduced_data = reduced_data.get() if isinstance(reduced_data, cupy.ndarray) else reduced_data\n",
    "print(np.unique(soft_label, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_state_z = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "def map_discrete_cbar(colors):\n",
    "\tN = len(colors)  # Number of discrete colors\n",
    "\tcmap = mpl.colors.ListedColormap(colors)\n",
    "\tbounds = np.arange(-0.5, N + .5, 1)  # [-0.5, 0.5, 1.5, ..., N-0.5]\n",
    "\tnorm = mpl.colors.BoundaryNorm(bounds, N)\n",
    "\treturn cmap, norm\n",
    "\n",
    "# full_state_z = data_dict['annotations_test'][:reduced_data.shape[0]]\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000',\n",
    "                 '#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0'])\n",
    "clrs_1 = clrs[[0,1,2,9,4,6,7,8]]\n",
    "clrs_2 = clrs[:len(np.unique(soft_label))]\n",
    "def map_discrete_cbar(colors):\n",
    "\tN = len(colors)  # Number of discrete colors\n",
    "\tcmap = mpl.colors.ListedColormap(colors)\n",
    "\tbounds = np.arange(-0.5, N + .5, 1)  # [-0.5, 0.5, 1.5, ..., N-0.5]\n",
    "\tnorm = mpl.colors.BoundaryNorm(bounds, N)\n",
    "\treturn cmap, norm\n",
    "\n",
    "cmap1, norm1 = map_discrete_cbar(clrs_1)\n",
    "cmap2, norm2 = map_discrete_cbar(clrs_2)\n",
    "\n",
    "fig = plt.figure(figsize=(8.5,3), dpi=300)\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=full_state_z, cmap=cmap1, alpha=.1, rasterized=True)\n",
    "# ax.scatter(reduced_data[:1000,0], reduced_data[:1000,1], reduced_data[:1000,2], c=np.arange(1000), cmap='turbo', alpha=0.1)\n",
    "cbar = fig.colorbar(im, boundaries=norm1.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('PCA 1',fontsize=fontsize)\n",
    "ax.set_ylabel('PCA 2',fontsize=fontsize)\n",
    "ax.set_zlabel('PCA 3',fontsize=fontsize)\n",
    "# ax.set_xlim([-5.5,5.5])\n",
    "# ax.set_ylim([-5.5,5.5])\n",
    "# ax.set_zlim([-5.5,5.5])\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "im = ax.scatter(reduced_data[:,0], reduced_data[:,1], reduced_data[:,2], c=soft_label, cmap=cmap2, alpha=.01, rasterized=True)\n",
    "cbar = fig.colorbar(im, boundaries=norm2.boundaries, spacing='uniform', extend='neither', aspect=15, pad=.2, shrink=0.75)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(soft_label)),1))\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "cbar.solids.set_alpha(1)\n",
    "ax.set_xlabel('PCA 1',fontsize=fontsize)\n",
    "ax.set_ylabel('PCA 2',fontsize=fontsize)\n",
    "ax.set_zlabel('PCA 3',fontsize=fontsize)\n",
    "# ax.set_xlim([-5.5,5.5])\n",
    "# ax.set_ylim([-5.5,5.5])\n",
    "# ax.set_zlim([-5.5,5.5])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(cfg.paths.fig_dir/'PCA_Clustering.pdf',dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d4e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff84cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_results = analyze_effective_timescales(result_dict_single, dt=1)\n",
    "effective_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0cc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(R_hat[0])\n",
    "umap = cuml.manifold.UMAP(n_components=3, n_neighbors=15, min_dist=0.05, metric='euclidean')\n",
    "reduced_data = umap.fit_transform(X_scaled)\n",
    "\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=100, min_samples=20, cluster_selection_epsilon=0.25, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "labels = clusterer.labels_\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n",
    "\n",
    "soft_label = jnp.argmax(soft_clusters[:,1:], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(reduced_data[:,0],\n",
    "        reduced_data[:,1],\n",
    "        reduced_data[:,2],\n",
    "        c=full_state_z, cmap=cmap_b, alpha=0.05)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5434c",
   "metadata": {},
   "source": [
    "# Save Multi Seq Len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2664017",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slds_dict = ioh5.load('/data2/users/eabe/TiDHy/SSM/Debug/run_id=SLDS_baseline/logs/ssm_slds_test_full_6D_8K_42seed.h5')\n",
    "rslds_dict = ioh5.load('/data2/users/eabe/TiDHy/SSM/Debug/run_id=SLDS_baseline/logs/ssm_rslds_test_full_6D_8K_42seed.h5')\n",
    "SLDS_seed = 42\n",
    "SLDS_latents = slds_dict['SLDS_latents']\n",
    "SLDS_states = slds_dict['SLDS_states']\n",
    "SLDS_emission = slds_dict['SLDS_emission']\n",
    "rSLDS_latents = rslds_dict['rSLDS_latents']\n",
    "rSLDS_states = rslds_dict['rSLDS_states']\n",
    "rSLDS_emission = rslds_dict['rSLDS_emission']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537481f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=2\n",
    "reg_variables = [\n",
    "    result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['W'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_bar'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['I'],\n",
    "    SLDS_latents,\n",
    "    rSLDS_latents,\n",
    "    ]\n",
    "\n",
    "labels = ['R2_hat','W','R_bar','R_hat','I','SLDS','rSLDS']\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    neigh = RidgeClassifierCV()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "\n",
    "# state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z],axis=0)\n",
    "state_compare = np.stack([rSLDS_states, SLDS_states, best_pred,full_state_z],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(SLDS_latents[:1000])\n",
    "# plt.plot(rSLDS_latents[:1000])\n",
    "plt.plot(slds_dict['ground_truth_states_z'][:1000])\n",
    "plt.plot(full_state_z[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_comps=ssm_params['latent_dim']\n",
    "t=1500; dt=200\n",
    "    \n",
    "fig = plt.figure(constrained_layout=True, figsize=(7,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=5,ncols=4, figure=fig, wspace=.45,hspace=.2)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=6,ncols=1,subplot_spec=gs0[:,:2],wspace=.1,hspace=.2)\n",
    "# gs01 = gridspec.GridSpecFromSubplotSpec(3,1, subplot_spec=gs0[0:,:1],wspace=.05,hspace=.8)\n",
    "axs = np.array([fig.add_subplot(gs00[:-3,0]),\n",
    "                fig.add_subplot(gs00[-3:-2,0]),\n",
    "                fig.add_subplot(gs00[-2:,0]),\n",
    "                fig.add_subplot(gs0[:3,2:]),\n",
    "                fig.add_subplot(gs0[3:,2:])])\n",
    "axs[4].sharex(axs[3])\n",
    "ts = -1\n",
    "##### Pannel a #####\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'][0]#.reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'][0]#.reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "ax = axs[0]\n",
    "spacing = 1\n",
    "xrange = 100\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z)))\n",
    "linestyle =  (0, (5, 1))#(0, (5, 5))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=2,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing, linestyle=linestyle, color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "# ax.set_yticks(hlines_I)\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "##### Pannel c #####\n",
    "ax = axs[3]\n",
    "count=0\n",
    "acc_slds = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_rslds = accuracy_score(full_state_z, rSLDS_states)\n",
    "acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "spacing = .5\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'][0]#.reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'][0]#.reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "hlines_x,hlines_Rhat = [],[]\n",
    "for p in range(ssm_params['Nlds']):\n",
    "    # states_x_cca = states_x_train[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    # states_x_cca = states_x_test[(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    states_x_cca = states_x_test[p]\n",
    "    # states_x_cca = states_x_test[:,p:p+1]\n",
    "    cca = CCA(n_components=ssm_params['latent_dim'],max_iter=1000)\n",
    "    X_c,Y_c = cca.fit_transform(states_x_cca, R_hat)\n",
    "    cca_coefficient = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=n_comps)\n",
    "    x_w = cca.x_weights_\n",
    "    y_w = cca.y_weights_\n",
    "    cca_angles = [np.rad2deg(angle_between(X_c[:,n],Y_c[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_x = [np.rad2deg(angle_between(X_c[:,n],states_x_cca[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_r = [np.rad2deg(angle_between(Y_c[:,n],R_hat[:,n])) for n in range(n_comps)]\n",
    "    for n in range(n_comps):\n",
    "        print('comp {}, cc: {:.03}, ang: {:.03}, ang_x:{:.03}, ang_r:{:.03}'.format(n,cca_coefficient[n],cca_angles[n],cca_angles_x[n],cca_angles_r[n]))\n",
    "\n",
    "    for i in range(X_c.shape[-1]):\n",
    "        mean_centered_x = X_c[t:t+dt,i] - np.mean(X_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_x=mean_centered_x/(np.max(np.abs(mean_centered_x)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_x + count/spacing,color='k', lw=2,zorder=1)\n",
    "        hlines_x.append(np.mean(mean_centered_x + count/spacing,axis=0))\n",
    "        mean_centered_Rhat = Y_c[t:t+dt,i] - np.mean(Y_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_Rhat=mean_centered_Rhat/(np.max(np.abs(mean_centered_Rhat)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_Rhat + count/spacing,linestyle=linestyle,color='r', lw=1.5,zorder=2,label='$\\hat{{r}}_{{{}}}$={:.02}'.format(n,cca_coefficient[n]),alpha=1)\n",
    "        hlines_Rhat.append(np.mean(mean_centered_Rhat + count/spacing,axis=0))\n",
    "        ax.text(x=dt+15,y=hlines_x[count],s='cc = {:.02}'.format(cca_coefficient[i]),fontsize=fontsize-2)\n",
    "        count += 1\n",
    "    ax.set_yticks(hlines_x) \n",
    "    ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "    # ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "    ax.set_ylabel('latent variables',fontsize=fontsize)\n",
    "# ax.set_xticks([])\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps', fontsize=fontsize)\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*6))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*6+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "##### Pannel b #####\n",
    "ax = axs[1]\n",
    "As = data_dict['As']#.reshape(-1,result_dict['{}'.format(seq_len[ts])]['As'].shape[-1])\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "# timescales_As = 1/np.abs(np.log(np.real(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'][0]#.reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'][0]#.reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "min_error_idx = np.array([np.argmin(np.abs(timescales.reshape(-1) - timescales_As.reshape(-1)[n])) for n in range(timescales_As.shape[0])])\n",
    "min_error_timescales = timescales[min_error_idx]\n",
    "# timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "ax.scatter(x=min_error_timescales,y=np.abs(evals_Uhat_all.reshape(-1)[min_error_idx]), marker='x',\n",
    "            c='k',s=50,alpha=1,edgecolor='None',zorder=3)\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlim(0.25,2.5e2)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n",
    "\n",
    "######### Accuracy and reconstruction error #########\n",
    "ax = axs[2]\n",
    "Input_Errors_SLDS = np.abs(slds_dict['ground_truth_inputs']-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(rslds_dict['ground_truth_inputs']-rSLDS_emission)\n",
    "Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'][0])\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs[1],label='TiDHy')\n",
    "ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs[1],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.legend(frameon=False,fontsize=fontsize-3,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "##### Pannel d #####\n",
    "ax = axs[4]\n",
    "ylabels_states = ['rSLDS \\n {:02}%'.format(int(np.round(acc_rslds*100))),\n",
    "                  'SLDS \\n {:02}%'.format(int(np.round(acc_slds*100))),\n",
    "                  'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),\n",
    "                  'True']\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,4,1))\n",
    "ax.set_yticklabels(ylabels_states,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=-.2)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Benchmark_Compare3.pdf',dpi=300)\n",
    "# fig.savefig(fig_path/'{}_Fig2_V2.pdf'.format(nfig),dpi=300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(slds_dict['ground_truth_inputs'][:1000,0],'k', label='input 0',alpha=1.0)\n",
    "plt.plot(rslds_dict['rSLDS_emission'][:1000,0],'r', label='rSLDS emission 0',alpha=0.7)\n",
    "plt.plot(slds_dict['SLDS_emission'][:1000,0],'m', label='SLDS emission 0',alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=1\n",
    "best_pred_all = []\n",
    "acc_ssm = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_TiDHy_all = []\n",
    "for ts in range(len(seq_len)):\n",
    "    reg_variables = [\n",
    "        # np.concatenate([result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['W']],axis=-1)\n",
    "        result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "        # q_lem_x,\n",
    "        ]\n",
    "\n",
    "    labels = ['R2_hat']\n",
    "    # reg_variables = W\n",
    "    max_acc = 0\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        print(labels[k],scores)\n",
    "        acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "        acc_TiDHy_all.append(acc_TiDHy)\n",
    "        best_pred_all.append(y_pred)\n",
    "\n",
    "state_compare = np.concatenate([SLDS_states[None,:],np.stack(best_pred_all),full_state_z[None,:]],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbac2ec",
   "metadata": {},
   "source": [
    "## Load multi-seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "result_dict_all = {}\n",
    "for n in range(len(configs)):\n",
    "    cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "    default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "    cfg = load_cfg(cfg_path, default_model_config)\n",
    "    cfg, result_dict = run_seq_len_model(seq_len, model, data_dict, device, 1500, cfg, rerun=False)\n",
    "    result_dict_all['seed_{}'.format(cfg.dataset.ssm_params.seed)] = result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values\n",
    "# seeds = np.arange(10,10+len(configs))\n",
    "seed = 10\n",
    "ts = 200\n",
    "\n",
    "##### Set up combinatorics of timescales #####\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "lst = list(itertools.product([0, 1], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros((len(seeds),ssm_params['time_bins_test']),dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for i, seed in enumerate(seeds):\n",
    "    for n in range(len(lst)):\n",
    "        full_state_z[i,np.apply_along_axis(lambda x: np.all(x == lst[n]),0,result_dict_all[f'seed_{seed}'][f'{ts}']['states_z_test'])] = n\n",
    "    \n",
    "I_all = np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['I'] for seed in seeds])\n",
    "I_hat_all = np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['I_hat'] for seed in seeds])\n",
    "R2_hat_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R2_hat'] for ts in seq_len])for seed in seeds])  # (n_seeds, Time, Z_dim)\n",
    "R_bar_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R_bar'] for ts in seq_len]) for seed in seeds])   # (n_seeds, TempWinN, Time, r2_dim)\n",
    "R_hat_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['R_hat'] for ts in seq_len]) for seed in seeds])   # (n_seeds, TempWinN, Time, r_dim)\n",
    "W_all = np.stack([np.stack([result_dict_all[f'seed_{seed}'][f'{ts}']['W'] for ts in seq_len]) for seed in seeds])           # (n_seeds, TempWinN, Time, r_dim)\n",
    "As_all = np.stack([np.stack([v for v in result_dict_all[f'seed_{seed}'][f'{ts}']['As'].values()]) for seed in seeds])\n",
    "result_dict_all[f'seed_{seed}'][f'{ts}'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# from ssm.util import random_rotation, find_permutation\n",
    "inputs_train_SLDS=data_dict['inputs_train']\n",
    "inputs_test_SLDS=data_dict['inputs_test']\n",
    "seed = cfg.dataset.ssm_params['seed']\n",
    "TotalTime = cfg.dataset.ssm_params.time_bins_test\n",
    "# SLDS_path = Path('/data/users/eabe/hypernets/SLDS/TiDHy_SLDS/L1ShortWin/L1_alpha=0.001,dataset.ssm_params.seed=10,dataset.train.sequence_length=200')\n",
    "N = inputs_train_SLDS.shape[-1]\n",
    "K = len(np.unique(full_state_z))\n",
    "D = data_dict['states_x_test'].shape[-1]\n",
    "SLDS_latents = np.zeros((len(seeds),TotalTime,D))\n",
    "SLDS_states = np.zeros((len(seeds),TotalTime))\n",
    "SLDS_emission = np.zeros((len(seeds),TotalTime,N))\n",
    "rSLDS_latents = np.zeros((len(seeds),TotalTime,D))\n",
    "rSLDS_states = np.zeros((len(seeds),TotalTime))\n",
    "rSLDS_emission = np.zeros((len(seeds),TotalTime,N))\n",
    "for k, seed in enumerate(seeds):\n",
    "    SLDS_path = configs[k].parent#/'SSM/DPC_SSM/Benchmark/dataset.ssm_params.seed={}/'.format(seed)\n",
    "    with open(SLDS_path/'ssm_slds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "    # with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "        slds = pickle.load(handle)\n",
    "        posterior = pickle.load(handle)\n",
    "        SLDS_latents[k] = pickle.load(handle)\n",
    "        SLDS_states[k] = pickle.load(handle)\n",
    "        SLDS_emission[k] = pickle.load(handle)\n",
    "\n",
    "    with open(SLDS_path/'ssm_rslds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "    # with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "        slds = pickle.load(handle)\n",
    "        posterior = pickle.load(handle)\n",
    "        rSLDS_latents[k] = pickle.load(handle)\n",
    "        rSLDS_states[k] = pickle.load(handle)\n",
    "        rSLDS_emission[k] = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=-1\n",
    "\n",
    "labels = seeds = [42]\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "scores_all = []\n",
    "pred_all = np.zeros((len(seeds),len(seq_len),cfg.dataset.ssm_params.time_bins_test))\n",
    "for k in range(len(seeds)):\n",
    "    for ts in range(len(seq_len)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(R2_hat_all[k,ts], full_state_z[k].reshape(-1), test_size=0.25, random_state=42)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        y_pred = neigh.predict(R2_hat_all[k,ts])\n",
    "        pred_all[k,ts] = y_pred\n",
    "        scores_all.append(accuracy_score(full_state_z[k].reshape(-1),y_pred))\n",
    "        print(labels[k],seq_len[ts],scores)\n",
    "        if (scores > max_acc) & (labels[k] != 'I'):\n",
    "            max_acc = scores\n",
    "            best_label = k\n",
    "            best_pred = y_pred\n",
    "\n",
    "# state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z[best_label]],axis=0)\n",
    "\n",
    "scores_all = np.stack(scores_all).reshape(len(seeds),len(seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f817574",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(8,6))\n",
    "gs0 = gridspec.GridSpec(nrows=4,ncols=2, figure=fig, wspace=.15,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs0[:2,0]),fig.add_subplot(gs0[2:,0]),fig.add_subplot(gs0[:2,1]),fig.add_subplot(gs0[2:,1])])\n",
    "t=1500; dt=1000\n",
    "# seq_len=[100,200,500]\n",
    "ts = 1\n",
    "Input_Errors_SLDS = (I_all[k]-SLDS_emission[k])\n",
    "ax = axs[-1]\n",
    "Input_Errors_SLDS = np.mean([np.abs(I_all[k]-SLDS_emission[k]) for k in range(len(seeds))],axis=-1)\n",
    "Input_Errors_rSLDS = np.mean([np.abs(I_all[k]-rSLDS_emission[k]) for k in range(len(seeds))],axis=-1)\n",
    "err_slds  = np.mean([100*(1-accuracy_score(full_state_z[k], SLDS_states[k])) for k in range(len(seeds))])\n",
    "err_rslds = np.mean([100*(1-accuracy_score(full_state_z[k], rSLDS_states[k])) for k in range(len(seeds))])\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors =np.stack([np.abs(I_all[seed,ts]-I_hat_all[seed,ts]) for seed in range(len(seeds))])\n",
    "    err_TiDHy = np.mean([100*(1-accuracy_score(full_state_z[k], pred_all[k,ts])) for k in range(len(seeds))])\n",
    "    ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs_b[ts],label='TiDHy')\n",
    "    ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs_b[ts],xerr=np.mean(np.std(Input_Errors,axis=-1)),yerr=np.mean(np.std(Input_Errors,axis=0)))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=np.mean(err_rslds), s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=np.mean(err_rslds), ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.tick_params(axis='y', labelsize=fontsize-2)\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "tts+=1\n",
    "ax.annotate('SLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[2],fontsize=fontsize-2)\n",
    "tts += 1\n",
    "ax.annotate('rSLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[6],fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs[2]\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "# timescales_As = 1/np.abs(np.log(np.real(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    for seed in seeds: \n",
    "        R_bar = result_dict_all['seed_{}'.format(seed)]['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "        R_hat = result_dict_all['seed_{}'.format(seed)]['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "        ##### Plottign Eigenvalues #####\n",
    "        Uhat_all = []\n",
    "        for state in range(len(np.unique(full_state_z[0]))):\n",
    "            # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "            # model = VAR(rhat2)\n",
    "            # results = model.fit(maxlags=20,ic='aic')\n",
    "            # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "            # Uhat_all.append(eig_Ut.reshape(-1))\n",
    "        # for p in range(ssm_params.Nlds):\n",
    "        #     for state in range(ssm_params.n_disc_states):\n",
    "                # rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "            rhat2 = R_hat[np.where(full_state_z[0]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "\n",
    "        Uhat_all = np.stack(Uhat_all)\n",
    "        # evals_Uhat_all = np.hstack(Uhat_all)\n",
    "        # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "        # timescales = np.real(np.log(Uhat_all))\n",
    "        evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "        timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "        # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "        min_error_idx = np.array([np.argmin(np.abs(timescales.reshape(-1) - timescales_As.reshape(-1)[n])) for n in range(timescales_As.shape[0])])\n",
    "        min_error_timescales = timescales[min_error_idx]\n",
    "        ax.scatter(x=min_error_timescales,y=np.abs(evals_Uhat_all.reshape(-1)[min_error_idx])+ts,\n",
    "                c=clrs_b[ts],alpha=.75,edgecolor='None',marker='x',s=25,zorder=3)\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "        ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    ax.set_xscale('symlog',linthresh=.1)\n",
    "    ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "    ax.set_ylabel('T',fontsize=fontsize,labelpad=-2)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=.015)\n",
    "y = .99\n",
    "dy = 0.075\n",
    "ax.annotate('System 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-2)\n",
    "ax.annotate('System 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-2)\n",
    "ax.annotate('System 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-2)\n",
    "\n",
    "ax=axs[0]\n",
    "spacing= 5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xlim([0,dt])\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(1.05,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "\n",
    "# legend = ax.legend(['T={}'.format(seq_len[p]) for p in range(len(seq_len))],frameon=False,fontsize=fontsize,loc='upper right',\n",
    "#           bbox_to_anchor=(1.25,1),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1)\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "state_compare = np.concatenate([SLDS_states[:1,:],pred_all[0],full_state_z[:1,:]],axis=0)\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z[0])))\n",
    "acc_slds = accuracy_score(full_state_z[0], SLDS_states[0])\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(scores_all[0,n]*100))) for n in range(len(scores_all[0]))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_slds*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+200,200))\n",
    "# ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "fig.savefig(fig_path/'{}_Fig3.pdf'.format(0),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1\n",
    "Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "np.mean(Input_Errors),np.std(Input_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(8,6))\n",
    "gs0 = gridspec.GridSpec(nrows=4,ncols=2, figure=fig, wspace=.15,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs0[:2,0]),fig.add_subplot(gs0[2:,0]),fig.add_subplot(gs0[:2,1]),fig.add_subplot(gs0[2:,1])])\n",
    "t=1500; dt=1000\n",
    "# seq_len=[100,200,500]\n",
    "\n",
    "Input_Errors_SLDS = (inputs_test_SLDS-SLDS_emission)\n",
    "ax = axs[0]\n",
    "Input_Errors_SLDS = np.abs(inputs_test_SLDS-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(inputs_test_SLDS-rSLDS_emission)\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "    err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "    ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs_b[ts],label='TiDHy')\n",
    "    ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs_b[ts],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.tick_params(axis='y', labelsize=fontsize-2)\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "tts+=1\n",
    "ax.annotate('SLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[2],fontsize=fontsize-2)\n",
    "tts += 1\n",
    "ax.annotate('rSLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[6],fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs[1]\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        # model = VAR(rhat2)\n",
    "        # results = model.fit(maxlags=20,ic='aic')\n",
    "        # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "        # Uhat_all.append(eig_Ut.reshape(-1))\n",
    "    # for p in range(ssm_params.Nlds):\n",
    "    #     for state in range(ssm_params.n_disc_states):\n",
    "            # rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    # evals_Uhat_all = np.hstack(Uhat_all)\n",
    "    # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "    # timescales = np.real(np.log(Uhat_all))\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "    \n",
    "    ax.scatter(x=timescales,y=np.abs(evals_Uhat_all)+ts,\n",
    "               c=clrs_b[ts],alpha=.75,edgecolor='None',s=25)\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "        ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    ax.set_xscale('symlog',linthresh=.1)\n",
    "    ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "    ax.set_ylabel('T',fontsize=fontsize,labelpad=-2)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=.015)\n",
    "y = .99\n",
    "dy = 0.075\n",
    "ax.annotate('System 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-2)\n",
    "ax.annotate('System 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-2)\n",
    "ax.annotate('System 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-2)\n",
    "\n",
    "ax=axs[2]\n",
    "spacing= 5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xlim([0,dt])\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(1.05,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "\n",
    "# legend = ax.legend(['T={}'.format(seq_len[p]) for p in range(len(seq_len))],frameon=False,fontsize=fontsize,loc='upper right',\n",
    "#           bbox_to_anchor=(1.25,1),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1)\n",
    "\n",
    "\n",
    "ax = axs[3]\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(acc_TiDHy_all[n]*100))) for n in range(len(acc_TiDHy_all))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_ssm*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+200,200))\n",
    "# ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "fig.savefig(cfg.paths.fig_dir/'{}_Fig3.pdf'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60973a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade off plot between accuracy of reconstruction and correct identification of unique timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(5,3))\n",
    "ax = axs\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "ax.scatter(x=timescales,y=np.abs(evals_Uhat_all),\n",
    "            c='k',s=40,alpha=.5,edgecolor='None')\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd0f76",
   "metadata": {},
   "source": [
    "# Load Anymal Terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29254605",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'AnymalTerrain'\n",
    "version = 'Debug'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 1#7 #22 #2 #8\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd().parent / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg.dataset.name}: {cfg.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6662783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_train = stack_data(data_dict['inputs_train'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "inputs_val = stack_data(data_dict['inputs_val'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)\n",
    "inputs_test = stack_data(data_dict['inputs_test'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create RNG\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Get model params as dict and unpack directly\n",
    "model_params = OmegaConf.to_container(cfg.model, resolve=True)\n",
    "# model_params.pop('batch_converge')\n",
    "model_params['input_dim'] = inputs_test.shape[-1]\n",
    "model_params['show_inf_progress'] = False\n",
    "# model_params['max_iter'] = 1000\n",
    "# model_params['lr_r'] = 7.5e-3\n",
    "# model_params['lr_r2'] = 7.5e-3\n",
    "\n",
    "\n",
    "model = TiDHy(**model_params, rngs=rngs)\n",
    "# model.l0= jnp.zeros(3)\n",
    "# model.loss_weights = jnp.array([1.0, 1.0, 1.0])\n",
    "print(f\"\\nModel initialized successfully!\")\n",
    "print(f\"input_dim: {model.input_dim}, r_dim: {model.r_dim}, r2_dim: {model.r2_dim}, mix_dim: {model.mix_dim}\")\n",
    "jit_model = jax.jit(model)\n",
    "# out = jit_model(inputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da250eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_first_two_dims(data_dict):\n",
    "    \"\"\"\n",
    "    Recursively reshape arrays in a dictionary to collapse the first 2 dimensions.\n",
    "    \n",
    "    Arrays with shape (a, b, c, d, ...) will be reshaped to (a*b, c, d, ...).\n",
    "    Arrays with fewer than 2 dimensions are returned unchanged.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary containing arrays (possibly nested)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with reshaped arrays\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for key, value in data_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Recursively handle nested dictionaries\n",
    "            result[key] = collapse_first_two_dims(value)\n",
    "        elif hasattr(value, 'shape') and len(value.shape) >= 2:\n",
    "            # Reshape arrays with at least 2 dimensions\n",
    "            new_shape = (value.shape[0] * value.shape[1],) + value.shape[2:]\n",
    "            result[key] = jnp.reshape(value, new_shape)\n",
    "        else:\n",
    "            # Keep other values unchanged\n",
    "            result[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1000\n",
    "loaded_model = load_model(model,cfg.paths.ckpt_dir/f'epoch_{epoch:04d}')\n",
    "result_dict = {}\n",
    "seq_len = [100,200,500,1000]\n",
    "for seq in tqdm(seq_len):\n",
    "    inputs_test = stack_data(data_dict['inputs_test'], sequence_length=seq, overlap=seq)\n",
    "    spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict_single = evaluate_record(loaded_model, inputs_test)\n",
    "    result_dict['{}'.format(seq)] = collapse_first_two_dims(result_dict_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_single(seq):\n",
    "    return model(seq, return_internals=True)\n",
    "\n",
    "# in_axes=0 means vmap over first dimension (batch)\n",
    "vmapped_forward = jax.vmap(forward_single, in_axes=0)\n",
    "# batch_losses = vmapped_forward(inputs_train)\n",
    "out, internals = forward_single(inputs_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_model, history = train_model(\n",
    "    model,\n",
    "    inputs_train,\n",
    "    n_epochs=250,\n",
    "    learning_rate_s=0.01,  # Spatial decoder learning rate\n",
    "    learning_rate_t=0.01,  # Temporal parameters learning rate  \n",
    "    learning_rate_h=0.001,  # Hypernetwork learning rate\n",
    "    schedule_transition_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=2**16,\n",
    "    use_gradnorm=False,\n",
    "    val_data=inputs_val,\n",
    "    val_every_n_epochs=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "# cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b\n",
    "\n",
    "def shuffle_along_axis(a, axis):\n",
    "    idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "    return np.take_along_axis(a,idx,axis=axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_figs = False\n",
    "# W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "# I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "# Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "# Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "# R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "# R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "# R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])\n",
    "nfig = 0\n",
    "t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50825e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "t=0; dt=1000\n",
    "spacing= .1\n",
    "ts=-1\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "I_shuff  = deepcopy(result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1]))\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,5))\n",
    "ax = axs[0]\n",
    "# cmap,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=1,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing,ls='-',color='#4e7eb3ff', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*I.shape[-1]))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1]+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "# # im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1],1)),cmap=cmap,norm=norm,alpha=.5)\n",
    "# cbar = fig.colorbar(im,ax=axs,aspect=30)\n",
    "# cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "# cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "# cbar.outline.set_linewidth(1)\n",
    "# cbar.minorticks_off()\n",
    "# cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "ax.set_yticks(hlines_I)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(0,1200,200),fontsize=fontsize)\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "ax.set_ylabel('Observation #',fontsize=fontsize)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.025,1.05),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "# ax.set_title('Learned Observations',fontsize=fontsize)\n",
    "lim0 = 0\n",
    "lim1 = 20\n",
    "hbins = 1\n",
    "Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# Input_Errors_SLDS = np.mean((inputs_test_SLDS-q_lem_y)**2,axis=0)\n",
    "I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "ax = axs[1]\n",
    "count,edges = np.histogram(Input_Errors,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='#757575ff',width=hbins, alpha=1,zorder=1,label='data')\n",
    "count,edges = np.histogram(Input_Errors_shuff,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='r',width=hbins, alpha=1,zorder=1,label='shuffle') \n",
    "# count,edges = np.histogram(Input_Errors_SLDS,bins=np.arange(lim0,lim1,hbins))\n",
    "# edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "# ax.bar(edges_mid, count/len(Input_Errors_SLDS),color='g',width=hbins, alpha=.5,zorder=1,label='SLDS') \n",
    "ax.set_ylabel('Proportion',fontsize=fontsize)\n",
    "ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize)\n",
    "# ax = axs[1]\n",
    "# fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "# Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "# Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "# xs = np.arange(I.shape[-1])\n",
    "# heights = Input_Errors\n",
    "# ax.barh(y=hlines_I,width=heights,color='k',)\n",
    "# ax.errorbar(heights,hlines_I,xerr=np.std((I-Ihat),axis=0)/np.sqrt((I-Ihat).shape[0]),ls='none',color='tab:gray',capsize=3)\n",
    "# ax.axvline(x=np.mean(Input_Errors_shuff),c='k',ls='--',lw=1,label='shuffle error')\n",
    "# # ax.set_xticks(np.arange(0,.004,.001))\n",
    "# ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "# ax.set_title('Average Error',fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(heights.shape[-1])+1)\n",
    "# ax.legend(bbox_to_anchor=(.3, .95), loc='lower left', borderaxespad=0.,frameon=False,fontsize=fontsize)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_observations_pred.png'.format(nfig),dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d423127",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mean_centered_x + n/spacing,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "t=0; dt = 1000\n",
    "# cmap2,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig, axs = plt.subplots(2,2,figsize=(20,12))\n",
    "axs = axs.flatten()\n",
    "fontsize=13\n",
    "ax = axs[0]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R2_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-.5,.75,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('$\\hat{r}^h$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax.legend(['T={}'.format(Ln) for Ln in seq_len],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.15,1.05),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "ax = axs[1]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['W'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['W'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(0,2.25,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('W',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax = axs[2]\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_bar'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "\n",
    "ax = axs[3]\n",
    "spacing= 1\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['I_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[p])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=.5)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('$\\hat{Z}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_TempWindExpansion.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1073bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind = [2,8,9]\n",
    "clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "terrain_names = ['Flat','Slope','Inv. Slope','Stairs','Inv. Stairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain_type_train = data_dict['terrain_train'].reshape(-1)\n",
    "terrain_type_test = data_dict['terrain_test']\n",
    "terrain_difficulty_train = data_dict['terrain_difficulty_train'].reshape(-1)\n",
    "terrain_difficulty_test = data_dict['terrain_difficulty_test'].reshape(-1)\n",
    "terrain_slope_train = data_dict['terrain_slope_train'].reshape(-1)\n",
    "terrain_slope_test = data_dict['terrain_slope_test'].reshape(-1)\n",
    "command_vel_train = data_dict['command_vel_train'].reshape(-1)\n",
    "command_vel_test = data_dict['command_vel_test'].reshape(-1)\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "terrain_robot_test = np.stack([terrain_type_test[robot_type_test==0].reshape(-1),terrain_type_test[robot_type_test==1].reshape(-1)])\n",
    "terrain_type_test = data_dict['terrain_test'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59828be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "# seq_len=[100,200,500,1000]\n",
    "seq_len=[9000]\n",
    "ax = axs\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.random.uniform(y_ranges[ts,0],y_ranges[ts,1],timescales.shape[0]),\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None')\n",
    "\n",
    "# for n in range(len(timescales_M)):\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize)\n",
    "ax.set_ylabel('T',fontsize=fontsize)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize)\n",
    "ax.set_xticks([0,10e0,10e1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ts=-1\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([result_dict['R2_hat'],result_dict['W'],result_dict['R_bar']],axis=-1),result_dict['R2_hat'],result_dict['W'],result_dict['R_hat'],result_dict['I']]\n",
    "reg_variables = [result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['I']]\n",
    "labels = ['R2_hat','I']\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "command_vel_test = np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    # X_train = reg_vars[:tr_batch_size].reshape(-1,reg_vars.shape[-1])\n",
    "    # X_test = reg_vars[-batch_size:].reshape(-1,reg_vars.shape[-1])\n",
    "    # y_train = terrain[:tr_batch_size].reshape(-1)\n",
    "    # y_test = terrain[-batch_size:].reshape(-1)\n",
    "    # y_train = robot_type_test[:tr_batch_size].reshape(-1)\n",
    "    # y_test = robot_type_test[-batch_size:].reshape(-1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), robot_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    # neigh = RidgeClassifierCV()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    # y_pred = neigh.predict(X_test)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred_terrain = y_pred\n",
    "    else:\n",
    "        I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        \n",
    "state_compare = np.stack([I_pred,best_pred_terrain,terrain_type_test])\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([R2_hat,W,R_bar],axis=-1),R2_hat,W,R_hat,I]\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(I.reshape(-1,I.shape[-1]))\n",
    "reg_variables = [R2_hat,I] #,X_pca]\n",
    "labels = ['R2_hat','I'] #,'X_pca']\n",
    "label_type = ['terrain_difficulty_test','terrain_slope_test','command_vel_test']#,'robot_type_test']\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0).reshape(-1)\n",
    "command_vel_test = np.tile(data_dict['command_vel_test'],(9000,1)).transpose(1,0)\n",
    "slope_cat = np.unique(terrain_slope_test,return_inverse=True)[1]\n",
    "difficulty_cat =  np.unique(terrain_difficulty_test,return_inverse=True)[1]\n",
    "vel_cat = np.unique(command_vel_test,return_inverse=True)[1]\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "class_results = {}\n",
    "state_compare_best = {}\n",
    "for m, test_labels in enumerate([difficulty_cat,slope_cat,vel_cat]): #,robot_type_test\n",
    "\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), test_labels.reshape(-1), test_size=0.25, random_state=42)\n",
    "\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        print(label_type[m],labels[k],scores)\n",
    "        if (labels[k] != 'I'):\n",
    "            y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = y_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "            max_acc = scores\n",
    "            best_reg_vars = reg_vars\n",
    "            best_label = labels[k]\n",
    "            best_pred = y_pred\n",
    "        else:\n",
    "            I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = I_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "    state_compare_best['{}'.format(label_type[m])] = np.stack([I_pred,best_pred,test_labels.reshape(-1)],axis=0)\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n",
    "from sklearn import metrics\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_pred.shape,best_pred.shape,test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bafde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_compare_data = {'state_compare_best':state_compare_best,'state_compare':state_compare,'class_results':class_results}\n",
    "# ioh5.save(cfg.paths.log_dir/'state_compare.h5',state_compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e59514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_compare_data = ioh5.load(cfg.paths.log_dir/'state_compare.h5')\n",
    "state_compare = state_compare_data['state_compare']\n",
    "state_compare_best = state_compare_data['state_compare_best']\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=2500; dt=1000\n",
    "fontsize=13\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=6,ncols=3, figure=fig, wspace=.1,hspace=.1)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5, subplot_spec=gs0[:3,:],wspace=.1,hspace=.1)\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5,subplot_spec=gs0[4:,:],wspace=.1,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs00[:,:2]),\n",
    "                fig.add_subplot(gs00[:,2:4]),\n",
    "                fig.add_subplot(gs00[:,4:]),\n",
    "                fig.add_subplot(gs01[:,:2]),\n",
    "                fig.add_subplot(gs01[:,2:])])\n",
    "ax = axs[1]\n",
    "spacing= .5\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=1)\n",
    "ts=1\n",
    "hlines_x = []\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I_hat'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c='r',alpha=.75)\n",
    "    hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "# ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "# ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "# ax.set_yticks(hlines_x[1::2])\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(2,len(hlines_x)+1,2),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "# ax.set_ylim(-1,(1/spacing)*len(hlines_x)+.5)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xticks([0,10e0,10e1,10e2])\n",
    "ax.set_xlim(0,1e2)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=-1)\n",
    "\n",
    "\n",
    "ax=axs[3]\n",
    "ax.bar(np.arange(len(f1_TiDHy)),f1_TiDHy,width=.4,color=clrs2[0])\n",
    "ax.bar(np.arange(len(f1_Inputs))+.4,f1_Inputs,width=.4,color=clrs2[4])\n",
    "ax.set_ylabel('F1 Score',fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(.2,len(f1_TiDHy)+.2))\n",
    "ax.set_xticklabels(['difficulty','slope','velocity'],fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(0,1.1,.25))\n",
    "ax.set_yticklabels(np.arange(0,1.1,.25),fontsize=fontsize-2)\n",
    "ax.legend(['TiDHy','Obs.'],fontsize=fontsize,frameon=False,loc='upper right',bbox_to_anchor=(1.025,1.25),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "ax = axs[-1]\n",
    "acc_obs = accuracy_score(terrain_type_test.reshape(-1),state_compare[0])\n",
    "acc_TiDHy = accuracy_score(terrain_type_test.reshape(-1),state_compare[1])\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(terrain_type_test)))\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+3000,3000))\n",
    "# ax.set_xticklabels(np.arange(0,dt+3000,3000),fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(.5,3,1))\n",
    "ax.set_yticklabels(['Obs. \\n {:02}%'.format(int(np.round(acc_obs*100))),'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=.05)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(terrain_type_test)),1))\n",
    "cbar.set_ticklabels([''.join(terrain_names[n]) for n in range(len(terrain_names))],fontsize=fontsize-2)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_Fig4.pdf'.format(nfig),dpi=300,transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "from sklearn.cluster import HDBSCAN, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0579cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.manifold.UMAP(n_components=5, n_neighbors=15, min_dist=0.75, metric='euclidean', init='spectral')\n",
    "reduced_data = umap.fit_transform(R2_hat[-1])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(R_hat[1])\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=50, min_samples=20, cluster_selection_epsilon=0.55, prediction_data=True)\n",
    "clusterer.fit(reduced_data)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97241e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterer.labels_.get()\n",
    "\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_label = jnp.argmax(soft_clusters[:,1:].get(), axis=1)\n",
    "r_data = reduced_data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(r_data[:, 0], r_data[:, 1], s=2,  c=terrain_type_test, alpha=0.05)\n",
    "plt.axis((-10,10,-10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = sns.color_palette('Paired', 12)\n",
    "cluster_colors = [color_palette[np.argmax(x)]\n",
    "                  for x in soft_clusters]\n",
    "plt.scatter(*projection.T, s=50, linewidth=0, c=cluster_colors, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA()\n",
    "comp = pca.fit_transform(R2_hat[1])\n",
    "subset = comp[labels!=-1,:]\n",
    "fig, axs = plt.subplots(2,2,figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "for n in range(len(axs)):\n",
    "    ax = axs[n]\n",
    "    ax.scatter(subset[:,n], subset[:,n+1], s=2, c=labels[labels!=-1], alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e6de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab8eed0f",
   "metadata": {},
   "source": [
    "# Load CalMS21 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2523fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CalMS21'\n",
    "# version = 'HierarchicalMultiTimescale'\n",
    "version = 'SLDS'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/biomech_model/Flybody/{dataset}/{version}')\n",
    "base_dir = Path(f'/data2/users/eabe/TiDHy/{dataset}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    temp = OmegaConf.load(run_cfg)\n",
    "    print(n, temp.dataset.name, temp.version, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "cfg_num = 2\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using workstation.yaml template\n",
    "cfg_ssm = load_config_and_override_paths(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    new_paths_template=\"workstation\",    # Use workstation.yaml for local paths\n",
    "    config_dir=Path.cwd().parent / \"configs\",\n",
    ")\n",
    "\n",
    "print(f'✅ Loaded experiment: {cfg_num}, {cfg_ssm.dataset.name}: {cfg_ssm.version} from {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "cfg_ssm.paths = convert_dict_to_path(cfg_ssm.paths)\n",
    "print(\"✅ Successfully converted all paths to Path objects and created directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2246768",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = ioh5.load(cfg.paths.log_dir/'evaluation_results.h5')\n",
    "seq_len = natsorted(list(result_dict.keys()))\n",
    "W = jnp.stack([result_dict[str(seq)]['W'].reshape(-1, result_dict[str(seq)]['W'].shape[-1]) for seq in seq_len])\n",
    "I = jnp.stack([result_dict[str(seq)]['I'].reshape(-1, result_dict[str(seq)]['I'].shape[-1]) for seq in seq_len])\n",
    "Ihat = jnp.stack([result_dict[str(seq)]['I_hat'].reshape(-1, result_dict[str(seq)]['I_hat'].shape[-1]) for seq in seq_len])\n",
    "Ibar = jnp.stack([result_dict[str(seq)]['I_bar'].reshape(-1, result_dict[str(seq)]['I_bar'].shape[-1]) for seq in seq_len])\n",
    "R_hat = jnp.stack([result_dict[str(seq)]['R_hat'].reshape(-1, result_dict[str(seq)]['R_hat'].shape[-1]) for seq in seq_len])\n",
    "R_bar = jnp.stack([result_dict[str(seq)]['R_bar'].reshape(-1, result_dict[str(seq)]['R_bar'].shape[-1]) for seq in seq_len])\n",
    "R2_hat = jnp.stack([result_dict[str(seq)]['R2_hat'].reshape(-1, result_dict[str(seq)]['R2_hat'].shape[-1]) for seq in seq_len])\n",
    "Ut = jnp.stack([result_dict[str(seq)]['Ut'].reshape((-1,)+ result_dict[str(seq)]['Ut'].shape[2:]) for seq in seq_len])\n",
    "W.shape, R2_hat.shape, R_hat.shape, Ut.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from TiDHy.utils.state_annotation_comparison import analyze_state_annotation_correspondence\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53',\"#32373B\",'#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "full_state_z = data_dict['annotations_test'][:R2_hat.shape[0]]\n",
    "cmap_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ab3cf",
   "metadata": {},
   "source": [
    "## Record Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slds_dict = ioh5.load(list(cfg_ssm.paths.log_dir.glob('ssm_slds_*.h5'))[0])\n",
    "rslds_dict = ioh5.load(list(cfg_ssm.paths.log_dir.glob('ssm_rslds_*.h5'))[0])\n",
    "slds_dict['SLDS_emission'].shape, slds_dict['SLDS_latents'].shape, slds_dict['SLDS_states'].shape\n",
    "behavior_names = list(data_dict['vocabulary'].keys())\n",
    "annotations = data_dict['annotations_test']\n",
    "\n",
    "results_slds = analyze_state_annotation_correspondence(\n",
    "    states=slds_dict['SLDS_states'],\n",
    "    annotations=annotations,\n",
    "    behavior_names=behavior_names,\n",
    "    verbose=False\n",
    ")\n",
    "results_rslds = analyze_state_annotation_correspondence(\n",
    "    states=rslds_dict['rSLDS_states'],\n",
    "    annotations=annotations,\n",
    "    behavior_names=behavior_names,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "slds_matched_states = results_slds['matching']['matched_states']\n",
    "rslds_matched_states = results_rslds['matching']['matched_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88393e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False\n",
    "ts=-1\n",
    "W = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "Ut = result_dict['{}'.format(seq_len[ts])]['Ut'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-2],result_dict['{}'.format(seq_len[ts])]['Ut'].shape[-1])\n",
    "full_state_z = data_dict['annotations_test'][:R2_hat.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25508806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "ts = -1\n",
    "reg_variables = [np.concatenate([R2_hat,W,R_hat],axis=-1),R2_hat,W,R_hat,R_bar]\n",
    "labels = ['all','R2_hat','W','R_hat','R_bar','I', 'SLDS_latents']\n",
    "# reg_variables = [slds_dict['SLDS_latents']]\n",
    "# labels = ['SLDS_latents']\n",
    "# reg_variables = [np.concatenate([H,C],axis=-1),H,C,I]\n",
    "# labels = ['all','H','C','I']\n",
    "\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    # neigh = RidgeClassifierCV()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "# plt.imshow(confusion_matrix(full_state_z, best_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_state_z = data_dict['annotations_test']\n",
    "# from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "import cv2\n",
    "cfg_ssm.train.normalize=False\n",
    "cfg_ssm.train.feature_type='raw'\n",
    "raw_data = load_data(cfg_ssm)\n",
    "\n",
    "cap = cv2.VideoCapture('/data/users/eabe/hypernets/CalMS21/datasets/mouse001_task1_annotator1.mp4')\n",
    "\n",
    "# Get video properties\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"Total frames: {total_frames}\")\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"Resolution: {width}x{height}\")\n",
    "\n",
    "# Now load a middle frame\n",
    "frame_number = 50\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d15570",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=13\n",
    "fps = 30\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.75))\n",
    "gs  = gridspec.GridSpec(nrows=8, ncols=4,hspace=10,wspace=.5) \n",
    "gs0 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[:3,:],  wspace=.5,hspace=.2)\n",
    "# gsb = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:3,2:3], wspace=.1,hspace=.5)\n",
    "\n",
    "# gsc = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:5,3:], wspace=.5,hspace=.1)\n",
    "gs1 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[3:6,:], wspace=.2,hspace=.1)\n",
    "\n",
    "gs2 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[6:,:-1],   wspace=0, hspace=.10)\n",
    "\n",
    "##### plotting video frame ######\n",
    "t = 50; dt = 1\n",
    "skeleton = np.array([[0,1],[0,2],[2,3],[1,3],[3,4],[3,5],[4,6],[5,6]])\n",
    "ani1_x = raw_data['inputs_train'][t:t+dt,:7]\n",
    "ani1_y = raw_data['inputs_train'][t:t+dt,7:14]\n",
    "ani2_x = raw_data['inputs_train'][t:t+dt,14:21]\n",
    "ani2_y = raw_data['inputs_train'][t:t+dt,21:28]\n",
    "I1_x = Ihat[t:t+dt,:7]\n",
    "I1_y = Ihat[t:t+dt,7:14]\n",
    "I2_x = Ihat[t:t+dt,14:21]\n",
    "I2_y = Ihat[t:t+dt,21:28]\n",
    "labels = raw_data['annotations_train'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "axs = np.array([fig.add_subplot(gs0[:2])])\n",
    "ax = axs[0]\n",
    "ax.imshow(frame) ##### Load frames from video in cell below\n",
    "for n in range(7):\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=25,c='#ffa600ff')\n",
    "    im = ax.scatter(ani2_x[:,n],ani2_y[:,n],s=25,c='#4daf50ff')\n",
    "tt=0\n",
    "for n in range(len(skeleton)):\n",
    "    ax.plot(ani1_x[:,skeleton[n]].squeeze(),ani1_y[:,skeleton[n]].squeeze(),'#ffa600ff',lw=4,zorder=1)\n",
    "    ax.plot(ani2_x[:,skeleton[n]].squeeze(),ani2_y[:,skeleton[n]].squeeze(),'#4daf50ff',lw=4,zorder=1)\n",
    "ax.axis('off')\n",
    "\n",
    "###### Plotting Observations #####\n",
    "axs = np.array([fig.add_subplot(gs0[2])])\n",
    "ax = axs[0]\n",
    "spacing = .75; fontsize=13\n",
    "t = 0; dt = 10000\n",
    "hlines,hlines_Ihat= [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    mean_centered = mean_centered\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered + n/spacing,color='k', lw=1)\n",
    "    hlines.append(np.mean(mean_centered + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered_Ihat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Timescales #####\n",
    "axs = np.array([fig.add_subplot(gs0[3])])\n",
    "ax = axs[0]\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "        \n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))/fps\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xscale('symlog',linthresh=100)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=0)\n",
    "ax.set_xticks([10e0,10e1,10e2,10e3,10e4,10e5])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=0)\n",
    "ax.set_xlim([-1,7e4])\n",
    "\n",
    "###### PCA #####\n",
    "n = 0; m = 1\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(R_hat.reshape(-1,R_hat.shape[-1]))\n",
    "lst = list(itertools.combinations(np.arange(X_pca.shape[-1]), 2))\n",
    "unq = set(lst)\n",
    "axs = np.array([fig.add_subplot(gs1[n]) for n in range(4)])\n",
    "for state in range(4):\n",
    "    ax = axs[state]\n",
    "    comps = X_pca[full_state_z==state]\n",
    "    comps = comps/np.max(np.abs(comps),axis=0)\n",
    "    # comps = X_pca[(full_state_z==0) | (full_state_z==1) | (full_state_z==2)]\n",
    "    h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=50,range=[[-1,1],[-1,1]],density=True)\n",
    "    im = ax.imshow((h).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_title('{}'.format(list(data_dict['vocabulary'].keys())[state]),fontsize=fontsize,y=.95)\n",
    "    ax.set_xlabel('PC{}'.format(n+1),fontsize=fontsize)\n",
    "    if (state==0):\n",
    "        ax.set_ylabel('PC{}'.format(m+1),fontsize=fontsize)\n",
    "ax = axs[-1]\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10,shrink=.5)\n",
    "cbar.set_ticks([0,np.max(h)])\n",
    "cbar.set_ticklabels(['low','high'],rotation=90)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "\n",
    "##### Behaviroal State ID #####\n",
    "t = 80000; dt = 10000; \n",
    "acc_TiDHy=accuracy_score(best_pred, full_state_z)\n",
    "acc_SLDS=accuracy_score(slds_matched_states, full_state_z)\n",
    "acc_rSLDS=accuracy_score(rslds_matched_states, full_state_z)\n",
    "# acc_TiDHy=neigh.score(I_pred, full_state_z)\n",
    "state_compare = np.stack([rslds_matched_states, slds_matched_states,best_pred,full_state_z],axis=0)\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "axs = np.array([fig.add_subplot(gs2[n]) for n in range(1)])\n",
    "ax = axs[0]\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_yticks(np.arange(.5,4,1))\n",
    "ax.set_yticklabels(['rSLDS {:02}%'.format(int(np.round(acc_rSLDS*100))), 'SLDS {:02}%'.format(int(np.round(acc_SLDS*100))),'TiDHy {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt,60*fps))\n",
    "ax.set_xticklabels((np.arange(0,dt,60*fps)*1/fps).astype(int),fontsize=fontsize-2)\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "\n",
    "# plt.tight_layout()\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10, pad=.01)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()),fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "# fig.savefig(cfg.paths.fig_dir / 'Fig5.pdf',dpi=300,transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3c8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ecf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d85230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.utils.analysis import compute_timescale_spectrum, cluster_timescales\n",
    "compute_timescale_spectrum_batch = vmap(compute_timescale_spectrum, in_axes=(0, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, T, r_dim, _ = result_dict[f'{seq_len[ts]}']['Ut'].shape\n",
    "V_t_all = result_dict[f'{seq_len[ts]}']['Ut']  # Shape: (batch, T, r_dim, r_dim)\n",
    "\n",
    "V_t_flat = V_t_all.reshape(batch_size * T, r_dim, r_dim)\n",
    "\n",
    "all_spectra = compute_timescale_spectrum_batch(V_t_flat, dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702674a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract finite stable timescales from all matrices\n",
    "finite_mask = (jnp.isfinite(all_spectra['timescales']) &\n",
    "                all_spectra['is_stable'] &\n",
    "                (~jnp.isnan(all_spectra['timescales'])))\n",
    "\n",
    "# Flatten the finite timescales (removes masked values)\n",
    "all_timescales = all_spectra['timescales'][finite_mask]\n",
    "unique_timescales = cluster_timescales(all_timescales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99bb657",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timescales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d11791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "log_timescales = jnp.log10(all_timescales[:,None] + 1e-10)\n",
    "\n",
    "clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500, min_samples=20, cluster_selection_epsilon=0.75, prediction_data=True)\n",
    "clusterer.fit(log_timescales)\n",
    "soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "labels = clusterer.labels_\n",
    "l_lab, l_counts = np.unique(labels,return_counts=True)\n",
    "print(l_lab)\n",
    "print(l_counts)\n",
    "print(l_counts[0], sum(l_counts[1:]))\n",
    "\n",
    "soft_label = jnp.argmax(soft_clusters[:,1:].get() if isinstance(soft_clusters, cupy.ndarray) else soft_clusters[:,1:], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e262598",
   "metadata": {},
   "source": [
    "### Load video frames for CalMS21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/data/users/eabe/hypernets/CalMS21/datasets/mouse001_task1_annotator1.mp4')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    " \n",
    "# Read until video is completed\n",
    "frames = []\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    frames.append(frame)   \n",
    "  # Break the loop\n",
    "  else: \n",
    "    break\n",
    " \n",
    "# When everything done, release the video capture object\n",
    "# cap.release()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/data/users/eabe/hypernets/CalMS21/datasets/mouse001_task1_annotator1.mp4')\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668274d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compare_tidhy_slds import compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980dd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slds_path = list(cfg_ssm.paths.log_dir.glob('ssm_slds_*.h5'))[0]\n",
    "tidhy_path = cfg.paths.log_dir/'evaluation_results.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = compare_models(tidhy_path, slds_path, dt=1/30, dataset_name='CalMS21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.utils import *\n",
    "from TiDHy.utils.slds_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slds_analysis = analyze_slds_comprehensive(slds_dict)\n",
    "slds_analysis.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slds_analysis['dynamics_timescales']['state_timescales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41097339",
   "metadata": {},
   "source": [
    "## Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41097339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import cv2\n",
    "import gc\n",
    "mpl.use('agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_plt_im(t, X_pca,data_dict,full_state_z):   #\n",
    "    trail = 5\n",
    "    labels = full_state_z[t-trail:t+1]\n",
    "    cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "    bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "    ax = axs[0]\n",
    "    im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "    im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1,1])\n",
    "    ax.set_ylim([-1,1])\n",
    "    cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "    cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "    cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "    ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "    ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "    ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "    ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "    ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "    labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "    for n in range(7):\n",
    "        ax = axs[1]\n",
    "        im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 1')\n",
    "        ax = axs[2]\n",
    "        ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    images = np.frombuffer(fig.canvas.tostring_rgb(),\n",
    "                        dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    plt.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = make_plt_im(t, X_pca,data_dict)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# full_state_z = data_dict['annotations_test']\n",
    "full_state_z = cluster_labels\n",
    "X_pca = pca.fit_transform(R_bar)\n",
    "\n",
    "t = 80000; dt = 5000\n",
    "trail = 5\n",
    "labels = full_state_z[t-trail:t+1]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig(cfg.paths.fig_dir/'test.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(cluster_labels==2)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "##### initialize time points for animation and progressbar #####\n",
    "t = 0 ; dt = 10000\n",
    "full_state_z = cluster_labels\n",
    "state = 'I'\n",
    "time_range = np.arange(t,t+dt)#  np.where(cluster_labels==state)[0]#\n",
    "num_ticks = np.size(time_range)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(I)\n",
    "\n",
    "##### Put large arrays into shared memory #####\n",
    "X_pca_r = ray.put(X_pca)\n",
    "data_dict_r = ray.put(data_dict)\n",
    "full_state_z_r = ray.put(full_state_z)\n",
    "##### Loop over parameters appending process ids #####\n",
    "result_ids = []\n",
    "for t in time_range:\n",
    "    result_ids.append(make_plt_im.remote(t, X_pca_r, data_dict_r,full_state_z_r))\n",
    "\n",
    "##### pring progressbar and get results #####\n",
    "results_p = ray.get(result_ids)\n",
    "images = np.stack([results_p[i] for i in range(len(results_p))])\n",
    "\n",
    "##### Make video with opencv #####\n",
    "aniname = 'PCA_Evolve_{}.mp4'.format(state) \n",
    "\n",
    "\n",
    "vid_name = cfg.paths.fig_dir / aniname\n",
    "FPS = 30\n",
    "out = cv2.VideoWriter(vid_name.as_posix(), cv2.VideoWriter_fourcc(*'mp4v'), FPS, (images.shape[-2], images.shape[-3]))\n",
    "\n",
    "for fm in tqdm(range(images.shape[0])):\n",
    "    out.write(cv2.cvtColor(images[fm], cv2.COLOR_BGR2RGB))\n",
    "out.release()\n",
    "print('Making Animation {}: {}'.format(aniname, time.time()-start))\n",
    "del results_p, X_pca_r, data_dict_r, full_state_z_r\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 80000 ; dt = 5000\n",
    "labels = full_state_z[t:t+dt]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[t:t+dt,0],X_pca[t:t+dt,1],s=1,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,4,1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t:t+dt,:7]\n",
    "ani1_y = data_dict['inputs_test'][t:t+dt,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t:t+dt,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t:t+dt,21:28]\n",
    "labels = data_dict['annotations_test'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.axis('square')\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e9076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2293f3c1",
   "metadata": {},
   "source": [
    "# Training Debuging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TiDHy.models.TiDHy_nnx_vmap_training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learning rates - check if multi-LR is configured\n",
    "learning_rate_s = cfg.train.learning_rate_s\n",
    "learning_rate_t = cfg.train.learning_rate_t\n",
    "learning_rate_h = cfg.train.learning_rate_h\n",
    "weight_decay = cfg.train.weight_decay\n",
    "\n",
    "optimizer = create_multi_lr_optimizer(\n",
    "    model,\n",
    "    learning_rate_s,\n",
    "    learning_rate_t,\n",
    "    learning_rate_h,\n",
    "    weight_decay,\n",
    "    use_schedule=True,\n",
    "    schedule_transition_steps=cfg.train.schedule_transition_steps,\n",
    "    schedule_decay_rate=cfg.train.schedule_decay,\n",
    ")\n",
    "inputs_test = stack_data(data_dict['inputs_test'], sequence_length=cfg.train.sequence_length, overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = loaded_model.rngs()\n",
    "spatial_loss_rhat_avg, spatial_loss_rbar_avg, temp_loss_avg, result_dict_single = evaluate_record(loaded_model, inputs_test, rng_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_single.keys()\n",
    "result_dict_single['inf_stats']['iterations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6dfa1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidhy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
