{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0e1841-1962-46c0-a629-11e884c6c99c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eaebf-52c1-4b95-a6d9-34a1b1525b1b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os.path as op\n",
    "import ssm\n",
    "import itertools\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as pltx\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import TiDHy\n",
    "from TiDHy.utils.utils import *\n",
    "from TiDHy.datasets import load_dataset\n",
    "import TiDHy.utils.io_dict_to_hdf5 as ioh5\n",
    "\n",
    "##### Plotting settings ######\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size':          10,\n",
    "                     'axes.linewidth':     2,\n",
    "                     'xtick.major.size':   5,\n",
    "                     'ytick.major.size':   5,\n",
    "                     'xtick.major.width':  2,\n",
    "                     'ytick.major.width':  2,\n",
    "                     'axes.spines.right':  False,\n",
    "                     'axes.spines.top':    False,\n",
    "                     'pdf.fonttype':       42,\n",
    "                     'xtick.labelsize':    10,\n",
    "                     'ytick.labelsize':    10,\n",
    "                     'figure.facecolor':   'white',\n",
    "                     'pdf.use14corefonts': True,\n",
    "                     'font.family':        'Arial',\n",
    "                    })\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "clrs = ['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000']\n",
    "cmap = ListedColormap(clrs)\n",
    "\n",
    "device = torch.device(1) #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df481e9",
   "metadata": {},
   "source": [
    "# Load SLDS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7da2f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'SLDS'\n",
    "base_dir = Path('/data/users/eabe/hypernets/{}/TiDHy_{}/'.format(dataset,dataset))\n",
    "configs = sorted(list(base_dir.rglob('*config.yaml'))[::2])\n",
    "\n",
    "for n,conf in enumerate(configs):\n",
    "    print(n,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe884c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "n = 1\n",
    "cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "cfg = load_cfg(cfg_path, default_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dataset.new_label = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dataset.new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b7c65",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "data_dict, cfg = load_dataset(cfg)\n",
    "\n",
    "# ##### Convert to float tensors #####\n",
    "train_inputs = torch.tensor(data_dict['inputs_train']).float()\n",
    "test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "val_inputs = torch.tensor(data_dict['inputs_val']).float()\n",
    "\n",
    "input_dim = test_inputs.shape[-1]\n",
    "# test_inputs = test_inputs.unsqueeze(0)\n",
    "# test_inputs = stack_data(test_inputs,cfg.train.sequence_length,overlap=cfg.train.sequence_length//cfg.train.overlap_factor)\n",
    "test_inputs = test_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "val_inputs = val_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "# train_inputs = train_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "# cfg.model.input_dim = input_dim\n",
    "\n",
    "\n",
    "\n",
    "print(f'Our inputs have shape: {test_inputs.shape}')\n",
    "cfg.model.input_dim = input_dim\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_inputs)\n",
    "# dataloader_train = torch.utils.data.DataLoader(train_dataset,batch_size=train_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=test_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "# dataloader_val = torch.utils.data.DataLoader(val_dataset,batch_size=val_inputs.shape[0],shuffle=False,pin_memory=True,drop_last=True)\n",
    "# device = torch.device(\"cuda:{}\".formt(cfg.train['gpu']) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46977af0",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "cfg.model.batch_converge=False\n",
    "# cfg.model.Orth_alpha_spat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce1a91",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "##### Load weights and params #####\n",
    "rerecord = False\n",
    "epoch = 1000\n",
    "model = TiDHy.TiDHy(cfg.model, device, show_progress=cfg.train.show_progress).to(device)\n",
    "# set_seed(42)\n",
    "load_path = cfg.paths.ckpt_dir/'best.pth.tar'\n",
    "load_path = sorted(list(cfg.paths.ckpt_dir.rglob('last_{}.pth.tar'.format(epoch))))[0]\n",
    "data_load = torch.load(load_path,map_location=device)\n",
    "model.load_state_dict(data_load['state_dict'])\n",
    "model.to(device)\n",
    "print('Loaded from {} epoch'.format(data_load['epoch']))\n",
    "epoch = data_load['epoch']\n",
    "\n",
    "\n",
    "# result_dict = load_results(model, dataloader_test, data_dict, device, cfg, epoch, rerecord=rerecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5434c",
   "metadata": {},
   "source": [
    "# Save Multi Seq Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c005a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seq_len_model(seq_len, model, data_dict, device, epoch, cfg):\n",
    "    cfg.train.sequence_length = int(np.max(seq_len))\n",
    "    data_dict, cfg = load_dataset(cfg)\n",
    "    result_dict_all = {}\n",
    "    for new_seq_len in seq_len:\n",
    "        set_seed(42)\n",
    "        test_inputs = torch.tensor(data_dict['inputs_test'].reshape(-1,new_seq_len,cfg.model.input_dim)).float()\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "        dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=test_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "        result_dict={}\n",
    "        for Nbatch, batch in enumerate(dataloader_test):\n",
    "            X = batch[0].to(device,non_blocking=True)\n",
    "            spatial_loss,temp_loss,result_dict_temp = model.evaluate_record(X)\n",
    "            if Nbatch==0:\n",
    "                result_dict = result_dict_temp\n",
    "            else:\n",
    "                for key in result_dict.keys():\n",
    "                    if isinstance(result_dict[key],torch.Tensor):\n",
    "                        result_dict[key] = torch.cat((result_dict[key],result_dict_temp[key]),dim=0)\n",
    "\n",
    "\n",
    "        for key in result_dict.keys():\n",
    "            if isinstance(result_dict[key],torch.Tensor):\n",
    "                result_dict[key] = result_dict[key].cpu().detach().numpy()\n",
    "        if 'states_x_test' in data_dict.keys():\n",
    "            result_dict['states_x_test'] = data_dict['states_x_test']\n",
    "            result_dict['states_x_train'] = data_dict['states_x']\n",
    "        if 'As' in data_dict.keys():\n",
    "            result_dict['As'] = data_dict['As']\n",
    "            result_dict['bs'] = data_dict['bs']\n",
    "        if 'states_z_test' in data_dict.keys():\n",
    "            result_dict['states_z_test'] = data_dict['states_z_test']\n",
    "            result_dict['states_z_train'] = data_dict['states_z']\n",
    "\n",
    "        for key in result_dict.keys():\n",
    "            if isinstance(result_dict[key],dict):\n",
    "                result_dict[key] = [result_dict[key][key2] for key2 in result_dict[key].keys()]\n",
    "                \n",
    "                \n",
    "        ##### Save Results #####\n",
    "        ioh5.save(cfg.paths.log_dir/'temp_results_{}_T{:04d}.h5'.format(epoch,new_seq_len),result_dict)\n",
    "        \n",
    "        reshape_list = ['W','I','I_hat','I_bar','R_hat','R_bar','R2_hat']\n",
    "        for key in reshape_list:\n",
    "            if key in result_dict.keys():\n",
    "                result_dict[key] = result_dict[key].reshape(-1,result_dict[key].shape[-1])\n",
    "        result_dict_all['{}'.format(new_seq_len)] = result_dict\n",
    "    ioh5.save(cfg.paths.log_dir/'temp_results_{}_seq.h5'.format(epoch),result_dict_all)\n",
    "    print('Done')\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cae1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "cfg = run_seq_len_model(seq_len, model, data_dict, device, epoch, cfg)\n",
    "\n",
    "data_dict, cfg = load_dataset(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2664017",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2024b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "result_dict = ioh5.load(cfg.paths.log_dir/'temp_results_{}_seq.h5'.format(epoch))\n",
    "seq_len = sorted([int(el) for el in list(result_dict.keys())])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa3122",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "states_x_test = data_dict['states_x_test']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "# states_z_test = data_dict['states_z']\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "##### Set up combinatorics of timescales #####\n",
    "lst = list(itertools.product([0, 1], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros(ssm_params['time_bins_test'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,states_z_test)] = n\n",
    "\n",
    "\n",
    "save_figs = False\n",
    "p=-1\n",
    "W = result_dict['{}'.format(seq_len[p])]['W'].reshape(-1,result_dict['{}'.format(seq_len[p])]['W'].shape[-1])\n",
    "# b = result_dict['b'].reshape(-1,result_dict['b'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[p])]['I'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[p])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1])\n",
    "Ibar = result_dict['{}'.format(seq_len[p])]['I_bar'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[p])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[p])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[p])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1])\n",
    "# Ut = result_dict['Ut'].reshape(-1,result_dict['Ut'].shape[-2],result_dict['Ut'].shape[-1])\n",
    "# ##### Plot dynamic matrices #####\n",
    "if cfg.model.low_rank_temp:\n",
    "    Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "else:\n",
    "    Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "    U_t = torch.matmul(torch.Tensor(W[:,None,:]), model.temporal.unsqueeze(0).cpu().detach()).reshape(-1, model.r_dim, model.r_dim).numpy()\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "bs = np.stack([v for v in data_dict['bs'].values()])\n",
    "nfig = 0\n",
    "fontsize=13\n",
    "# t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f503d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ssm.util import random_rotation, find_permutation\n",
    "inputs_train_SLDS=data_dict['inputs_train']\n",
    "inputs_test_SLDS=data_dict['inputs_test']\n",
    "seed = cfg.dataset.ssm_params['seed']\n",
    "SLDS_path = cfg.paths.log_dir#/'SSM/DPC_SSM/Benchmark/dataset.ssm_params.seed={}/'.format(seed)\n",
    "N = inputs_train_SLDS.shape[-1]\n",
    "K = len(np.unique(full_state_z))\n",
    "D = data_dict['states_x_test'].shape[-1]\n",
    "with open(SLDS_path/'ssm_slds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "# with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "    slds = pickle.load(handle)\n",
    "    posterior = pickle.load(handle)\n",
    "    SLDS_latents = pickle.load(handle)\n",
    "    SLDS_states = pickle.load(handle)\n",
    "    SLDS_emission = pickle.load(handle)\n",
    "\n",
    "with open(SLDS_path/'ssm_rslds_test_full_{}D_{}K_{}seed.pickle'.format(D,K,seed), 'rb') as handle:\n",
    "# with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "    slds = pickle.load(handle)\n",
    "    posterior = pickle.load(handle)\n",
    "    rSLDS_latents = pickle.load(handle)\n",
    "    rSLDS_states = pickle.load(handle)\n",
    "    rSLDS_emission = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537481f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=-1\n",
    "reg_variables = [\n",
    "    result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['W'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_bar'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['I'],\n",
    "    SLDS_latents,]\n",
    "\n",
    "labels = ['R2_hat','W','R_bar','R_hat','I','SLDS']\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "\n",
    "state_compare = np.stack([rSLDS_states, SLDS_states,best_pred,full_state_z],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5a790",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[8,8,1,1,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_comps=ssm_params['latent_dim']\n",
    "t=1500; dt=200\n",
    "    \n",
    "fig = plt.figure(constrained_layout=True, figsize=(7,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=5,ncols=4, figure=fig, wspace=.45,hspace=.2)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=6,ncols=1,subplot_spec=gs0[:,:2],wspace=.1,hspace=.2)\n",
    "# gs01 = gridspec.GridSpecFromSubplotSpec(3,1, subplot_spec=gs0[0:,:1],wspace=.05,hspace=.8)\n",
    "axs = np.array([fig.add_subplot(gs00[:-3,0]),\n",
    "                fig.add_subplot(gs00[-3:-2,0]),\n",
    "                fig.add_subplot(gs00[-2:,0]),\n",
    "                fig.add_subplot(gs0[:3,2:]),\n",
    "                fig.add_subplot(gs0[3:,2:])])\n",
    "axs[4].sharex(axs[3])\n",
    "ts = -1\n",
    "##### Pannel a #####\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "ax = axs[0]\n",
    "spacing = 1\n",
    "xrange = 100\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z)))\n",
    "linestyle =  (0, (5, 1))#(0, (5, 5))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=2,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing, linestyle=linestyle, color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "# ax.set_yticks(hlines_I)\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "##### Pannel c #####\n",
    "ax = axs[3]\n",
    "count=0\n",
    "acc_slds = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_rslds = accuracy_score(full_state_z, rSLDS_states)\n",
    "acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "spacing = .5\n",
    "\n",
    "hlines_x,hlines_Rhat = [],[]\n",
    "for p in range(ssm_params['Nlds']):\n",
    "    # states_x_cca = states_x_train[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    states_x_cca = states_x_test[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    # states_x_cca = states_x_test[:,p:p+1]\n",
    "    cca = CCA(n_components=ssm_params['latent_dim'],max_iter=1000)\n",
    "    X_c,Y_c = cca.fit_transform(states_x_cca,R_hat)\n",
    "    cca_coefficient = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=n_comps)\n",
    "    x_w = cca.x_weights_\n",
    "    y_w = cca.y_weights_\n",
    "    cca_angles = [np.rad2deg(angle_between(X_c[:,n],Y_c[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_x = [np.rad2deg(angle_between(X_c[:,n],states_x_cca[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_r = [np.rad2deg(angle_between(Y_c[:,n],R_hat[:,n])) for n in range(n_comps)]\n",
    "    for n in range(n_comps):\n",
    "        print('comp {}, cc: {:.03}, ang: {:.03}, ang_x:{:.03}, ang_r:{:.03}'.format(n,cca_coefficient[n],cca_angles[n],cca_angles_x[n],cca_angles_r[n]))\n",
    "\n",
    "    for i in range(X_c.shape[-1]):\n",
    "        mean_centered_x = X_c[t:t+dt,i] - np.mean(X_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_x=mean_centered_x/(np.max(np.abs(mean_centered_x)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_x + count/spacing,color='k', lw=2,zorder=1)\n",
    "        hlines_x.append(np.mean(mean_centered_x + count/spacing,axis=0))\n",
    "        mean_centered_Rhat = Y_c[t:t+dt,i] - np.mean(Y_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_Rhat=mean_centered_Rhat/(np.max(np.abs(mean_centered_Rhat)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_Rhat + count/spacing,linestyle=linestyle,color='r', lw=1.5,zorder=2,label='$\\hat{{r}}_{{{}}}$={:.02}'.format(n,cca_coefficient[n]),alpha=1)\n",
    "        hlines_Rhat.append(np.mean(mean_centered_Rhat + count/spacing,axis=0))\n",
    "        ax.text(x=dt+15,y=hlines_x[count],s='cc = {:.02}'.format(cca_coefficient[i]),fontsize=fontsize-2)\n",
    "        count += 1\n",
    "    ax.set_yticks(hlines_x) \n",
    "    ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "    # ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "    ax.set_ylabel('latent variables',fontsize=fontsize)\n",
    "# ax.set_xticks([])\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps', fontsize=fontsize)\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*6))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*6+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "##### Pannel b #####\n",
    "ax = axs[1]\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "ax.scatter(x=timescales,y=np.abs(evals_Uhat_all),\n",
    "            c='k',s=40,alpha=.5,edgecolor='None')\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n",
    "\n",
    "######### Accuracy and reconstruction error #########\n",
    "ax = axs[2]\n",
    "Input_Errors_SLDS = np.abs(inputs_test_SLDS-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(inputs_test_SLDS-rSLDS_emission)\n",
    "Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs[1],label='TiDHy')\n",
    "ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs[1],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.legend(frameon=False,fontsize=fontsize-3,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "##### Pannel d #####\n",
    "ax = axs[4]\n",
    "ylabels_states = ['rSLDS \\n {:02}%'.format(int(np.round(acc_rslds*100))),\n",
    "                  'SLDS \\n {:02}%'.format(int(np.round(acc_slds*100))),\n",
    "                  'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),\n",
    "                  'True']\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_xticks(np.arange(0,dt+xrange,xrange))\n",
    "ax.set_xticklabels(np.arange(0,dt+xrange,xrange),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,4,1))\n",
    "ax.set_yticklabels(ylabels_states,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=-.2)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Benchmark_Compare3.pdf',dpi=300)\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_Fig2_V2.pdf'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "np.log((np.linalg.eigvals(As))),np.linalg.eigvals(As),np.log([.97,.78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=1\n",
    "best_pred_all = []\n",
    "acc_ssm = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_TiDHy_all = []\n",
    "for ts in range(len(seq_len)):\n",
    "    reg_variables = [\n",
    "        # np.concatenate([result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['W']],axis=-1)\n",
    "        result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "        # q_lem_x,\n",
    "        ]\n",
    "\n",
    "    labels = ['R2_hat']\n",
    "    # reg_variables = W\n",
    "    max_acc = 0\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        print(labels[k],scores)\n",
    "        acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "        acc_TiDHy_all.append(acc_TiDHy)\n",
    "        best_pred_all.append(y_pred)\n",
    "\n",
    "state_compare = np.concatenate([SLDS_states[None,:],np.stack(best_pred_all),full_state_z[None,:]],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1\n",
    "Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "np.mean(Input_Errors),np.std(Input_Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(8,6))\n",
    "gs0 = gridspec.GridSpec(nrows=4,ncols=2, figure=fig, wspace=.15,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs0[:2,0]),fig.add_subplot(gs0[2:,0]),fig.add_subplot(gs0[:2,1]),fig.add_subplot(gs0[2:,1])])\n",
    "t=1500; dt=1000\n",
    "# seq_len=[100,200,500]\n",
    "\n",
    "Input_Errors_SLDS = (inputs_test_SLDS-SLDS_emission)\n",
    "ax = axs[0]\n",
    "Input_Errors_SLDS = np.abs(inputs_test_SLDS-SLDS_emission)\n",
    "Input_Errors_rSLDS = np.abs(inputs_test_SLDS-rSLDS_emission)\n",
    "err_slds  = 100*(1-accuracy_score(full_state_z, SLDS_states))\n",
    "err_rslds = 100*(1-accuracy_score(full_state_z, rSLDS_states))\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors = np.abs(I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "    err_TiDHy = 100*(1-accuracy_score(full_state_z, y_pred))\n",
    "    ax.scatter(x=np.mean(Input_Errors),y=err_TiDHy, s=25, c=clrs_b[ts],label='TiDHy')\n",
    "    ax.errorbar(x=np.mean(Input_Errors),y=err_TiDHy, ecolor=clrs_b[ts],xerr=np.std(Input_Errors))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "\n",
    "ax.scatter(x=np.mean(Input_Errors_SLDS),y=err_slds, s=25, c=clrs[2],label='SLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_SLDS),y=err_slds, ecolor=clrs[2],xerr=np.std(Input_Errors_SLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.scatter(x=np.mean(Input_Errors_rSLDS),y=err_rslds, s=25,c=clrs[6],label='rSLDS')\n",
    "ax.errorbar(x=np.mean(Input_Errors_rSLDS),y=err_rslds, ecolor=clrs[6],xerr=np.std(Input_Errors_rSLDS))#/np.sqrt(len(Input_Errors_SLDS)))\n",
    "ax.set_yticks([0,25,50,75,100])\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('dyn. % error',fontsize=fontsize)\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0,0),useLocale=True)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.tick_params(axis='y', labelsize=fontsize-2)\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "tts+=1\n",
    "ax.annotate('SLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[2],fontsize=fontsize-2)\n",
    "tts += 1\n",
    "ax.annotate('rSLDS', xy=(.8,y-tts*dy),xycoords='axes fraction',color=clrs[6],fontsize=fontsize-2)\n",
    "\n",
    "\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs[1]\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        # model = VAR(rhat2)\n",
    "        # results = model.fit(maxlags=20,ic='aic')\n",
    "        # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "        # Uhat_all.append(eig_Ut.reshape(-1))\n",
    "    # for p in range(ssm_params.Nlds):\n",
    "    #     for state in range(ssm_params.n_disc_states):\n",
    "            # rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    # evals_Uhat_all = np.hstack(Uhat_all)\n",
    "    # timescales = 1/np.abs(np.log(np.real(evals_Uhat_all)))\n",
    "    # timescales = np.real(np.log(Uhat_all))\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "    \n",
    "    ax.scatter(x=timescales,y=np.abs(evals_Uhat_all)+ts,\n",
    "               c=clrs_b[ts],alpha=.75,edgecolor='None',s=25)\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "        ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    ax.set_xscale('symlog',linthresh=.1)\n",
    "    ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "    ax.set_ylabel('T',fontsize=fontsize,labelpad=-2)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=.015)\n",
    "y = .99\n",
    "dy = 0.075\n",
    "ax.annotate('System 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-2)\n",
    "ax.annotate('System 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-2)\n",
    "ax.annotate('System 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-2)\n",
    "\n",
    "ax=axs[2]\n",
    "spacing= 5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize,labelpad=-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xlim([0,dt])\n",
    "y = .95\n",
    "dy = 0.075\n",
    "for tts in range(len(seq_len)):\n",
    "    ax.annotate('T={}'.format(seq_len[tts]), xy=(1.05,y-tts*dy),xycoords='axes fraction',color=clrs_b[tts],fontsize=fontsize-2)\n",
    "\n",
    "# legend = ax.legend(['T={}'.format(seq_len[p]) for p in range(len(seq_len))],frameon=False,fontsize=fontsize,loc='upper right',\n",
    "#           bbox_to_anchor=(1.25,1),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1)\n",
    "\n",
    "\n",
    "ax = axs[3]\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(acc_TiDHy_all[n]*100))) for n in range(len(acc_TiDHy_all))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_ssm*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+200,200))\n",
    "# ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+(dt//4),(dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,dt+(dt//4),(dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_Fig3.pdf'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60973a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade off plot between accuracy of reconstruction and correct identification of unique timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(5,3))\n",
    "ax = axs\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges=np.repeat([[0,1]],3,axis=0)\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As)))[:,:,0].reshape(-1))\n",
    "evals_As = np.linalg.eigvals(As)[:,:,0].reshape(-1)\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "Uhat_all = []\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    Uhat_all.append(Uhat_0)\n",
    "    \n",
    "# for p in range(ssm_params.Nlds):\n",
    "#     for state in range(ssm_params.n_disc_states):\n",
    "#         rhat2 = R_hat[np.where(states_z_test[p]==state)[0],:]\n",
    "#         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "#         Uhat_all.append(Uhat_0)\n",
    "        \n",
    "Uhat_all = np.stack(Uhat_all)\n",
    "evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "timescales = 1/np.abs(np.real(np.log(evals_Uhat_all)))\n",
    "ax.scatter(x=timescales,y=np.abs(evals_Uhat_all),\n",
    "            c='k',s=40,alpha=.5,edgecolor='None')\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    ax.scatter(x=timescales_As[n],y=np.abs(evals_As[n]),\n",
    "            c=sys_clrs[clr_ind[n]],s=40,alpha=1,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.1)\n",
    "ax.set_yticks([0,.5,1])\n",
    "ax.set_ylim(0,1.1)\n",
    "# ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "# ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_ylabel('|$\\lambda$|',fontsize=fontsize)\n",
    "y = 1\n",
    "dy = 0.15\n",
    "ax.annotate('system 1', xy=(.01,y),xycoords='axes fraction',color=sys_clrs[clr_ind[0]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 2', xy=(.01,y-dy),xycoords='axes fraction',color=sys_clrs[clr_ind[2]],fontsize=fontsize-3.5)\n",
    "ax.annotate('system 3', xy=(.01,y-2*dy),xycoords='axes fraction',color=sys_clrs[clr_ind[4]],fontsize=fontsize-3.5)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f50830",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "seq_len=[100,200,500,1000]\n",
    "states_z_test = data_dict['states_z_test']\n",
    "ax = axs\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = 1/np.abs(np.real(np.log(np.linalg.eigvals(As))))[:,:,0].reshape(-1)\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    # R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    # Uhat_all = []\n",
    "    # for p in range(ssm_params.Nlds):\n",
    "    #     for state in range(ssm_params.n_disc_states):\n",
    "    #         # rhat2 = R_hat[np.where(states_z_test[p]==k)[0],:]\n",
    "    #         rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    #         Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    #         Uhat_all.append(Uhat_0)\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        Ut_state = U_t[np.where(full_state_z==state)[0],:,:]\n",
    "        Ut_state = Ut_state[np.where(np.sum(Ut_state,axis=(-1,-2))!=0)[0],:,:]\n",
    "        eig_Ut = np.linalg.eigvals(Ut_state).reshape(-1)\n",
    "        timescales = 1/np.abs(np.real(np.log(eig_Ut)))\n",
    "    # Uhat_all = np.stack(Uhat_all)\n",
    "    # evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    # timescales = np.real(np.log(evals_Uhat_all))\n",
    "        ax.scatter(x=timescales,y=np.abs(eig_Ut)+ts,\n",
    "               c=clrs_b[ts],alpha=.75,edgecolor='None',s=25)\n",
    "        # ax.scatter(x=timescales,y=np.random.uniform(y_ranges[ts,0],y_ranges[ts,1],timescales.shape[0]),\n",
    "        #         c=clrs_b[ts],alpha=.5,edgecolor='None')\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "    # ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(seq_len))])\n",
    "    # ax.set_yticklabels(seq_len,fontsize=fontsize)\n",
    "    ax.set_ylabel('T',fontsize=fontsize)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('log(Re($\\lambda$))',fontsize=fontsize)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xscale('symlog',linthresh=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(8,4))\n",
    "ax = axs\n",
    "TiDHy_acc = ['T={} \\n {:02}%'.format(seq_len[n],int(np.round(acc_TiDHy_all[n]*100))) for n in range(len(acc_TiDHy_all))]\n",
    "Ylabels = ['SLDS \\n {:02}%'.format(int(np.round(acc_ssm*100)))] + TiDHy_acc + ['True']  \n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(.5,len(Ylabels),1))\n",
    "ax.set_yticklabels(Ylabels,fontsize=fontsize)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=ax,aspect=10, pad=.01)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Errors_SLDS = (inputs_test_SLDS-SLDS_emission)\n",
    "\n",
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "ax = axs\n",
    "for ts in range(len(seq_len[1:2])):\n",
    "    Input_Errors = (I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "    plot_hist(Input_Errors.flatten(),0,.1,1e-3,ax,'TiDHy:T={}'.format(seq_len[ts+1]),clr=clrs_b[ts])\n",
    "# plot_hist(Input_Errors_SLDS.flatten(),0,1,1e-1,ax,'SLDS',clr='k')\n",
    "ax.legend(frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=1)\n",
    "ax.set_xlabel('reconstruction error',fontsize=fontsize)\n",
    "ax.set_ylabel('proportion',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "fig.savefig(cfg.paths.fig_dir/'{}hist.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs_test_SLDS[:100,1])\n",
    "plt.plot(SLDS_emission[:100,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a02fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Errors_SLDS = (inputs_test_SLDS-q_lem_y)\n",
    "\n",
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "ax = axs\n",
    "total_error = []\n",
    "for ts in range(len(seq_len)):\n",
    "    Input_Errors = (I-result_dict['{}'.format(seq_len[ts])]['I_hat'])\n",
    "    total_error.append(Input_Errors)\n",
    "total_error.append(Input_Errors_SLDS)\n",
    "total_error = np.stack(total_error)\n",
    "\n",
    "ax.boxplot(np.mean(np.abs(total_error),axis=(1)).T)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "t=0; dt = 1000\n",
    "cmap2,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig, axs = plt.subplots(2,2,figsize=(20,12))\n",
    "axs = axs.flatten()\n",
    "fontsize=20\n",
    "ax = axs[0]\n",
    "spacing= 20\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R2_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-.5,.75,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('$\\hat{r}^h$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax.legend(['T={}'.format(Ln) for Ln in seq_len],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.15,1.05),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "ax = axs[1]\n",
    "spacing= 10\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['W'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['W'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(0,2.25,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('W',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax = axs[2]\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_bar'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "\n",
    "ax = axs[3]\n",
    "spacing= 1\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['I_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[p])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=.5)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('$\\hat{Z}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_TempWindExpansion.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22623ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "U_t = (torch.Tensor(W[:,:,None])*model.temporal.unsqueeze(0).cpu().detach()).numpy()\n",
    "U_t = U_t[:,U_t.sum((0,-1))!=0]\n",
    "U_t = U_t.reshape(U_t.shape[0],U_t.shape[1],model.r_dim,model.r_dim)\n",
    "Ut = U_t[np.where(states_z_test[p]==k)[0]]\n",
    "eig_Ut = np.linalg.eigvals(Ut)\n",
    "eig_Ut = eig_Ut[np.where(np.real(eig_Ut)!=0)]\n",
    "# U_t = torch.bmm(torch.Tensor(W[:,None,:]), model.temporal.unsqueeze(0).cpu().detach()).reshape(-1, model.r_dim, model.r_dim).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0\n",
    "seq_len=[200,'SLDS']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "fig, axs = plt.subplots(1,1,figsize=(4,4))\n",
    "ax = axs\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = np.log(np.abs(np.real(np.linalg.eigvals(As))))[:,:,0].reshape(-1)\n",
    "timescales_SLDS = np.unique(np.log(np.abs(np.real(np.linalg.eigvals(slds.dynamics.As)))))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for timescale in range(len(seq_len)-1):\n",
    "    R_bar = result_dict['{}'.format(seq_len[timescale])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[timescale])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[timescale])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[timescale])]['R_hat'].shape[-1])\n",
    "    \n",
    "\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []; SLDS_all=[]\n",
    "    for p in range(ssm_params.Nlds):\n",
    "        for k in range(ssm_params.n_disc_states):\n",
    "            rhat2 = R_hat[np.where(states_z_test[p]==k)[0],:]\n",
    "            SLDS_X = q_lem_x[np.where(states_z_test[p]==k)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            SLDS_A = np.linalg.inv(SLDS_X[:-1].T@SLDS_X[:-1])@SLDS_X[:-1].T@SLDS_X[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            SLDS_all.append(SLDS_A)\n",
    "\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    SLDS_all = np.stack(SLDS_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    evals_SLDS_all = np.linalg.eigvals(SLDS_all).reshape(-1)\n",
    "    timescales = np.log(np.abs(np.real(evals_Uhat_all)))\n",
    "    timescales_SLDS = np.log(np.abs(np.real(evals_SLDS_all)))\n",
    "    \n",
    "    ax.scatter(x=timescales,y=np.random.uniform(y_ranges[timescale,0],y_ranges[timescale,1],timescales.shape[0]),\n",
    "               c=clrs_b[timescale],alpha=.5,edgecolor='None')\n",
    "    ax.scatter(x=timescales_SLDS,y=np.random.uniform(y_ranges[timescale+1,0],y_ranges[timescale+1,1],timescales_SLDS.shape[0]),\n",
    "               c=clrs_b[timescale+1],alpha=.5,edgecolor='None')\n",
    "\n",
    "    for n in range(len(timescales_As)):\n",
    "        ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "    # ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[timescale])\n",
    "    ax.set_xscale('symlog',linthresh=1)\n",
    "    ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(seq_len))])\n",
    "    ax.set_yticklabels(seq_len,fontsize=fontsize)\n",
    "    ax.set_ylabel('T',fontsize=fontsize)\n",
    "    # ax.spines.left.set_visible(False)\n",
    "    ax.set_xlabel('log(Re($\\lambda$))',fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9730af5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "p=0\n",
    "seq_len=[200,'SLDS']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "fig, axs = plt.subplots(1,1,figsize=(4,4))\n",
    "ax = axs\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "timescales_As = np.log(np.abs(np.real(np.linalg.eigvals(As))))[:,:,0].reshape(-1)\n",
    "timescales_SLDS = np.unique(np.log(np.abs(np.real(np.linalg.eigvals(slds.dynamics.As)))))\n",
    "ts = 0\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for timescale in range(len(seq_len)-1):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[timescale])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[timescale])]['R_bar'].shape[-1])\n",
    "    # R_hat = result_dict['{}'.format(seq_len[timescale])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[timescale])]['R_hat'].shape[-1])\n",
    "    \n",
    "    eig_Ut = eig_Ut[np.where(np.real(eig_Ut)!=0)]\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    # Uhat_all = []; SLDS_all=[]\n",
    "    W = result_dict['{}'.format(seq_len[timescale])]['W'].reshape(-1,result_dict['{}'.format(seq_len[timescale])]['W'].shape[-1])\n",
    "    U_t = torch.matmul(torch.Tensor(W[:,None,:]), model.temporal.unsqueeze(0).cpu().detach()).reshape(-1, model.r_dim, model.r_dim).numpy()\n",
    "    for p in range(ssm_params.Nlds):\n",
    "        for k in range(ssm_params.n_disc_states):\n",
    "            Ut = U_t[np.where(states_z_test[p]==k)[0]]\n",
    "            eig_Ut = np.linalg.eigvals(Ut)\n",
    "            timescales = np.log(np.abs(np.real(eig_Ut)))\n",
    "\n",
    "            plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[p])\n",
    "ax.set_xscale('symlog',linthresh=1)\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3478eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f96276",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ut.shape,timescales.shape,timescales_As.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in range(10):\n",
    "    # As[states_z_test[:,t]]\n",
    "full_state_As = np.stack([As[(0,1,2),lst[n]] for n in range(len(lst))])\n",
    "\n",
    "timescales_As = np.log(np.abs(np.real(np.linalg.eigvals(full_state_As))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ranges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(4,4))\n",
    "ax = axs\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(np.unique(full_state_z)))[:,(0,-1)]\n",
    "ts=0\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []; SLDS_all=[]\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    eig_Ut = np.linalg.eigvals(Uhat_0)\n",
    "    timescales = np.log(np.abs(np.real(eig_Ut)))\n",
    "    \n",
    "    for p in range(timescales_As.shape[1]):\n",
    "        for k in range(timescales_As.shape[2]):\n",
    "            ax.axvline(x=timescales_As[state,p,k],ymin=y_ranges[state,0],ymax=y_ranges[state,1],c=clrs[state])\n",
    "# plot_hist(timescales,-2,.1,.1,ax,'TiDHy',clr=clrs[state])\n",
    "    ax.scatter(x=timescales,y=np.random.uniform(y_ranges[state,0],y_ranges[state,1],timescales.shape[0]),\n",
    "                c=clrs[state],s=40,alpha=.75,edgecolor='None')\n",
    "# ax.set_xscale('symlog',linthresh=1)\n",
    "ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "ax.set_yticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize)\n",
    "ax.set_xlabel('log(Re($\\lambda$))',fontsize=fontsize)\n",
    "# ax.set_ylabel('T',fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea08801",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(15,10))\n",
    "minmax = 10\n",
    "for n in range(As.shape[0]):\n",
    "    for m in range(As.shape[1]):\n",
    "        ax = axs[n,m]\n",
    "        q = plot_dynamics_2d(As[n,m], \n",
    "                            bias_vector=bs[n,m],#np.zeros(2),\n",
    "                            mins=[-minmax,-minmax],#plot_x.min(axis=0),\n",
    "                            maxs=[minmax,minmax],#plot_x.max(axis=0),\n",
    "                            npts=7,\n",
    "                            axis=ax)\n",
    "        ax.axis('off')\n",
    "        ax.axis('equal')\n",
    "# fig.savefig(cfg.paths.fig_dir / 'dynamics.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize= (5,4)\n",
    "t=0; dt = 1000\n",
    "ts = -1\n",
    "##### Calculated correlation ######\n",
    "# R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "R_hat = q_lem_x\n",
    "states_x_test = result_dict['{}'.format(seq_len[ts])]['states_x_test']/np.max(np.abs(result_dict['{}'.format(seq_len[ts])]['states_x_test']),axis=0)\n",
    "# states_x_test = result_dict['{}'.format(seq_len[ts])]['states_x_train']/np.max(np.abs(result_dict['{}'.format(seq_len[ts])]['states_x_train']),axis=0)\n",
    "# R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "ccs = np.zeros((R_hat.shape[-1],states_x_test.shape[-1]))\n",
    "for n in range(R_hat.shape[-1]):\n",
    "    for m in range(states_x_test.shape[-1]):\n",
    "        ccs[n,m] = np.corrcoef(R_hat[:,n],states_x_test[:,m])[0,1]\n",
    "Rhat_plot = np.zeros_like(states_x_test)\n",
    "# Rbar_plot = np.zeros_like(states_x_test)\n",
    "x_plot = np.zeros_like(R_hat)\n",
    "norm = True\n",
    "ind = np.zeros((states_x_test.shape[-1],),dtype=int)\n",
    "for n in range(states_x_test.shape[-1]):\n",
    "    ind[n] = np.nanargmax(np.abs(ccs[:,n]))\n",
    "    flip_Rhat = -1 if ccs[ind[n],n,] < 0 else 1\n",
    "    Rhat_plot[:,n] = flip_Rhat*R_hat[:,ind[n]]\n",
    "    # Rbar_plot[:,n] = flip_Rhat*R_bar[:,ind[n]]\n",
    "    if norm:\n",
    "        Rhat_plot[:,n] = Rhat_plot[:,n]/np.max(np.abs(Rhat_plot[:,n]))\n",
    "        # Rbar_plot[:,n] = Rbar_plot[:,n]/np.max(np.abs(Rbar_plot[:,n]))\n",
    "# R_acor = np.stack([autocorr(R_hat[:,n],norm=norm) for n in range(R_hat.shape[-1])],axis=1)\n",
    "# x_acor = np.stack([autocorr(states_x_test[:,n],norm=norm) for n in range(states_x_test.shape[-1])],axis=1)\n",
    "\n",
    "print(ind,np.max(np.abs(np.round(ccs,decimals=2)),axis=0))\n",
    "\n",
    "\n",
    "##### Plotting latent states #####\n",
    "spacing= .5\n",
    "max_ccs = np.max(np.abs(np.round(ccs,decimals=2)),axis=0)\n",
    "fig,axs = plt.subplots(1,1,figsize=(5,5),sharey=True,gridspec_kw={'wspace':.15})\n",
    "ax = axs\n",
    "colormap = plt.get_cmap(cmap, len(np.unique(full_state_z)))\n",
    "hlines_x,hlines_Rhat = [],[]\n",
    "for n in range(states_x_test.shape[-1]):\n",
    "    mean_centered_x = states_x_test[t:t+dt,n] - np.mean(states_x_test[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_x + n/spacing,color='k', lw=1,zorder=1)\n",
    "    hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "    mean_centered_Rhat = Rhat_plot[t:t+dt,n] - np.mean(Rhat_plot[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Rhat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='$\\hat{{R}}_{{{}}}$={:.02}'.format(ind[n],max_ccs[n]))\n",
    "    hlines_Rhat.append(np.mean(mean_centered_Rhat + n/spacing,axis=0))\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1))\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "ax.set_ylabel('$\\hat{{r}}_t$',fontsize=fontsize)\n",
    "ax.set_title('Learned Latent States',fontsize=fontsize)\n",
    "ax.set_xlabel('Time')\n",
    "for n in range(len(hlines_x)):\n",
    "    ax.text(x=dt+15,y=hlines_x[n],s='$r_{{{:d}}}={:.02}$'.format(ind[n],max_ccs[n]),fontsize=10)\n",
    "# ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_Rhat_vs_x.png'.format(nfig),dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result_dict['{}'.format(seq_len[ts])]['I'][t:t+dt,0])\n",
    "plt.plot(result_dict['{}'.format(seq_len[ts])]['I_hat'][t:t+dt,0])\n",
    "plt.plot(q_lem_y[t:t+dt,0],'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Errors_SLDS = (inputs_test_SLDS-q_lem_y)\n",
    "Input_Errors = (I-Ihat)\n",
    "\n",
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "ax = axs\n",
    "\n",
    "plot_hist(Input_Errors_SLDS.flatten(),0,.05,1e-3,ax,'SLDS',clr='k')\n",
    "plot_hist(Input_Errors.flatten(),0,.05,1e-3,ax,'TiDHy',clr='r')\n",
    "ax.legend(frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb091262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "t=0; dt=1000\n",
    "fontsize=12\n",
    "spacing= .5\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "I_shuff  = deepcopy(result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1]))\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,5))\n",
    "ax = axs[0]\n",
    "cmap,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=1,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing,ls='-',color='#4e7eb3ff', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*I.shape[-1]))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1]+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "# # im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1],1)),cmap=cmap,norm=norm,alpha=.5)\n",
    "# cbar = fig.colorbar(im,ax=axs,aspect=30)\n",
    "# cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "# cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "# cbar.outline.set_linewidth(1)\n",
    "# cbar.minorticks_off()\n",
    "# cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "ax.set_yticks(hlines_I)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(0,1200,200),fontsize=fontsize)\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "ax.set_ylabel('Observation #',fontsize=fontsize)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.025,1.05),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "# ax.set_title('Learned Observations',fontsize=fontsize)\n",
    "lim0 = 0\n",
    "lim1 = .105\n",
    "hbins = .0075\n",
    "Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# Input_Errors_SLDS = np.mean((inputs_test_SLDS-q_lem_y)**2,axis=0)\n",
    "I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "ax = axs[1]\n",
    "count,edges = np.histogram(Input_Errors,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='#757575ff',width=hbins, alpha=1,zorder=1,label='data')\n",
    "count,edges = np.histogram(Input_Errors_shuff,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='r',width=hbins, alpha=1,zorder=1,label='shuffle') \n",
    "# count,edges = np.histogram(Input_Errors_SLDS,bins=np.arange(lim0,lim1,hbins))\n",
    "# edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "# ax.bar(edges_mid, count/len(Input_Errors_SLDS),color='g',width=hbins, alpha=.5,zorder=1,label='SLDS') \n",
    "ax.set_ylabel('Proportion',fontsize=fontsize)\n",
    "ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize)\n",
    "# ax = axs[1]\n",
    "# fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "# Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "# Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "# xs = np.arange(I.shape[-1])\n",
    "# heights = Input_Errors\n",
    "# ax.barh(y=hlines_I,width=heights,color='k',)\n",
    "# ax.errorbar(heights,hlines_I,xerr=np.std((I-Ihat),axis=0)/np.sqrt((I-Ihat).shape[0]),ls='none',color='tab:gray',capsize=3)\n",
    "# ax.axvline(x=np.mean(Input_Errors_shuff),c='k',ls='--',lw=1,label='shuffle error')\n",
    "# # ax.set_xticks(np.arange(0,.004,.001))\n",
    "# ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "# ax.set_title('Average Error',fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(heights.shape[-1])+1)\n",
    "# ax.legend(bbox_to_anchor=(.3, .95), loc='lower left', borderaxespad=0.,frameon=False,fontsize=fontsize)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_observations_pred.png'.format(nfig),dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166333e",
   "metadata": {},
   "source": [
    "# SLDS Single seq len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26223c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states_x_test = data_dict['states_x_test']\n",
    "states_z_test = data_dict['states_z_test']\n",
    "# states_z_test = data_dict['states_z']\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "##### Set up combinatorics of timescales #####\n",
    "lst = list(itertools.product([0, 1], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros(ssm_params['time_bins_test'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,states_z_test)] = n\n",
    "\n",
    "\n",
    "save_figs = False\n",
    "p=-1\n",
    "# W = result_dict['{}'.format(seq_len[p])]['W'].reshape(-1,result_dict['{}'.format(seq_len[p])]['W'].shape[-1])\n",
    "# b = result_dict['b'].reshape(-1,result_dict['b'].shape[-1])\n",
    "W = result_dict['W'].reshape(-1,result_dict['W'].shape[-1])\n",
    "I = result_dict['I'].reshape(-1,result_dict['I'].shape[-1])\n",
    "I_hat = result_dict['I_hat'].reshape(-1,result_dict['I_hat'].shape[-1])\n",
    "I_bar = result_dict['I_bar'].reshape(-1,result_dict['I_bar'].shape[-1])\n",
    "R_hat = result_dict['R_hat'].reshape(-1,result_dict['R_hat'].shape[-1])\n",
    "R_bar = result_dict['R_bar'].reshape(-1,result_dict['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['R2_hat'].reshape(-1,result_dict['R2_hat'].shape[-1])\n",
    "Vt = result_dict['Vt'].reshape(-1,result_dict['Vt'].shape[-2],result_dict['Vt'].shape[-1])\n",
    "# ##### Plot dynamic matrices #####\n",
    "if cfg.model.low_rank_temp:\n",
    "    Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "else:\n",
    "    Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "    U_t = torch.matmul(torch.Tensor(W[:,None,:]), model.temporal.unsqueeze(0).cpu().detach()).reshape(-1, model.r_dim, model.r_dim).numpy()\n",
    "As = np.stack([v for v in data_dict['As'].values()])\n",
    "bs = np.stack([v for v in data_dict['bs'].values()])\n",
    "timescales = data_dict['timescales'].reshape(3,2)\n",
    "nfig = 0\n",
    "fontsize=10\n",
    "t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_comps=ssm_params['latent_dim']\n",
    "t=1500; dt=200\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=5,ncols=2, figure=fig, wspace=.05,hspace=.1)\n",
    "\n",
    "# gs00 = gridspec.GridSpecFromSubplotSpec(1, 3, subplot_spec=gs0[0,1:],wspace=.8,hspace=.7)\n",
    "# gs01 = gridspec.GridSpecFromSubplotSpec(3,1, subplot_spec=gs0[0:,:1],wspace=.05,hspace=.8)\n",
    "axs = np.array([fig.add_subplot(gs0[:3,0]),\n",
    "                fig.add_subplot(gs0[3:,0]),\n",
    "                fig.add_subplot(gs0[:3,1]),\n",
    "                fig.add_subplot(gs0[3:,1])])\n",
    "axs[3].sharex(axs[2])\n",
    "ts = 2\n",
    "##### Pannel a #####\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "ax = axs[0]\n",
    "spacing = 1\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z)))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=1,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "ax.set_yticks(hlines_I)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "##### Pannel c #####\n",
    "ax = axs[2]\n",
    "count=0\n",
    "acc_ssm = accuracy_score(full_state_z, SLDS_states)\n",
    "acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "spacing = .5\n",
    "\n",
    "hlines_x,hlines_Rhat = [],[]\n",
    "for p in range(ssm_params['Nlds']):\n",
    "    # states_x_cca = states_x_train[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    states_x_cca = states_x_test[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    # states_x_cca = states_x_test[:,p:p+1]\n",
    "    cca = CCA(n_components=ssm_params['latent_dim'],max_iter=1000)\n",
    "    X_c,Y_c = cca.fit_transform(states_x_cca,R_hat)\n",
    "    cca_coefficient = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=n_comps)\n",
    "    x_w = cca.x_weights_\n",
    "    y_w = cca.y_weights_\n",
    "    cca_angles = [np.rad2deg(angle_between(X_c[:,n],Y_c[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_x = [np.rad2deg(angle_between(X_c[:,n],states_x_cca[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_r = [np.rad2deg(angle_between(Y_c[:,n],R_hat[:,n])) for n in range(n_comps)]\n",
    "    for n in range(n_comps):\n",
    "        print('comp {}, cc: {:.03}, ang: {:.03}, ang_x:{:.03}, ang_r:{:.03}'.format(n,cca_coefficient[n],cca_angles[n],cca_angles_x[n],cca_angles_r[n]))\n",
    "\n",
    "    for i in range(X_c.shape[-1]):\n",
    "        mean_centered_x = X_c[t:t+dt,i] - np.mean(X_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_x=mean_centered_x/(np.max(np.abs(mean_centered_x)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_x + count/spacing,color='k', lw=2,zorder=1)\n",
    "        hlines_x.append(np.mean(mean_centered_x + count/spacing,axis=0))\n",
    "        mean_centered_Rhat = Y_c[t:t+dt,i] - np.mean(Y_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_Rhat=mean_centered_Rhat/(np.max(np.abs(mean_centered_Rhat)))\n",
    "        ax.plot(np.arange(0,dt),mean_centered_Rhat + count/spacing,ls='--',color='r', lw=1.5,zorder=2,label='$\\hat{{r}}_{{{}}}$={:.02}'.format(n,cca_coefficient[n]),alpha=1)\n",
    "        hlines_Rhat.append(np.mean(mean_centered_Rhat + count/spacing,axis=0))\n",
    "        ax.text(x=dt+15,y=hlines_x[count],s='cc = {:.02}'.format(cca_coefficient[i]),fontsize=fontsize)\n",
    "        count += 1\n",
    "    ax.set_yticks(hlines_x) \n",
    "    ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "    # ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "    ax.set_ylabel('latent variables',fontsize=fontsize)\n",
    "# ax.set_xticks([])\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps', fontsize=fontsize)\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*6))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*6+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "##### Pannel b #####\n",
    "ax = axs[1]\n",
    "dts = 1/len(np.unique(full_state_z))\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(np.unique(full_state_z)))[:,(0,-1)]\n",
    "full_state_As = np.stack([As[(0,1,2),lst[n]] for n in range(len(lst))])\n",
    "timescales_As = np.real(np.log(np.linalg.eigvals(full_state_As)))\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "\n",
    "for state in range(len(np.unique(full_state_z))):\n",
    "    ##### Plottign Eigenvalues #####\n",
    "    rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "    eig_Ut = np.linalg.eigvals(Uhat_0)\n",
    "    # rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "    # model = VAR(rhat2)\n",
    "    # results = model.fit(maxlags=2,ic='aic')\n",
    "    # eig_Ut = np.linalg.eigvals(results.coefs).reshape(-1)\n",
    "    timescales = 1/np.abs(np.real(np.log(eig_Ut)))\n",
    "    \n",
    "    for p in range(timescales_As.shape[1]):\n",
    "        for k in range(timescales_As.shape[2]):\n",
    "            ax.axvline(x=1/np.abs(timescales_As[state,p,k]),ymin=y_ranges[state,0],ymax=y_ranges[state,1],c=clrs_b[state])\n",
    "# plot_hist(timescales,-2,.1,.1,ax,'TiDHy',clr=clrs[state])\n",
    "    # ax.scatter(x=timescales,y=np.random.uniform(y_ranges[state,0],y_ranges[state,1],timescales.shape[0]),\n",
    "    #             c=clrs_b[state],s=40,alpha=.5,edgecolor='None')\n",
    "    ax.scatter(x=timescales,y=np.abs(eig_Ut),\n",
    "                c=clrs_b[state],s=40,alpha=.5,edgecolor='None')\n",
    "ax.set_xscale('symlog',linthresh=.01)\n",
    "ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(np.unique(full_state_z)))])\n",
    "ax.set_yticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize-2)\n",
    "# ax.set_xticks([0,-10e0,-10e-1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "ax.set_xlabel('Re(log($\\lambda$))',fontsize=fontsize,labelpad=-1)\n",
    "\n",
    "\n",
    "\n",
    "##### Pannel d #####\n",
    "ax = axs[3]\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(.5,3,1))\n",
    "ax.set_yticklabels(['SLDS \\n {:02}%'.format(int(np.round(acc_ssm*100))),'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=-.2)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in np.arange(len(lst2)-1,-1,-1)],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Benchmark_Compare3.pdf',dpi=300)\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_Fig2.pdf'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6f0fa",
   "metadata": {},
   "source": [
    "## Run SSM Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.dataset.ssm_params['random_projection'] = False\n",
    "# cfg.dataset.ssm_params['partial_sup'] = False\n",
    "# cfg.dataset.ssm_params['normalize'] = False\n",
    "\n",
    "# data_dict, cfg = load_dataset(cfg)\n",
    "# print('Our inputs have shape: {}'.format(data_dict['inputs_train'].shape))\n",
    "inputs_train_SLDS=data_dict['inputs_train'] # (data_dict['inputs_train']-np.mean(data_dict['inputs_train'],axis=-1,keepdims=True))/np.std(data_dict['inputs_train'],axis=-1,keepdims=True)\n",
    "inputs_test_SLDS=data_dict['inputs_test'] # (data_dict['inputs_test']-np.mean(data_dict['inputs_test'],axis=-1,keepdims=True))/np.std(data_dict['inputs_test'],axis=-1,keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssm.util import random_rotation, find_permutation\n",
    "print(\"Fitting SLDS with Laplace-EM\")\n",
    "inputs_train_SLDS= data_dict['inputs_train']#/np.max(np.abs(data_dict['inputs_train']),axis=0)\n",
    "inputs_test_SLDS=data_dict['inputs_test'] # (data_dict['inputs_test']-np.mean(data_dict['inputs_test'],axis=-1,keepdims=True))/np.std(data_dict['inputs_test'],axis=-1,keepdims=True)\n",
    "\n",
    "# cfg.model.input_dim = input_dim\n",
    "\n",
    "# Create the model and initialize its parameters\n",
    "slds = ssm.SLDS(N=inputs_train_SLDS.shape[-1], # Input dimension\n",
    "                K=8, # number of sets of dynamics dimensions\n",
    "                D=10, # latent dim\n",
    "                emissions=\"gaussian\")\n",
    "\n",
    "# Fit the model using Laplace-EM with a structured variational posterior\n",
    "q_lem_elbos, q_lem = slds.fit(inputs_train_SLDS, method=\"laplace_em\",\n",
    "                               variational_posterior=\"structured_meanfield\",\n",
    "                               initialize=False,\n",
    "                               num_iters=100, alpha=0.0,)\n",
    "\n",
    "\n",
    "posterior = slds._make_variational_posterior( variational_posterior=\"structured_meanfield\",datas=inputs_test_SLDS,inputs=None, masks=None, tags=None,method=\"laplace_em\")\n",
    "q_lem_x = posterior.mean_continuous_states[0]\n",
    "\n",
    "# Find the permutation that matches the true and inferred states\n",
    "slds.permute(find_permutation(full_state_z, slds.most_likely_states(q_lem_x, inputs_test_SLDS)))\n",
    "q_lem_z = slds.most_likely_states(q_lem_x, inputs_test_SLDS)\n",
    "\n",
    "# Smooth the data under the variational posterior\n",
    "q_lem_y = slds.smooth(q_lem_x, inputs_test_SLDS)\n",
    "import pickle\n",
    "\n",
    "with open(cfg.paths.data_dir/'ssm_slds_test_full_10D.pickle', 'wb') as handle:\n",
    "    pickle.dump(slds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(posterior, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssm.util import random_rotation, find_permutation\n",
    "\n",
    "print(\"Fitting rSLDS with Laplace-EM\")\n",
    "inputs_train_SLDS=data_dict['inputs_train']#/np.max(np.abs(data_dict['inputs_train']),axis=0)\n",
    "inputs_test_SLDS=data_dict['inputs_test'] # (data_dict['inputs_test']-np.mean(data_dict['inputs_test'],axis=-1,keepdims=True))/np.std(data_dict['inputs_test'],axis=-1,keepdims=True)\n",
    "\n",
    "# cfg.model.input_dim = input_dim\n",
    "\n",
    "# Fit an rSLDS with its default initialization, using Laplace-EM with a structured variational posterior\n",
    "rslds = ssm.SLDS(N=inputs_train_SLDS.shape[-1], # Input dimension\n",
    "                K=8, # number of sets of dynamics dimensions\n",
    "                D=6, # latent dim\n",
    "                transitions=\"recurrent_only\",\n",
    "                dynamics=\"diagonal_gaussian\",\n",
    "                emissions=\"gaussian_orthog\",\n",
    "                single_subspace=True)\n",
    "# rslds.initialize(inputs_train_SLDS)\n",
    "q_elbos_lem, q_lem = rslds.fit(inputs_train_SLDS, method=\"laplace_em\",\n",
    "                               variational_posterior=\"structured_meanfield\",\n",
    "                               initialize=False, num_iters=100, alpha=0.0)\n",
    "# xhat_lem = q_lem.mean_continuous_states[0]\n",
    "# rslds.permute(find_permutation(full_state_z, rslds.most_likely_states(xhat_lem, inputs_test_SLDS)))\n",
    "# zhat_lem = rslds.most_likely_states(xhat_lem, inputs_test_SLDS)\n",
    "\n",
    "posterior = rslds._make_variational_posterior( variational_posterior=\"structured_meanfield\",datas=inputs_test_SLDS,inputs=None, masks=None, tags=None,method=\"laplace_em\")\n",
    "q_lem_x = posterior.mean_continuous_states[0]\n",
    "\n",
    "# Find the permutation that matches the true and inferred states\n",
    "rslds.permute(find_permutation(full_state_z, rslds.most_likely_states(q_lem_x, inputs_test_SLDS)))\n",
    "q_lem_z = rslds.most_likely_states(q_lem_x, inputs_test_SLDS)\n",
    "\n",
    "# Smooth the data under the variational posterior\n",
    "q_lem_y = rslds.smooth(q_lem_x, inputs_test_SLDS)\n",
    "import pickle\n",
    "\n",
    "with open(cfg.paths.data_dir/'ssm_rslds_test_full.pickle', 'wb') as handle:\n",
    "    pickle.dump(rslds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(posterior, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3096cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ssm.util import random_rotation, find_permutation\n",
    "\n",
    "\n",
    "with open(cfg.paths.data_dir/'ssm_slds_test_full.pickle', 'rb') as handle:\n",
    "# with open(cfg.paths.data_dir/'ssm_rslds_test.pickle', 'rb') as handle:\n",
    "    slds = pickle.load(handle)\n",
    "    posterior = pickle.load(handle)\n",
    "    # q_lem = pickle.load(handle)\n",
    "# Get the posterior mean of the continuous states\n",
    "q_lem_x = posterior.mean_continuous_states[0]\n",
    "\n",
    "# Find the permutation that matches the true and inferred states\n",
    "slds.permute(find_permutation(full_state_z, slds.most_likely_states(q_lem_x, inputs_test_SLDS)))\n",
    "q_lem_z = slds.most_likely_states(q_lem_x, inputs_test_SLDS)\n",
    "\n",
    "# Smooth the data under the variational posterior\n",
    "q_lem_y = slds.smooth(q_lem_x, inputs_test_SLDS)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open(cfg.paths.save_dir/'ssm_slds_test.pickle', 'wb') as handle:\n",
    "#     pickle.dump(slds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     pickle.dump(test_elbo, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     pickle.dump(test_post, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_variables = [np.concatenate([result_dict['R2_hat'],result_dict['W'],result_dict['R_bar']],axis=-1),\n",
    "    result_dict['R2_hat'],\n",
    "    result_dict['W'],\n",
    "    result_dict['R_bar'],\n",
    "    result_dict['R_hat'],\n",
    "    result_dict['I']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68463585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nani = full_state_z.shape[0]\n",
    "ts=-1\n",
    "reg_variables = [np.concatenate([result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['W'],result_dict['{}'.format(seq_len[ts])]['R_bar']],axis=-1),\n",
    "    result_dict['{}'.format(seq_len[ts])]['R2_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['W'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_bar'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['R_hat'],\n",
    "    result_dict['{}'.format(seq_len[ts])]['I'],\n",
    "    q_lem_x]\n",
    "\n",
    "labels = ['all','R2_hat','W','R_bar','R_hat','I','SLDS']\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    # X_train = reg_vars[:tr_batch_size].reshape(-1,reg_vars.shape[-1])\n",
    "    # X_test = reg_vars[-batch_size:].reshape(-1,reg_vars.shape[-1])\n",
    "    # y_train = full_state_z[:tr_batch_size].reshape(-1)\n",
    "    # y_test = full_state_z[-batch_size:].reshape(-1)\n",
    "    # y_train = robot_type_test[:tr_batch_size].reshape(-1)\n",
    "    # y_test = robot_type_test[-batch_size:].reshape(-1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    # neigh = RidgeClassifierCV()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    # clf = RidgeClassifierCV().fit(reg_variables, full_state_z)\n",
    "    # y_pred = neigh.predict(X_test)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_compare = np.stack([q_lem_z,best_pred,full_state_z],axis=0)\n",
    "# state_compare = np.stack([q_lem_z,full_state_z],axis=0)\n",
    "# np.save(cfg.paths.save_dir/'state_compare_test2.npy',state_compare)\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "n_comps=ssm_params['latent_dim']\n",
    "fontsize=20\n",
    "t=6500; dt=1000\n",
    "spacing = .5\n",
    "fig,axs = plt.subplots(2,1,figsize=(10,11),layout='constrained',sharex=True,gridspec_kw={'height_ratios': [2, 1]})\n",
    "ax = axs[0]\n",
    "count=0\n",
    "acc_ssm = accuracy_score(full_state_z, q_lem_z)\n",
    "acc_TiDHy = accuracy_score(full_state_z, y_pred)\n",
    "cmap2,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "\n",
    "hlines_x,hlines_Rhat = [],[]\n",
    "for p in range(ssm_params['Nlds']):\n",
    "    # states_x_cca = states_x_train[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    states_x_cca = states_x_test[:,(ssm_params['latent_dim']*(p)):(p+1)*ssm_params['latent_dim']]\n",
    "    # states_x_cca = states_x_test[:,p:p+1]\n",
    "    cca = CCA(n_components=ssm_params['latent_dim'],max_iter=1000)\n",
    "    X_c,Y_c = cca.fit_transform(states_x_cca,R_hat)\n",
    "    cca_coefficient = np.corrcoef(X_c.T, Y_c.T).diagonal(offset=n_comps)\n",
    "    x_w = cca.x_weights_\n",
    "    y_w = cca.y_weights_\n",
    "    cca_angles = [np.rad2deg(angle_between(X_c[:,n],Y_c[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_x = [np.rad2deg(angle_between(X_c[:,n],states_x_cca[:,n])) for n in range(n_comps)]\n",
    "    cca_angles_r = [np.rad2deg(angle_between(Y_c[:,n],R_hat[:,n])) for n in range(n_comps)]\n",
    "    for n in range(n_comps):\n",
    "        print('comp {}, cc: {:.03}, ang: {:.03}, ang_x:{:.03}, ang_r:{:.03}'.format(n,cca_coefficient[n],cca_angles[n],cca_angles_x[n],cca_angles_r[n]))\n",
    "\n",
    "    for i in range(X_c.shape[-1]):\n",
    "        mean_centered_x = X_c[t:t+dt,i] - np.mean(X_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_x=mean_centered_x/(np.max(np.abs(mean_centered_x)))\n",
    "        ax.plot(mean_centered_x + count/spacing,color='k', lw=3,zorder=1)\n",
    "        hlines_x.append(np.mean(mean_centered_x + count/spacing,axis=0))\n",
    "        mean_centered_Rhat = Y_c[t:t+dt,i] - np.mean(Y_c[t:t+dt,i],axis=0)\n",
    "        mean_centered_Rhat=mean_centered_Rhat/(np.max(np.abs(mean_centered_Rhat)))\n",
    "        ax.plot(mean_centered_Rhat + count/spacing,ls='--',color='r', lw=2.5,zorder=2,label='$\\hat{{r}}_{{{}}}$={:.02}'.format(n,cca_coefficient[n]),alpha=1)\n",
    "        hlines_Rhat.append(np.mean(mean_centered_Rhat + count/spacing,axis=0))\n",
    "        ax.text(x=dt+15,y=hlines_x[count],s='cc = {:.02}'.format(cca_coefficient[i]),fontsize=fontsize)\n",
    "        count += 1\n",
    "    ax.set_yticks(hlines_x) \n",
    "    ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize)\n",
    "    # ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "    ax.set_ylabel('latent variables',fontsize=fontsize)\n",
    "    # ax.set_title('CCA per system',fontsize=fontsize)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*6))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*6+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.1,1.1),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_xticks(np.arange(0,dt+200,200))\n",
    "ax.set_xticklabels(np.arange(0,dt+200,200),fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(.5,3,1))\n",
    "ax.set_yticklabels(['SSM \\n {:02}%'.format(int(np.round(acc_ssm*100))),'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize)\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=-.2)\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Benchmark_Compare3.pdf',dpi=300)\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_Fig2C.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baae4a0",
   "metadata": {},
   "source": [
    "## SLDS Figure Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b614d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "# reg_variables = np.concatenate([R2_hat,W,R_bar],axis=1)\n",
    "# X_pca = pca.fit_transform(reg_variables)\n",
    "X_pca = pca.fit_transform(result_dict['R2_hat'])\n",
    "# X_pca = R2_hat\n",
    "# X_pca = pca.fit_transform(R_hat)\n",
    "# X_pca = pca.fit_transform(states_x_test)\n",
    "# X_pca = pca.fit_transform(W[:,np.sum(W,axis=0)>0])\n",
    "# X_pca = pca.fit_transform(R_bar)\n",
    "# X_pca = pca.fit_transform(I)\n",
    "# X_pca = pca.fit_transform(b)\n",
    "# X_pca = pca.fit_transform(Vt.reshape(Vt.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ab262",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "t = 0; dt = len(X_pca)\n",
    "fig = go.Figure()\n",
    "comps = X_pca[full_state_z==n][t:t+dt]\n",
    "# comps = X_pca[states_z_test[0]==n][t:t+dt]\n",
    "# labels = full_state_z[full_state_z==n]\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=X_pca[:,0], y=X_pca[:,1], z=X_pca[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=full_state_z,                # set color to an array/list of desired values\n",
    "            colorscale='turbo',   # choose a colorscale\n",
    "            opacity=.5\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# fig.add_trace(\n",
    "#     go.Scatter3d(\n",
    "#         x=comps[:,0], y=comps[:,1], z=comps[:,2],\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=2,\n",
    "#             color='red',                # set color to an array/list of desired values\n",
    "#             # colorscale='Viridis',   # choose a colorscale\n",
    "#             opacity=.1\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=500,\n",
    "    height=500,\n",
    "    autosize=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ab477",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=10\n",
    "spacing= .5\n",
    "I = result_dict['I'].reshape(-1,result_dict['I'].shape[-1])\n",
    "Ihat = result_dict['I_hat'].reshape(-1,result_dict['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig,axs = plt.subplots(1,1,figsize=(4,4),sharey=True)\n",
    "ax = axs\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=1,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*I.shape[-1]))\n",
    "im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1]+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1],1)),cmap=cmap,norm=norm,alpha=.5)\n",
    "cbar = fig.colorbar(im,ax=axs,aspect=30)\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))])\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "ax.set_yticks(hlines_I)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_I)+1))\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "ax.set_ylabel('Observation #',fontsize=fontsize)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=10,loc='upper right',bbox_to_anchor=(1.025,1.15),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "# ax.set_title('Learned Observations',fontsize=fontsize)\n",
    "\n",
    "# ax = axs[1]\n",
    "# Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "# Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "# xs = np.arange(I.shape[-1])\n",
    "# heights = Input_Errors\n",
    "# ax.barh(y=hlines_I,width=heights,color='k',)\n",
    "# ax.errorbar(heights,hlines_I,xerr=np.std((I-Ihat),axis=0)/np.sqrt((I-Ihat).shape[0]),ls='none',color='tab:gray',capsize=3)\n",
    "# ax.axvline(x=np.mean(Input_Errors_shuff),c='k',ls='--',lw=2,label='shuffle error')\n",
    "# # ax.set_xticks(np.arange(0,.004,.001))\n",
    "# ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "# ax.set_title('Average Error',fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(heights.shape[-1])+1)\n",
    "# ax.legend(bbox_to_anchor=(.3, .95), loc='lower left', borderaxespad=0.,frameon=False,fontsize=fontsize)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_observations_pred.pdf'.format(nfig),dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd0f76",
   "metadata": {},
   "source": [
    "# Load Anymal Terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffe840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'AnymalTerrain'\n",
    "base_dir = Path('/data/users/eabe/hypernets/{}/DPC_{}/'.format(dataset,dataset))\n",
    "configs = sorted(list(base_dir.rglob('*config.yaml'))[::2])\n",
    "\n",
    "for n,conf in enumerate(configs):\n",
    "    print(n,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3dfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "params = OmegaConf.load(default_model_config)\n",
    "cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "for k in cfg.paths.keys():\n",
    "    cfg.paths[k] = Path(cfg.paths[k])\n",
    "    cfg.paths[k].mkdir(parents=True, exist_ok=True)\n",
    "params_curr = cfg.dataset.model\n",
    "cfg.dataset.model = OmegaConf.merge(params, params_curr)\n",
    "cfg.dataset.train.normalize_obs = False\n",
    "cfg.model.batch_converge=False\n",
    "cfg.model.Orth_alpha_spat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238228b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict, cfg = load_dataset(cfg)\n",
    "# ##### Convert to float tensors #####\n",
    "train_inputs = torch.tensor(data_dict['inputs_train']).float()\n",
    "test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "\n",
    "input_dim = train_inputs.shape[-1]\n",
    "\n",
    "train_inputs = train_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "test_inputs = test_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "cfg.model.input_dim = input_dim\n",
    "\n",
    "if cfg.train.batch_size_input:\n",
    "    batch_size_train = train_inputs.shape[0]\n",
    "    batch_size_test = test_inputs.shape[0]\n",
    "else:\n",
    "    batch_size_train = cfg.train.batch_size\n",
    "    # batch_size_test = cfg.train.batch_size\n",
    "    batch_size_test = test_inputs.shape[0]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "dataloader_train = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size_train,pin_memory=True,shuffle=True,drop_last=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size_test,pin_memory=True,drop_last=True)\n",
    "device = torch.device(\"cuda:{}\".format(cfg.train['gpu']) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Load weights and params #####\n",
    "rerecord = False\n",
    "epoch = 1000\n",
    "model = TiDHy(cfg.model, device, show_progress=cfg.train.show_progress).to(device)\n",
    "# set_seed(42)\n",
    "load_path = cfg.paths.ckpt_dir/'best.pth.tar'\n",
    "# load_path = sorted(list(cfg.paths.ckpt_dir.rglob('last_{}.pth.tar'.format(epoch))))[0]\n",
    "data_load = torch.load(load_path,map_location=device)\n",
    "model.load_state_dict(data_load['state_dict'])\n",
    "model.to(device)\n",
    "print('Loaded from {} epoch'.format(data_load['epoch']))\n",
    "epoch = data_load['epoch']\n",
    "\n",
    "\n",
    "# result_dict = load_results(model, dataloader_test, data_dict, device, cfg, epoch, rerecord=rerecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "# cmap_small = ListedColormap(clrs[:len(np.unique(full_state_z))])\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "# cmap_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b51ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000,1500]\n",
    "result_dict_all = {}\n",
    "for new_seq_len in seq_len:\n",
    "    set_seed(42)\n",
    "    test_inputs = torch.tensor(data_dict['inputs_test'].reshape(-1,new_seq_len,input_dim)).float()\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "    dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=test_inputs.shape[0],pin_memory=True,shuffle=False,drop_last=True)\n",
    "    result_dict={}\n",
    "    for Nbatch, batch in enumerate(dataloader_test):\n",
    "        X = batch[0].to(device,non_blocking=True)\n",
    "        spatial_loss,temp_loss,result_dict_temp = model.evaluate_record(X)\n",
    "        if Nbatch==0:\n",
    "            result_dict = result_dict_temp\n",
    "        else:\n",
    "            for key in result_dict.keys():\n",
    "                if isinstance(result_dict[key],torch.Tensor):\n",
    "                    result_dict[key] = torch.cat((result_dict[key],result_dict_temp[key]),dim=0)\n",
    "\n",
    "\n",
    "    for key in result_dict.keys():\n",
    "        if isinstance(result_dict[key],torch.Tensor):\n",
    "            result_dict[key] = result_dict[key].cpu().detach().numpy()\n",
    "    if 'states_x_test' in data_dict.keys():\n",
    "        result_dict['states_x_test'] = data_dict['states_x_test']\n",
    "        result_dict['states_x_train'] = data_dict['states_x']\n",
    "    if 'As' in data_dict.keys():\n",
    "        result_dict['As'] = data_dict['As']\n",
    "        result_dict['bs'] = data_dict['bs']\n",
    "    if 'states_z_test' in data_dict.keys():\n",
    "        result_dict['states_z_test'] = data_dict['states_z_test']\n",
    "        result_dict['states_z_train'] = data_dict['states_z']\n",
    "\n",
    "    for key in result_dict.keys():\n",
    "        if isinstance(result_dict[key],dict):\n",
    "            result_dict[key] = [result_dict[key][key2] for key2 in result_dict[key].keys()]\n",
    "            \n",
    "            \n",
    "    ##### Save Results #####\n",
    "    ioh5.save(cfg.paths.log_dir/'temp_results_{}_T{:04d}.h5'.format(epoch,new_seq_len),result_dict)\n",
    "    \n",
    "    reshape_list = ['W','I','I_hat','I_bar','R_hat','R_bar','R2_hat']\n",
    "    for key in reshape_list:\n",
    "        if key in result_dict.keys():\n",
    "            result_dict[key] = result_dict[key].reshape(-1,result_dict[key].shape[-1])\n",
    "    result_dict_all['{}'.format(new_seq_len)] = result_dict\n",
    "ioh5.save(cfg.paths.log_dir/'temp_results_{}_seq.h5'.format(epoch),result_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = ioh5.load(cfg.paths.log_dir/'temp_results_{}_seq.h5'.format(epoch))\n",
    "seq_len = sorted([int(el) for el in list(result_dict.keys())])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_figs = False\n",
    "ts=-2\n",
    "W = result_dict['{}'.format(seq_len[ts])]['W'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['W'].shape[-1])\n",
    "# b = result_dict['b'].reshape(-1,result_dict['b'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "Ibar = result_dict['{}'.format(seq_len[ts])]['I_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_bar'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "# Ut = result_dict['Ut'].reshape(-1,result_dict['Ut'].shape[-2],result_dict['Ut'].shape[-1])\n",
    "# ##### Plot dynamic matrices #####\n",
    "if cfg.model.low_rank_temp:\n",
    "    Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "else:\n",
    "    Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "    U_t = torch.matmul(torch.Tensor(W[:,None,:]), model.temporal.unsqueeze(0).cpu().detach()).reshape(-1, model.r_dim, model.r_dim).numpy()\n",
    "\n",
    "nfig = 0\n",
    "t = 0; dt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50825e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "t=0; dt=1000\n",
    "spacing= .1\n",
    "ts=-2\n",
    "I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "I_shuff  = deepcopy(result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1]))\n",
    "Ihat = result_dict['{}'.format(seq_len[ts])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1])\n",
    "# fig,axs = plt.subplots(1,2,figsize=(5,3),sharey=True,gridspec_kw={'wspace':.15,'width_ratios':[2,1]})\n",
    "fig,axs = plt.subplots(1,2,figsize=(8,5))\n",
    "ax = axs[0]\n",
    "# cmap,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "hlines_I,hlines_Ihat = [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered_I = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_I + n/spacing,color='k', lw=1,zorder=1,label='Data')\n",
    "    hlines_I.append(np.mean(mean_centered_I + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(mean_centered_Ihat + n/spacing,ls='-',color='#4e7eb3ff', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "# X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*I.shape[-1]))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1]+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "# # im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(2*I.shape[-1],1)),cmap=cmap,norm=norm,alpha=.5)\n",
    "# cbar = fig.colorbar(im,ax=axs,aspect=30)\n",
    "# cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "# cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))],fontsize=fontsize)\n",
    "# cbar.outline.set_linewidth(1)\n",
    "# cbar.minorticks_off()\n",
    "# cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "ax.set_yticks(hlines_I)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_I)+1),fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(0,1200,200),fontsize=fontsize)\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "ax.set_ylabel('Observation #',fontsize=fontsize)\n",
    "ax.legend(['data','pred'],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.025,1.05),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "# ax.set_title('Learned Observations',fontsize=fontsize)\n",
    "lim0 = 0\n",
    "lim1 = 20\n",
    "hbins = 1\n",
    "Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# Input_Errors_SLDS = np.mean((inputs_test_SLDS-q_lem_y)**2,axis=0)\n",
    "I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "ax = axs[1]\n",
    "count,edges = np.histogram(Input_Errors,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='#757575ff',width=hbins, alpha=1,zorder=1,label='data')\n",
    "count,edges = np.histogram(Input_Errors_shuff,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(Input_Errors),color='r',width=hbins, alpha=1,zorder=1,label='shuffle') \n",
    "# count,edges = np.histogram(Input_Errors_SLDS,bins=np.arange(lim0,lim1,hbins))\n",
    "# edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "# ax.bar(edges_mid, count/len(Input_Errors_SLDS),color='g',width=hbins, alpha=.5,zorder=1,label='SLDS') \n",
    "ax.set_ylabel('Proportion',fontsize=fontsize)\n",
    "ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize)\n",
    "# ax = axs[1]\n",
    "# fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "# Input_Errors = np.mean((I-Ihat)**2,axis=0)\n",
    "# I_shuff = shuffle_along_axis(I_shuff,axis=1)\n",
    "# Input_Errors_shuff = np.mean((I_shuff-Ihat)**2,axis=0)\n",
    "# xs = np.arange(I.shape[-1])\n",
    "# heights = Input_Errors\n",
    "# ax.barh(y=hlines_I,width=heights,color='k',)\n",
    "# ax.errorbar(heights,hlines_I,xerr=np.std((I-Ihat),axis=0)/np.sqrt((I-Ihat).shape[0]),ls='none',color='tab:gray',capsize=3)\n",
    "# ax.axvline(x=np.mean(Input_Errors_shuff),c='k',ls='--',lw=1,label='shuffle error')\n",
    "# # ax.set_xticks(np.arange(0,.004,.001))\n",
    "# ax.set_xlabel('MSE',fontsize=fontsize)\n",
    "# ax.set_title('Average Error',fontsize=fontsize)\n",
    "# ax.set_xticklabels(np.arange(heights.shape[-1])+1)\n",
    "# ax.legend(bbox_to_anchor=(.3, .95), loc='lower left', borderaxespad=0.,frameon=False,fontsize=fontsize)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_observations_pred.png'.format(nfig),dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "t=0; dt = 1000\n",
    "# cmap2,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig, axs = plt.subplots(2,2,figsize=(20,12))\n",
    "axs = axs.flatten()\n",
    "fontsize=13\n",
    "ax = axs[0]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R2_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-.5,.75,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('$\\hat{r}^h$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax.legend(['T={}'.format(Ln) for Ln in seq_len],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.15,1.05),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "ax = axs[1]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['W'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['W'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(0,2.25,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('W',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax = axs[2]\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_bar'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "\n",
    "ax = axs[3]\n",
    "spacing= 1\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['I_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[p])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=.5)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('$\\hat{Z}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_TempWindExpansion.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1073bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind = [2,8,9]\n",
    "clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "terrain_names = ['Flat','Slope','Inv. Slope','Stairs','Inv. Stairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain_type_train = data_dict['terrain_train'].reshape(-1)\n",
    "terrain_type_test = data_dict['terrain_test']\n",
    "terrain_difficulty_train = data_dict['terrain_difficulty_train'].reshape(-1)\n",
    "terrain_difficulty_test = data_dict['terrain_difficulty_test'].reshape(-1)\n",
    "terrain_slope_train = data_dict['terrain_slope_train'].reshape(-1)\n",
    "terrain_slope_test = data_dict['terrain_slope_test'].reshape(-1)\n",
    "command_vel_train = data_dict['command_vel_train'].reshape(-1)\n",
    "command_vel_test = data_dict['command_vel_test'].reshape(-1)\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "terrain_robot_test = np.stack([terrain_type_test[robot_type_test==0].reshape(-1),terrain_type_test[robot_type_test==1].reshape(-1)])\n",
    "terrain_type_test = data_dict['terrain_test'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59828be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(4,4))\n",
    "seq_len=[100,200,500,1000,1500]\n",
    "ax = axs\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.random.uniform(y_ranges[ts,0],y_ranges[ts,1],timescales.shape[0]),\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None')\n",
    "\n",
    "# for n in range(len(timescales_M)):\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([np.diff(y_ranges)[0,0]/2 + q*dts for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize)\n",
    "ax.set_ylabel('T',fontsize=fontsize)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize)\n",
    "ax.set_xticks([0,10e0,10e1])\n",
    "ax.tick_params(axis='x', labelsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ts=-2\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([result_dict['R2_hat'],result_dict['W'],result_dict['R_bar']],axis=-1),result_dict['R2_hat'],result_dict['W'],result_dict['R_hat'],result_dict['I']]\n",
    "reg_variables = [result_dict['{}'.format(seq_len[ts])]['R2_hat'],result_dict['{}'.format(seq_len[ts])]['I']]\n",
    "labels = ['R2_hat','I']\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "command_vel_test = np.tile(robot_type_test,(9000,1)).transpose(1,0)\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "    # X_train = reg_vars[:tr_batch_size].reshape(-1,reg_vars.shape[-1])\n",
    "    # X_test = reg_vars[-batch_size:].reshape(-1,reg_vars.shape[-1])\n",
    "    # y_train = terrain[:tr_batch_size].reshape(-1)\n",
    "    # y_test = terrain[-batch_size:].reshape(-1)\n",
    "    # y_train = robot_type_test[:tr_batch_size].reshape(-1)\n",
    "    # y_test = robot_type_test[-batch_size:].reshape(-1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), robot_type_test.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    # neigh = RidgeClassifierCV()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    # y_pred = neigh.predict(X_test)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred_terrain = y_pred\n",
    "    else:\n",
    "        I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        \n",
    "state_compare = np.stack([I_pred,best_pred_terrain,terrain_type_test])\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded530d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_type_test.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# nani = terrain.shape[0]\n",
    "# reg_variables = [np.concatenate([R2_hat,W,R_bar],axis=-1),R2_hat,W,R_hat,I]\n",
    "# labels = ['all','R2_hat','W','R_hat','I']\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(I.reshape(-1,I.shape[-1]))\n",
    "reg_variables = [R2_hat,I] #,X_pca]\n",
    "labels = ['R2_hat','I'] #,'X_pca']\n",
    "label_type = ['terrain_difficulty_test','terrain_slope_test','command_vel_test']#,'robot_type_test']\n",
    "\n",
    "robot_type_test = data_dict['robot_type_test']\n",
    "robot_type_test=np.tile(robot_type_test,(9000,1)).transpose(1,0).reshape(-1)\n",
    "command_vel_test = np.tile(data_dict['command_vel_test'],(9000,1)).transpose(1,0)\n",
    "slope_cat = np.unique(terrain_slope_test,return_inverse=True)[1]\n",
    "difficulty_cat =  np.unique(terrain_difficulty_test,return_inverse=True)[1]\n",
    "vel_cat = np.unique(command_vel_test,return_inverse=True)[1]\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "class_results = {}\n",
    "state_compare_best = {}\n",
    "for m, test_labels in enumerate([difficulty_cat,slope_cat,vel_cat]): #,robot_type_test\n",
    "\n",
    "    for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), test_labels.reshape(-1), test_size=0.25, random_state=42)\n",
    "\n",
    "        neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        scores = neigh.score(X_test, y_test)\n",
    "        print(label_type[m],labels[k],scores)\n",
    "        if (labels[k] != 'I'):\n",
    "            y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = y_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "            max_acc = scores\n",
    "            best_reg_vars = reg_vars\n",
    "            best_label = labels[k]\n",
    "            best_pred = y_pred\n",
    "        else:\n",
    "            I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "            class_results['{}_{}_pred'.format(label_type[m],labels[k])] = I_pred\n",
    "            class_results['{}_{}_score'.format(label_type[m],labels[k])] = scores\n",
    "    state_compare_best['{}'.format(label_type[m])] = np.stack([I_pred,best_pred,test_labels],axis=0)\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, y_pred),cmap='viridis')\n",
    "# plt.colorbar()\n",
    "from sklearn import metrics\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bafde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_compare_data = {'state_compare_best':state_compare_best,'state_compare':state_compare,'class_results':class_results}\n",
    "ioh5.save(cfg.paths.log_dir/'state_compare.h5',state_compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e59514",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_compare_data = ioh5.load(cfg.paths.log_dir/'state_compare.h5')\n",
    "state_compare = state_compare_data['state_compare']\n",
    "state_compare_best = state_compare_data['state_compare_best']\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=2500; dt=1000\n",
    "fontsize=13\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.5))\n",
    "gs0 = gridspec.GridSpec(nrows=6,ncols=3, figure=fig, wspace=.1,hspace=.1)\n",
    "\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5, subplot_spec=gs0[:3,:],wspace=.1,hspace=.1)\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=5,subplot_spec=gs0[4:,:],wspace=.1,hspace=.2)\n",
    "axs = np.array([fig.add_subplot(gs00[:,:2]),\n",
    "                fig.add_subplot(gs00[:,2:4]),\n",
    "                fig.add_subplot(gs00[:,4:]),\n",
    "                fig.add_subplot(gs01[:,:2]),\n",
    "                fig.add_subplot(gs01[:,2:])])\n",
    "ax = axs[1]\n",
    "spacing= .5\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=1)\n",
    "ts=1\n",
    "hlines_x = []\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I_hat'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c='r',alpha=.75)\n",
    "    hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "# ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "# ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "# ax.set_yticks(hlines_x[1::2])\n",
    "ax.set_yticks([])\n",
    "# ax.set_yticklabels(np.arange(2,len(hlines_x)+1,2),fontsize=fontsize-2)\n",
    "ax.set_ylabel('observations',fontsize=fontsize)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "# ax.set_ylim(-1,(1/spacing)*len(hlines_x)+.5)\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "dts = 1/len(seq_len)\n",
    "# timescales_M = 1/np.abs(np.real(np.log(np.linalg.eigvals(M))))[1:]\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    # R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    R2_hat = result_dict['{}'.format(seq_len[ts])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R2_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for p in range(terrain_robot_test.shape[0]):\n",
    "        for state in range(len(np.unique(terrain_type_test))):\n",
    "            rhat2 = R_hat[np.where(terrain_robot_test[p]==state)[0],:]\n",
    "            Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "            Uhat_all.append(Uhat_0)\n",
    "            # eig_Ut = np.linalg.eigvals(Uhat_0).reshape(-1)\n",
    "            # timescales = np.real(np.log(eig_Ut))\n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "ax.set_xticks([0,10e0,10e1,10e2])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=-2)\n",
    "\n",
    "\n",
    "ax=axs[3]\n",
    "ax.bar(np.arange(len(f1_TiDHy)),f1_TiDHy,width=.4,color=clrs2[0])\n",
    "ax.bar(np.arange(len(f1_Inputs))+.4,f1_Inputs,width=.4,color=clrs2[4])\n",
    "ax.set_ylabel('F1 Score',fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(.2,len(f1_TiDHy)+.2))\n",
    "ax.set_xticklabels(['difficulty','slope','velocity'],fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(0,1.1,.25))\n",
    "ax.set_yticklabels(np.arange(0,1.1,.25),fontsize=fontsize-2)\n",
    "ax.legend(['TiDHy','Obs.'],fontsize=fontsize,frameon=False,loc='upper right',bbox_to_anchor=(1.025,1.25),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "\n",
    "ax = axs[-1]\n",
    "acc_obs = accuracy_score(terrain_type_test.reshape(-1),state_compare[0])\n",
    "acc_TiDHy = accuracy_score(terrain_type_test.reshape(-1),state_compare[1])\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(terrain_type_test)))\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+3000,3000))\n",
    "# ax.set_xticklabels(np.arange(0,dt+3000,3000),fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(.5,3,1))\n",
    "ax.set_yticklabels(['Obs. \\n {:02}%'.format(int(np.round(acc_obs*100))),'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize-2)\n",
    "ax.set_xlabel('timesteps',fontsize=fontsize)\n",
    "\n",
    "cbar = fig.colorbar(im,ax=axs[-1],aspect=10, pad=.05)\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(terrain_type_test)),1))\n",
    "cbar.set_ticklabels([''.join(terrain_names[n]) for n in range(len(terrain_names))],fontsize=fontsize-2)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2)\n",
    "\n",
    "fig.savefig(cfg.paths.fig_dir/'{}_Fig4.pdf'.format(nfig),dpi=300,transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f35936",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(5,5))\n",
    "ax = axs\n",
    "spacing= .5\n",
    "ts = -1\n",
    "hlines_x = []\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I_hat'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c='r',alpha=.75)\n",
    "    hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "for n in range(result_dict['{}'.format(seq_len[ts])]['I_hat'].shape[-1]):\n",
    "    trace = result_dict['{}'.format(seq_len[ts])]['I'][t:t+dt,n]\n",
    "    mean_centered_x = (trace - np.mean(trace,axis=0))/np.max(np.abs(trace))\n",
    "    ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[-1],alpha=.5)\n",
    "# ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "# ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('$\\hat{Z}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "\n",
    "\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-1,2*6))\n",
    "im = ax.pcolormesh(X,Y,np.tile(terrain_type_test[None,t:t+dt],(2*6+1,1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ee4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ssm = accuracy_score(terrain_type_test.reshape(-1),I_pred)\n",
    "acc_TiDHy = accuracy_score(terrain_type_test.reshape(-1),best_pred)\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(terrain_type_test)))\n",
    "fontsize=30\n",
    "t = 0; dt = 5000\n",
    "fig,axs = plt.subplots(1,1,figsize=(10,5),sharex=True)\n",
    "ax = axs\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "# ax.set_xticks(np.arange(0,dt+3000,3000))\n",
    "# ax.set_xticklabels(np.arange(0,dt+3000,3000),fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(.5,3,1))\n",
    "ax.set_yticklabels(['Obs. \\n {:02}%'.format(int(np.round(acc_ssm*100))),'TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize)\n",
    "ax.set_xlabel('Timesteps',fontsize=fontsize)\n",
    "\n",
    "# cbar = fig.colorbar(im,ax=axs,aspect=10, pad=-.2)\n",
    "cbar = add_colorbar(im,ax=axs,aspect=50, pad=.2)\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(terrain_type_test)),1))\n",
    "cbar.set_ticklabels([''.join(terrain_names[n]) for n in range(len(terrain_names))],fontsize=22)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Terrain_Classifer.pdf',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "f1_TiDHy = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][1],average='weighted') for key in state_compare_best.keys()]\n",
    "f1_Inputs = [metrics.f1_score(state_compare_best[key][-1],state_compare_best[key][0],average='weighted') for key in state_compare_best.keys()]\n",
    "clrs2 =['#0F4C5C','#F4D03F','#FF6B6B','#2E8B57','#87CEEB','#778899','#40E0D0','#FFDB58','#4169E1','#C2B280','#FFDAB9','#000080','#556B2F','#008080','#FFFDD0','#B7410E','#6A5ACD','#FFBF00','#FFD1DC','#800080','#98FF98','#FA8072','#E6E6FA','#36454F']\n",
    "cmap2 = ListedColormap(clrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17813b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(6,5))\n",
    "ax = axs\n",
    "ax.bar(np.arange(len(f1_TiDHy)),f1_TiDHy,width=.4,color=clrs2[0])\n",
    "ax.bar(np.arange(len(f1_Inputs))+.4,f1_Inputs,width=.4,color=clrs2[4])\n",
    "ax.set_ylabel('F1 Score',fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(.2,len(f1_TiDHy)+.2))\n",
    "ax.set_xticklabels(['Difficulty','Slope','Velocity'],fontsize=fontsize-2)\n",
    "ax.set_yticks(np.arange(0,1.1,.25))\n",
    "ax.set_yticklabels(np.arange(0,1.1,.25),fontsize=fontsize-2)\n",
    "ax.legend(['TiDHy','Obs.'],fontsize=fontsize,frameon=False,loc='upper right',bbox_to_anchor=(1.025,1.15),labelcolor='linecolor',handlelength=0,handleheight=0,ncols=2,columnspacing=.1)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'Terrain_Classifer_F1.pdf',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8eed0f",
   "metadata": {},
   "source": [
    "# Load CalMS21 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "fontsize=13\n",
    "\n",
    "clrs = np.array(['#1A237E','#7E57C2','#757575','#BDBDBD','#4CAF50','#FF9800','#795548','#FF4081','#00BCD4','#FF1744','#FFFFFF','#000000'])\n",
    "sys_clrs = ['#E3A19F','#E3BE53','#708090','#90CCA9','#B7522E','#B0E0E6','#A89AC2','#556B2F','#FF6F61','#87CEEB','#FFDAB9','#40E0D0']\n",
    "cmap_sys = ListedColormap(sys_clrs)\n",
    "clr_ind =[2,2,8,8,9,9]\n",
    "# clr2 = [sys_clrs[clr_ind[n]] for n in range(len(clr_ind))]\n",
    "clr_ind3 = [2,8,9]\n",
    "clr2b = [sys_clrs[clr_ind3[n]] for n in range(len(clr_ind3))]\n",
    "\n",
    "clrs_b = clrs[[0,1,2,9,4,6,7,8,11]]\n",
    "cmap = ListedColormap(clrs)\n",
    "cmap_b = ListedColormap(clrs_b)\n",
    "cmap\n",
    "cmap_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CalMS21'\n",
    "base_dir = Path('/data/users/eabe/hypernets/{}/DPC_{}/'.format(dataset,dataset))\n",
    "configs = sorted(list(base_dir.rglob('*config.yaml'))[::2])\n",
    "\n",
    "for n,conf in enumerate(configs):\n",
    "    print(n,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "default_model_config = '/home/eabe/Research/MyRepos/HyperNets/conf/dataset/model/default_model.yaml'\n",
    "params = OmegaConf.load(default_model_config)\n",
    "cfg_path =configs[n]# Path('/data/users/eabe/hypernets/SLDS/DPC_SLDS/TestingNoTempLearning/mix_dim=15/config.yaml')\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "for k in cfg.paths.keys():\n",
    "    cfg.paths[k] = Path(cfg.paths[k])\n",
    "    cfg.paths[k].mkdir(parents=True, exist_ok=True)\n",
    "params_curr = cfg.dataset.model\n",
    "cfg.dataset.model = OmegaConf.merge(params, params_curr)\n",
    "# cfg.dataset.model['L1_inf_r2'] = 0\n",
    "# cfg.dataset.model['L1_inf_r'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ce630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.dataset.train.add_basic_features=False\n",
    "cfg.dataset.train.normalize=False\n",
    "data_dict, cfg = load_dataset(cfg)\n",
    "# ##### Convert to float tensors #####\n",
    "# test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "test_inputs = torch.tensor(data_dict['inputs_test']).float()\n",
    "\n",
    "input_dim = test_inputs.shape[-1]\n",
    "test_inputs = test_inputs.reshape(-1, cfg.train.sequence_length, input_dim)\n",
    "cfg.model.input_dim = input_dim\n",
    "\n",
    "print(f'Our inputs have shape: {test_inputs.shape}')\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset,batch_size=test_inputs.shape[0],pin_memory=True)\n",
    "device = torch.device(\"cuda:{}\".format(cfg.train['gpu']) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb797386",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['inputs_train'].shape,data_dict['inputs_val'].shape, data_dict['inputs_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.batch_converge=False\n",
    "# cfg.model.Orth_alpha_spat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de418971",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load weights and params #####\n",
    "rerecord = False\n",
    "epoch = 1000\n",
    "model = TiDHy(cfg.model, device, show_progress=cfg.train.show_progress).to(device)\n",
    "# set_seed(42)\n",
    "load_path = cfg.paths.ckpt_dir/'best.pth.tar'\n",
    "# load_path = sorted(list(cfg.paths.ckpt_dir.rglob('last_{}.pth.tar'.format(epoch))))[0]\n",
    "data_load = torch.load(load_path,map_location=device)\n",
    "model.load_state_dict(data_load['state_dict'])\n",
    "model.to(device)\n",
    "print('Loaded from {} epoch'.format(data_load['epoch']))\n",
    "epoch = data_load['epoch']\n",
    "\n",
    "\n",
    "# result_dict = load_results(model, dataloader_test, data_dict, device, cfg, epoch, rerecord=rerecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b20afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500,1000]\n",
    "\n",
    "cfg = run_seq_len_model(seq_len, model, data_dict, device, epoch, cfg)\n",
    "\n",
    "data_dict, cfg = load_dataset(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = ioh5.load(cfg.paths.log_dir/'temp_results_{}_seq.h5'.format(epoch))\n",
    "seq_len = sorted([int(el) for el in list(result_dict.keys())])\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "t=0; dt = 2000\n",
    "# cmap2,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig, axs = plt.subplots(2,2,figsize=(20,12))\n",
    "axs = axs.flatten()\n",
    "fontsize=20\n",
    "ax = axs[0]\n",
    "spacing= .5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R2_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(-.5,.75,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('$\\hat{r}^h$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax.legend(['T={}'.format(Ln) for Ln in seq_len],frameon=False,fontsize=fontsize,loc='upper right',bbox_to_anchor=(1.15,1.05),labelcolor=clrs_b,handlelength=0,handleheight=0,ncols=1,columnspacing=.1)\n",
    "\n",
    "ax = axs[1]\n",
    "spacing= .5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['W'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['W'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "X,Y = np.meshgrid(np.arange(0,dt),np.arange(0,2.25,.25))\n",
    "# im = ax.pcolormesh(X,Y,np.tile(full_state_z[None,t:t+dt],(Y.shape[0],1)),cmap=cmap,norm=norm,alpha=.5,rasterized=True,zorder=-1)\n",
    "ax.set_ylabel('W',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "ax = axs[2]\n",
    "ax.set_ylabel('$\\hat{r}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['R_bar'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "\n",
    "ax = axs[3]\n",
    "spacing= 1.5\n",
    "for p in range(len(seq_len)):\n",
    "    hlines_x = []\n",
    "    for n in range(result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1]):\n",
    "        trace = result_dict['{}'.format(seq_len[p])]['I_hat'][t:t+dt,n]\n",
    "        mean_centered_x = trace - np.mean(trace,axis=0)\n",
    "        ax.plot(mean_centered_x + n/spacing, lw=1,zorder=1,c=clrs_b[p],alpha=.5)\n",
    "        hlines_x.append(np.mean(mean_centered_x + n/spacing,axis=0))\n",
    "ax.set_xticks(np.arange(0,t+dt+dt//4,(t+dt)//4))\n",
    "ax.set_xticklabels(np.arange(0,t+dt+dt//4,(t+dt)//4),fontsize=fontsize-2)\n",
    "ax.set_yticks(hlines_x)\n",
    "ax.set_yticklabels(np.arange(1,len(hlines_x)+1),fontsize=fontsize-2)\n",
    "ax.set_ylabel('$\\hat{Z}$',fontsize=fontsize)\n",
    "ax.set_xlabel('time',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'{}_TempWindExpansion.png'.format(nfig),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b18a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = -1\n",
    "pca = PCA(n_components=3)\n",
    "full_state_z = data_dict['annotations_test']\n",
    "# reg_all = np.concatenate([R2_hat,W,R_bar],axis=1)\n",
    "X_pca = pca.fit_transform(result_dict['{}'.format(seq_len[p])]['R_hat'])\n",
    "# X_pca = pca.fit_transform(R_hat)\n",
    "# X_pca = pca.fit_transform(R_bar)\n",
    "# X_pca = pca.fit_transform(reg_all)\n",
    "# X_pca = pca.fit_transform(W[:,np.sum(W,axis=0)!=0])\n",
    "# X_pca = pca.fit_transform(inputs_train)\n",
    "# X_pca = pca.fit_transform(b)\n",
    "# X_pca = pca.fit_transform(Vt.reshape(Vt.shape[0],-1))\n",
    "n = 0\n",
    "t = 0; dt = len(X_pca)\n",
    "fig = go.Figure()\n",
    "comps = X_pca[full_state_z==n][t:t+dt]\n",
    "# labels = full_state_z[full_state_z==n]\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=X_pca[:,0], y=X_pca[:,1], z=X_pca[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color='black',                # set color to an array/list of desired values\n",
    "            colorscale='Viridis',   # choose a colorscale\n",
    "            opacity=.1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=comps[:,0], y=comps[:,1], z=comps[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color='red',                # set color to an array/list of desired values\n",
    "            colorscale='Viridis',   # choose a colorscale\n",
    "            opacity=.01\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=500,\n",
    "    height=500,\n",
    "    autosize=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ab3cf",
   "metadata": {},
   "source": [
    "# Record Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88393e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = False\n",
    "p=2\n",
    "W = result_dict['{}'.format(seq_len[p])]['W'].reshape(-1,result_dict['{}'.format(seq_len[p])]['W'].shape[-1])\n",
    "# b = result_dict['{}'.format(seq_len[p])]['b'].reshape(-1,result_dict['{}'.format(seq_len[p])]['b'].shape[-1])\n",
    "I = result_dict['{}'.format(seq_len[p])]['I'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I'].shape[-1])\n",
    "Ihat = result_dict['{}'.format(seq_len[p])]['I_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['I_hat'].shape[-1])\n",
    "R_hat = result_dict['{}'.format(seq_len[p])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R_hat'].shape[-1])\n",
    "R_bar = result_dict['{}'.format(seq_len[p])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R_bar'].shape[-1])\n",
    "R2_hat = result_dict['{}'.format(seq_len[p])]['R2_hat'].reshape(-1,result_dict['{}'.format(seq_len[p])]['R2_hat'].shape[-1])\n",
    "Ut = result_dict['{}'.format(seq_len[p])]['Ut'].reshape(-1,result_dict['{}'.format(seq_len[p])]['Ut'].shape[-2],result_dict['{}'.format(seq_len[p])]['Ut'].shape[-1])\n",
    "##### Plot dynamic matrices #####\n",
    "if cfg.model.low_rank_temp:\n",
    "    Uk = torch.bmm(model.temporal.unsqueeze(-1),model.temporal.unsqueeze(1)).data.cpu().detach()\n",
    "else:\n",
    "    Uk = model.temporal.data.cpu().detach().reshape(model.mix_dim,model.r_dim,model.r_dim)\n",
    "full_state_z = data_dict['annotations_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25508806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "reg_variables = [np.concatenate([R2_hat,W,R_hat],axis=-1),R2_hat,W,R_hat,I]\n",
    "labels = ['all','R2_hat','W','R_hat','I']\n",
    "\n",
    "# reg_variables = W\n",
    "max_acc = 0\n",
    "tr_batch_size = 40\n",
    "batch_size = 10\n",
    "for k,reg_vars in enumerate(reg_variables):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), full_state_z.reshape(-1), test_size=0.25, random_state=42)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(reg_vars.reshape(-1,reg_vars.shape[-1]), terrain.reshape(-1), test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    scores = neigh.score(X_test, y_test)\n",
    "    print(labels[k],scores)\n",
    "    if (scores > max_acc) & (labels[k] != 'I'):\n",
    "        y_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "        max_acc = scores\n",
    "        best_reg_vars = reg_vars\n",
    "        best_label = labels[k]\n",
    "        best_pred = y_pred\n",
    "    else:\n",
    "        I_pred = neigh.predict(reg_vars.reshape(-1,reg_vars.shape[-1]))\n",
    "\n",
    "# plt.imshow(confusion_matrix(full_state_z, best_pred),cmap='viridis')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d15570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import size\n",
    "\n",
    "\n",
    "fontsize=13\n",
    "fps = 30\n",
    "fig = plt.figure(constrained_layout=True, figsize=(7.75,5.5))\n",
    "gs  = gridspec.GridSpec(nrows=8, ncols=4,hspace=10,wspace=.5) \n",
    "gs0 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[:3,:],  wspace=.5,hspace=.2)\n",
    "# gsb = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:3,2:3], wspace=.1,hspace=.5)\n",
    "\n",
    "# gsc = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[:5,3:], wspace=.5,hspace=.1)\n",
    "gs1 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[3:6,:], wspace=.2,hspace=.1)\n",
    "\n",
    "gs2 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[6:,:-1],   wspace=0, hspace=.10)\n",
    "\n",
    "##### plotting video frame ######\n",
    "t = 50; dt = 1\n",
    "skeleton = np.array([[0,1],[0,2],[2,3],[1,3],[3,4],[3,5],[4,6],[5,6]])\n",
    "ani1_x = data_dict['inputs_train'][t:t+dt,:7]\n",
    "ani1_y = data_dict['inputs_train'][t:t+dt,7:14]\n",
    "ani2_x = data_dict['inputs_train'][t:t+dt,14:21]\n",
    "ani2_y = data_dict['inputs_train'][t:t+dt,21:28]\n",
    "I1_x = Ihat[t:t+dt,:7]\n",
    "I1_y = Ihat[t:t+dt,7:14]\n",
    "I2_x = Ihat[t:t+dt,14:21]\n",
    "I2_y = Ihat[t:t+dt,21:28]\n",
    "labels = data_dict['annotations_train'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "axs = np.array([fig.add_subplot(gs0[:2])])\n",
    "ax = axs[0]\n",
    "for n in range(7):\n",
    "    ax.imshow(frames[t])\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=10,c='#ffa600ff')\n",
    "    im = ax.scatter(ani2_x[:,n],ani2_y[:,n],s=10,c='#4daf50ff')\n",
    "tt=0\n",
    "for n in range(len(skeleton)):\n",
    "    ax.plot(ani1_x[:,skeleton[n]].squeeze(),ani1_y[:,skeleton[n]].squeeze(),'#ffa600ff',lw=2,zorder=1)\n",
    "    ax.plot(ani2_x[:,skeleton[n]].squeeze(),ani2_y[:,skeleton[n]].squeeze(),'#4daf50ff',lw=2,zorder=1)\n",
    "ax.axis('off')\n",
    "\n",
    "###### Plotting Observations #####\n",
    "axs = np.array([fig.add_subplot(gs0[2])])\n",
    "ax = axs[0]\n",
    "spacing = .75; fontsize=13\n",
    "t = 0; dt = 10000\n",
    "hlines,hlines_Ihat= [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    mean_centered = mean_centered\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered + n/spacing,color='k', lw=1)\n",
    "    hlines.append(np.mean(mean_centered + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered_Ihat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=0)\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "ax.set_ylabel(\"observations\",fontsize=fontsize)\n",
    "ax.set_yticks([])\n",
    "\n",
    "##### Timescales #####\n",
    "axs = np.array([fig.add_subplot(gs0[3])])\n",
    "ax = axs[0]\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "        \n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))/fps\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xscale('symlog',linthresh=100)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=0)\n",
    "ax.set_xticks([10e0,10e1,10e2,10e3])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=0)\n",
    "ax.set_xlim([-1,5e4])\n",
    "\n",
    "###### PCA #####\n",
    "n = 0; m = 1\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(R_hat.reshape(-1,R_hat.shape[-1]))\n",
    "lst = list(itertools.combinations(np.arange(X_pca.shape[-1]), 2))\n",
    "unq = set(lst)\n",
    "axs = np.array([fig.add_subplot(gs1[n]) for n in range(4)])\n",
    "for state in range(4):\n",
    "    ax = axs[state]\n",
    "    comps = X_pca[full_state_z==state]\n",
    "    comps = comps/np.max(np.abs(comps),axis=0)\n",
    "    # comps = X_pca[(full_state_z==0) | (full_state_z==1) | (full_state_z==2)]\n",
    "    h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=50,range=[[-1,1],[-1,1]],density=True)\n",
    "    im = ax.imshow((h).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.set_title('{}'.format(list(data_dict['vocabulary'].keys())[state]),fontsize=fontsize,y=.95)\n",
    "    ax.set_xlabel('PC{}'.format(n),fontsize=fontsize)\n",
    "    if (state==0):\n",
    "        ax.set_ylabel('PC{}'.format(m),fontsize=fontsize)\n",
    "ax = axs[-1]\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10,shrink=.5)\n",
    "cbar.set_ticks([0,np.max(h)])\n",
    "cbar.set_ticklabels(['low','high'],rotation=90)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "\n",
    "##### Behaviroal State ID #####\n",
    "t = 80000; dt = 10000; \n",
    "acc_TiDHy=accuracy_score(best_pred, full_state_z)\n",
    "# acc_TiDHy=neigh.score(I_pred, full_state_z)\n",
    "state_compare = np.stack([y_pred,full_state_z],axis=0)\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "axs = np.array([fig.add_subplot(gs2[n]) for n in range(1)])\n",
    "ax = axs[0]\n",
    "im = ax.pcolormesh(state_compare[:,t:t+dt],cmap=cmap,norm=norm,alpha=.5,rasterized=True)\n",
    "ax.set_yticks(np.arange(.5,2,1))\n",
    "ax.set_yticklabels(['TiDHy \\n {:02}%'.format(int(np.round(acc_TiDHy*100))),'True'],fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(0,dt,60*fps))\n",
    "ax.set_xticklabels((np.arange(0,dt,60*fps)*1/fps).astype(int),fontsize=fontsize-2)\n",
    "ax.set_xlabel('Time (s)',fontsize=fontsize)\n",
    "\n",
    "# plt.tight_layout()\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=10, pad=.01)\n",
    "\n",
    "cbar.set_ticks(np.arange(0,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()),fontsize=fontsize)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "fig.savefig(cfg.paths.fig_dir / 'Fig5.pdf',dpi=300,transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = .75; fontsize=13\n",
    "t = 0; dt = 10000\n",
    "cmap,norm = map_discrete_cbar('tab10',len(np.unique(full_state_z)))\n",
    "fig,axs = plt.subplots(1,1,figsize=(2,3))\n",
    "\n",
    "ax = axs\n",
    "hlines,hlines_Ihat= [],[]\n",
    "for n in range(I.shape[-1]):\n",
    "    mean_centered = I[t:t+dt,n] - np.mean(I[t:t+dt,n],axis=0)\n",
    "    mean_centered = mean_centered\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered + n/spacing,color='k', lw=1)\n",
    "    hlines.append(np.mean(mean_centered + n/spacing,axis=0))\n",
    "    mean_centered_Ihat = Ihat[t:t+dt,n] - np.mean(Ihat[t:t+dt,n],axis=0)\n",
    "    ax.plot(1/fps*np.arange(0,dt,1),mean_centered_Ihat + n/spacing,ls='--',color='r', lw=1,zorder=2,label='Pred')\n",
    "    hlines_Ihat.append(np.mean(mean_centered_Ihat + n/spacing,axis=0))\n",
    "\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "ax.set_ylabel(\"Observations\",fontsize=fontsize)\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "# fig.savefig(cfg.paths.fig_dir/'CalMS21_Observations.pdf',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(2,3))\n",
    "ax = axs\n",
    "dts = 1/len(seq_len)\n",
    "y_ranges = np.arange(0,1,(dts*dts)).reshape(-1,len(seq_len))[:,(0,-1)]\n",
    "for ts in range(len(seq_len)):\n",
    "    R_bar = result_dict['{}'.format(seq_len[ts])]['R_bar'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_bar'].shape[-1])\n",
    "    R_hat = result_dict['{}'.format(seq_len[ts])]['R_hat'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['R_hat'].shape[-1])\n",
    "    I = result_dict['{}'.format(seq_len[ts])]['I'].reshape(-1,result_dict['{}'.format(seq_len[ts])]['I'].shape[-1])\n",
    "\n",
    "    # ##### Plottign Eigenvalues #####\n",
    "    Uhat_all = []\n",
    "    for state in range(len(np.unique(full_state_z))):\n",
    "        rhat2 = R_hat[np.where(full_state_z==state)[0],:]\n",
    "        Uhat_0 = np.linalg.inv(rhat2[:-1].T@rhat2[:-1])@rhat2[:-1].T@rhat2[1:]\n",
    "        Uhat_all.append(Uhat_0)\n",
    "        \n",
    "    Uhat_all = np.stack(Uhat_all)\n",
    "    evals_Uhat_all = np.linalg.eigvals(Uhat_all).reshape(-1)\n",
    "    timescales = np.real(np.log(evals_Uhat_all))\n",
    "\n",
    "    ax.scatter(x=1/np.abs(timescales),y=np.abs(evals_Uhat_all)+ts,\n",
    "            c=clrs_b[ts],alpha=.5,edgecolor='None',s=25)\n",
    "\n",
    "for ts in range(len(seq_len)):\n",
    "    ax.axhline(y=ts,c=clrs_b[ts],linestyle='--',zorder=-1)\n",
    "#     ax.axvline(x=timescales_M[n],c=sys_clrs[n])\n",
    "ax.set_xscale('symlog',linthresh=10)\n",
    "ax.set_yticks([1/2 + q for q in range(len(seq_len))])\n",
    "ax.set_yticklabels(seq_len,fontsize=fontsize-2)\n",
    "ax.set_ylabel('T',fontsize=fontsize,labelpad=-5)\n",
    "# ax.spines.left.set_visible(False)\n",
    "ax.set_xlabel('timescales',fontsize=fontsize,labelpad=-3)\n",
    "ax.set_xticks([0,10e0,10e1,10e2])\n",
    "ax.set_xlim([-1,5e2])\n",
    "ax.tick_params(axis='x', labelsize=fontsize-2, pad=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "X_pca = pca.fit_transform(R_hat)\n",
    "\n",
    "lst = list(itertools.combinations(np.arange(X_pca.shape[-1]), 2))\n",
    "unq = set(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d13de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_pca[:,1],X_pca[:,2],'.')\n",
    "comps = X_pca[full_state_z==1]\n",
    "plt.plot(comps[:,1],comps[:,2],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0; m = 1\n",
    "fontsize=10\n",
    "fig, axs = plt.subplots(1,5,figsize=(8,2))\n",
    "# axs = axs.flatten()\n",
    "ax = axs[0]\n",
    "comps = X_pca\n",
    "comps = comps/np.max(np.abs(comps),axis=0)\n",
    "h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=50,range=[[-1,1],[-1,1]])\n",
    "im = ax.imshow((h/X_pca.shape[0]).T,cmap=cmap,extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_title('All labels',fontsize=fontsize)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('PC{}'.format(n),fontsize=fontsize)\n",
    "ax.set_ylabel('PC{}'.format(m),fontsize=fontsize)\n",
    "for state in range(4):\n",
    "    ax = axs[state+1]\n",
    "    comps = X_pca[full_state_z==state]\n",
    "    comps = comps/np.max(np.abs(comps),axis=0)\n",
    "    # comps = X_pca[(full_state_z==0) | (full_state_z==1) | (full_state_z==2)]\n",
    "    h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=50,range=[[-1,1],[-1,1]])\n",
    "    im = ax.imshow((h/X_pca.shape[0]).T,cmap=cmap,extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('{}'.format(list(data_dict['vocabulary'].keys())[state]),fontsize=fontsize)\n",
    "    ax.set_xlabel('PC{}'.format(n),fontsize=fontsize)\n",
    "ax = axs[-1]\n",
    "cbar = fig.colorbar(im,ax=axs.flatten(),aspect=20,shrink=.75)\n",
    "# cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "# cbar.set_ticklabels([''.join(lst2[n]) for n in range(len(lst2))])\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.minorticks_off()\n",
    "cbar.ax.tick_params(width=1,which=\"major\")\n",
    "\n",
    "\n",
    "\n",
    "t = 80000; dt = 10000\n",
    "_,norm = map_discrete_cbar(cmap,len(np.unique(full_state_z)))\n",
    "fig,axs = plt.subplots(2,1,figsize=(10,5))\n",
    "ax = axs[0]\n",
    "im = ax.imshow(full_state_z[None,t:t+dt],cmap=cmap,aspect='auto',norm=norm,alpha=.5)\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Actual States')\n",
    "ax = axs[1]\n",
    "ax.imshow(y_pred[None,t:t+dt],cmap=cmap,aspect='auto',alpha=.5)\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Predicted States')\n",
    "\n",
    "plt.tight_layout()\n",
    "cbar = fig.colorbar(im, ax=axs.flat)\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_pca = pca.fit_transform(R2_hat)\n",
    "# X_pca = pca.fit_transform(R_hat)\n",
    "# X_pca = pca.fit_transform(W)\n",
    "X_pca = pca.fit_transform(R_bar)\n",
    "X_pca2 = pca.fit_transform(I)\n",
    "for n,m in unq:\n",
    "    if n!=m:\n",
    "        fig, axs = plt.subplots(2,5,figsize=(10,4))\n",
    "        # axs = axs.flatten()\n",
    "        ax = axs[0,0]\n",
    "        comps = X_pca\n",
    "        comps = comps/np.max(np.abs(comps),axis=0)\n",
    "        h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=100,range=[[-1,1],[-1,1]])\n",
    "        im = ax.imshow((h/X_pca.shape[0]).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "        ax.axis('square')\n",
    "        ax.set_title('All labels')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('PC{}'.format(n))\n",
    "        ax.set_ylabel('PC{}'.format(m))\n",
    "        ax = axs[1,0]\n",
    "        comps = X_pca2\n",
    "        comps = comps/np.max(np.abs(comps),axis=0)\n",
    "        h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=100,range=[[-1,1],[-1,1]])\n",
    "        im = ax.imshow((h/X_pca2.shape[0]).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "        ax.axis('square')\n",
    "        # ax.set_title('All labels')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('PC_I_{}'.format(n))\n",
    "        ax.set_ylabel('PC_I_{}'.format(m))\n",
    "        for state in range(4):\n",
    "            ax = axs[0,state+1]\n",
    "            comps = X_pca[full_state_z==state]\n",
    "            comps = comps/np.max(np.abs(comps),axis=0)\n",
    "            # comps = X_pca[(full_state_z==0) | (full_state_z==1) | (full_state_z==2)]\n",
    "            h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=100,range=[[-1,1],[-1,1]])\n",
    "            im = ax.imshow((h/X_pca.shape[0]).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "            ax.axis('square')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title('{}'.format(list(data_dict['vocabulary'].keys())[state]))\n",
    "            ax.set_xlabel('PC{}'.format(n))\n",
    "            ax = axs[1,state+1]\n",
    "            comps = X_pca2[full_state_z==state]\n",
    "            comps = comps/np.max(np.abs(comps),axis=0)\n",
    "            # comps = X_pca[(full_state_z==0) | (full_state_z==1) | (full_state_z==2)]\n",
    "            h,xedge,yedge = np.histogram2d(comps[:,n],comps[:,m],bins=100,range=[[-1,1],[-1,1]])\n",
    "            im = ax.imshow((h/X_pca2.shape[0]).T,cmap='turbo',extent=[xedge[0],xedge[-1],yedge[0],yedge[-1]],alpha=1)\n",
    "            ax.axis('square')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # ax.set_title('{}'.format(list(data_dict['vocabulary'].keys())[state]))\n",
    "            ax.set_xlabel('PC_I_{}'.format(n))\n",
    "        ax = axs[0,-1]\n",
    "        cax = ax.inset_axes([1.04, 0.2, 0.05, 0.6])\n",
    "        cbar = fig.colorbar(im,ax=axs[0,-1],cax=cax)\n",
    "        ax = axs[1,-1]\n",
    "        cax = ax.inset_axes([1.04, 0.2, 0.05, 0.6])\n",
    "        cbar = fig.colorbar(im,ax=axs[1,-1],cax=cax)\n",
    "        # fig.savefig(cfg.paths.fig_dir/'PCA_Rbar_I_{}_{}.png'.format(n,m),dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xticks = np.array([[['nose_x1', 'ear_left_x1', 'ear_right_x1', 'neck_x1', 'hip_left_x1', 'hip_right_x1', 'tail_base_x1'],\n",
    "          ['nose_y1', 'ear_left_y1', 'ear_right_y1', 'neck_y1', 'hip_left_y1', 'hip_right_y1', 'tail_base_y1']],\n",
    "          [['nose_x2', 'ear_left_x2', 'ear_right_x2', 'neck_x2', 'hip_left_x2', 'hip_right_x2', 'tail_base_x2'],\n",
    "          ['nose_y2', 'ear_left_y2', 'ear_right_y2', 'neck_y2', 'hip_left_y2', 'hip_right_y2', 'tail_base_y2']]]).reshape(-1)\n",
    "clustermap = sns.clustermap(spatial_decoder,figsize=(8,8),cmap='bwr')\n",
    "clustermap.tick_params(axis='both', which='major', labelsize=10, labelbottom = True, bottom=False, top = False, labeltop=False)\n",
    "clustermap.ax_heatmap.set_xlabel('Observations',fontsize=fontsize)\n",
    "clustermap.ax_heatmap.set_ylabel('Latent Variables',fontsize=fontsize)\n",
    "plt.savefig(cfg.paths.fig_dir / 'spatial_decoder_clustermap.png',dpi=300)\n",
    "# clustermap.ax_heatmap.set_xticklabels(xticks,rotation=90,fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['vocabulary']\n",
    "# np.unique(labels)\n",
    "# np.where(data_dict['annotations_test']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/data/users/eabe/hypernets/CalMS21/datasets/mouse001_task1_annotator1.mp4')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    " \n",
    "# Read until video is completed\n",
    "frames = []\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    frames.append(frame)   \n",
    "  # Break the loop\n",
    "  else: \n",
    "    break\n",
    " \n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dataset.train.normalize=False\n",
    "inputs_train,inputs_test,annotations_train,annotations_test,vocabulary,keypoint_names = load_CalMS21_dataset(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = np.array([[0,1],[0,2],[2,3],[1,3],[3,4],[3,5],[4,6],[5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 50; dt = 1\n",
    "fig, axs = plt.subplots(1,1,figsize=(12,5))\n",
    "ani1_x = inputs_train[t:t+dt,:7]\n",
    "ani1_y = inputs_train[t:t+dt,7:14]\n",
    "ani2_x = inputs_train[t:t+dt,14:21]\n",
    "ani2_y = inputs_train[t:t+dt,21:28]\n",
    "labels = data_dict['annotations_train'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "ax = axs\n",
    "for n in range(7):\n",
    "    ax.imshow(frames[50])\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c='r')\n",
    "    # ax.set_xlim([0,1])\n",
    "    # ax.set_ylim([0,1])\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c='b')\n",
    "    # ax.axis('square')\n",
    "for n in range(len(skeleton)):\n",
    "    ax.plot(ani1_x[:,skeleton[n]].squeeze(),ani1_y[:,skeleton[n]].squeeze(),'r',lw=2)\n",
    "    ax.plot(ani2_x[:,skeleton[n]].squeeze(),ani2_y[:,skeleton[n]].squeeze(),'b',lw=2)\n",
    "ax.axis('off')\n",
    "fig.savefig(cfg.paths.fig_dir / 'CalMS21_example.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 80000; dt = 1000\n",
    "I_hat = result_dict['I_hat'].reshape(-1,result_dict['I_hat'].shape[-1])\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "ani1_x = I_hat[t:t+dt,:7]\n",
    "ani1_y = I_hat[t:t+dt,7:14]\n",
    "ani2_x = I_hat[t:t+dt,14:21]\n",
    "ani2_y = I_hat[t:t+dt,21:28]\n",
    "labels = data_dict['annotations_test'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "for n in range(7):\n",
    "    ax = axs[0]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.axis('square')\n",
    "    ax = axs[1]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='left')\n",
    "cbar.set_ticks(np.arange(0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3632600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919311c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41097339",
   "metadata": {},
   "source": [
    "# Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41097339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import cv2\n",
    "import gc\n",
    "mpl.use('agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_plt_im(t, X_pca,data_dict,full_state_z):   #\n",
    "    trail = 5\n",
    "    labels = full_state_z[t-trail:t+1]\n",
    "    cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "    bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "    ax = axs[0]\n",
    "    im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "    im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1,1])\n",
    "    ax.set_ylim([-1,1])\n",
    "    cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "    cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "    cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "    ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "    ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "    ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "    ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "    ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "    labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "    for n in range(7):\n",
    "        ax = axs[1]\n",
    "        im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 1')\n",
    "        ax = axs[2]\n",
    "        ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "        ax.axis('square')\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    images = np.frombuffer(fig.canvas.tostring_rgb(),\n",
    "                        dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    plt.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = make_plt_im(t, X_pca,data_dict)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# full_state_z = data_dict['annotations_test']\n",
    "full_state_z = cluster_labels\n",
    "X_pca = pca.fit_transform(R_bar)\n",
    "\n",
    "t = 80000; dt = 5000\n",
    "trail = 5\n",
    "labels = full_state_z[t-trail:t+1]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[:,0],X_pca[:,1],s=1,c='k',alpha=.01)\n",
    "im = ax.scatter(X_pca[t-trail:t+1,0],X_pca[t-trail:t+1,1],s=3,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,len(np.unique(full_state_z)),1))\n",
    "cbar.set_ticklabels(np.arange(-1,len(np.unique(full_state_z))-1,1))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t-trail:t+1,:7]\n",
    "ani1_y = data_dict['inputs_test'][t-trail:t+1,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t-trail:t+1,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t-trail:t+1,21:28]\n",
    "labels = data_dict['annotations_test'][t-trail:t+1]\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=cmap,norm=norm,)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig(cfg.paths.fig_dir/'test.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(cluster_labels==2)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "##### initialize time points for animation and progressbar #####\n",
    "t = 0 ; dt = 10000\n",
    "full_state_z = cluster_labels\n",
    "state = 'I'\n",
    "time_range = np.arange(t,t+dt)#  np.where(cluster_labels==state)[0]#\n",
    "num_ticks = np.size(time_range)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(I)\n",
    "\n",
    "##### Put large arrays into shared memory #####\n",
    "X_pca_r = ray.put(X_pca)\n",
    "data_dict_r = ray.put(data_dict)\n",
    "full_state_z_r = ray.put(full_state_z)\n",
    "##### Loop over parameters appending process ids #####\n",
    "result_ids = []\n",
    "for t in time_range:\n",
    "    result_ids.append(make_plt_im.remote(t, X_pca_r, data_dict_r,full_state_z_r))\n",
    "\n",
    "##### pring progressbar and get results #####\n",
    "results_p = ray.get(result_ids)\n",
    "images = np.stack([results_p[i] for i in range(len(results_p))])\n",
    "\n",
    "##### Make video with opencv #####\n",
    "aniname = 'PCA_Evolve_{}.mp4'.format(state) \n",
    "\n",
    "\n",
    "vid_name = cfg.paths.fig_dir / aniname\n",
    "FPS = 30\n",
    "out = cv2.VideoWriter(vid_name.as_posix(), cv2.VideoWriter_fourcc(*'mp4v'), FPS, (images.shape[-2], images.shape[-3]))\n",
    "\n",
    "for fm in tqdm(range(images.shape[0])):\n",
    "    out.write(cv2.cvtColor(images[fm], cv2.COLOR_BGR2RGB))\n",
    "out.release()\n",
    "print('Making Animation {}: {}'.format(aniname, time.time()-start))\n",
    "del results_p, X_pca_r, data_dict_r, full_state_z_r\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 80000 ; dt = 5000\n",
    "labels = full_state_z[t:t+dt]\n",
    "cmap = plt.get_cmap('tab10',len(np.unique(full_state_z)))\n",
    "bounds = np.arange(len(np.unique(full_state_z))+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "ax = axs[0]\n",
    "im = ax.scatter(X_pca[t:t+dt,0],X_pca[t:t+dt,1],s=1,c=labels,cmap=cmap,norm=norm,alpha=1)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "cbar = fig.colorbar(im, ax=axs,location='right')\n",
    "cbar.set_ticks(np.arange(.5,4,1))\n",
    "cbar.set_ticklabels(list(data_dict['vocabulary'].keys()))\n",
    "ax.set_title('Rbar PCA comps')\n",
    "\n",
    "\n",
    "ani1_x = data_dict['inputs_test'][t:t+dt,:7]\n",
    "ani1_y = data_dict['inputs_test'][t:t+dt,7:14]\n",
    "ani2_x = data_dict['inputs_test'][t:t+dt,14:21]\n",
    "ani2_y = data_dict['inputs_test'][t:t+dt,21:28]\n",
    "labels = data_dict['annotations_test'][t:t+dt]\n",
    "clrs = plt.get_cmap('tab10',4)\n",
    "for n in range(7):\n",
    "    ax = axs[1]\n",
    "    im = ax.scatter(ani1_x[:,n],ani1_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.axis('square')\n",
    "    ax.set_title('Mouse 1')\n",
    "    ax = axs[2]\n",
    "    ax.scatter(ani2_x[:,n],ani2_y[:,n],s=1,c=labels,cmap=clrs)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_title('Mouse 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b35be4",
   "metadata": {},
   "source": [
    "# alternative loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result_dict(cfg,epoch,seq_len=None,reshape_list=None):\n",
    "    if reshape_list is None:\n",
    "        reshape_list = ['W','I','I_hat','I_bar','R_hat','R_bar','R2_hat']\n",
    "    result_dict_all = {}\n",
    "    if seq_len is None:\n",
    "        seq_len = [cfg.train.sequence_length]\n",
    "        file_name = 'results.h5'\n",
    "        result_dict = ioh5.load(cfg.paths.log_dir/file_name)   \n",
    "        for key in reshape_list:\n",
    "            if key in result_dict.keys():\n",
    "                result_dict[key] = result_dict[key].reshape(-1,result_dict[key].shape[-1])\n",
    "        result_dict_all['{}'.format(seq_len[0])] = result_dict     \n",
    "    elif isinstance(seq_len,int):\n",
    "        seq_len = [seq_len]\n",
    "        file_name = 'temp_results_{}_T{:04d}.h5'.format(epoch,seq_len[0])\n",
    "        result_dict[seq_len[0]] = ioh5.load(cfg.paths.log_dir/file_name)\n",
    "        for key in reshape_list:\n",
    "            if key in result_dict.keys():\n",
    "                result_dict[key] = result_dict[key].reshape(-1,result_dict[key].shape[-1])\n",
    "        result_dict_all[seq_len[n]] = result_dict     \n",
    "    else:\n",
    "        for n in range(len(seq_len)):\n",
    "            file_name = 'temp_results_{}_T{:04d}.h5'.format(epoch,seq_len[n])\n",
    "            result_dict = ioh5.load(cfg.paths.log_dir/file_name)        \n",
    "            for key in reshape_list:\n",
    "                if key in result_dict.keys():\n",
    "                    result_dict[key] = result_dict[key].reshape(-1,result_dict[key].shape[-1])\n",
    "            result_dict_all['{}'.format(seq_len[n])] = result_dict\n",
    "            print(seq_len[n])\n",
    "    return result_dict_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [100,200,500]\n",
    "# seq_len = [1000]\n",
    "# result_dict = load_result_dict(cfg,epoch)\n",
    "result_dict = load_result_dict(cfg,epoch,seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98bcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270073b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a35e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_params = cfg.dataset.ssm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98426f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_params['seed']=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lds_dict, data_dict = partial_superposition_SLDS(cfg,ssm_params,**ssm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90471f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "K = ssm_params['n_disc_states']\n",
    "\n",
    "z_timescale = ssm_params['z_timescale'][i]\n",
    "Ps = z_timescale * np.eye(K) + (1-z_timescale) #* npr.rand(K, K)\n",
    "Ps /= Ps.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fcb79",
   "metadata": {},
   "source": [
    "# Testing SLDS Eigvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_eigvalues,(set_eigvalues[0,0]),np.linalg.norm(set_eigvalues[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.real(set_eigvalues[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timescales = [-.033,-.25,-.1,-.5,-.02,-.75]\n",
    "\n",
    "freq = np.zeros(len(timescales)).astype(complex)\n",
    "\n",
    "freq.imag = np.sqrt(.985**2-np.exp(timescales)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b40411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timescales = [-.025,-.25,-.1,-.5,-.015,-.75]\n",
    "freq = np.zeros(len(timescales)).astype(complex)\n",
    "freq.imag = [.0075,.05,.025,.075,.01,.075]\n",
    "set_eigvalues = np.array([[np.exp(timescales) + 2*np.pi*np.array(freq)],[np.exp(timescales) - 2*np.pi*np.array(freq)]]).squeeze().T\n",
    "\n",
    "\n",
    "timescales_As = 1/np.abs(np.log(np.exp(timescales)))\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "for n in range(len(timescales_As)):\n",
    "    ax.axvline(x=timescales_As[n],c=sys_clrs[clr_ind[n]])\n",
    "\n",
    "ax.set_xscale('symlog',linthresh=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Fast to slow: 0-7 #####\n",
    "# set_eigvalues = np.array([\n",
    "#                     # np.array([-0.1995+0.930j,  -0.1995-0.930j]),  # 0\n",
    "#                     # np.array([0.894+0.322j,    0.894-0.322j]),    # 2\n",
    "                    \n",
    "#                     # np.array([0.949+0.17j,    0.949-0.17j]),    # 5\n",
    "#                     # np.array([0.999+0.017j,    0.999-0.017j]),  # 7\n",
    "                    \n",
    "#                     # np.array([0.5275+0.790j,   0.5275-0.790j]),   # 1\n",
    "#                     # np.array([-0.85+0.5j,     -0.85-0.5j]),     # 3\n",
    "                    \n",
    "#                     # np.array([0.943+0.114j,    0.943-0.114j]),    # 4\n",
    "#                     np.array([0.60653066+0.06283185j,    0.60653066-0.06283185j]),    # 4\n",
    "#                     # np.array([0.999+0.0005j,    0.999-0.0005j]),  # 8\n",
    "#                     np.array([0.90483742+0.06283185j,    0.90483742-0.06283185j]),  # 8\n",
    "                    \n",
    "#                     ])\n",
    "\n",
    "timescales = [-.033,-.25,-.1,-.5,-.02,-.75]\n",
    "freq = np.zeros(len(timescales)).astype(complex)\n",
    "freq.imag = np.sqrt(.985**2-np.exp(timescales)**2)\n",
    "# freq.imag = [0.16991344, 0.61120319, 0.40170791, 0.78244524,0.09825714,0.87004014]\n",
    "# timescales = [-.025,-.25,-.1,-.5,-.015,-.75]\n",
    "# freq = np.zeros(len(timescales)).astype(complex)\n",
    "# # freq.imag = [.0075,0.59486918,0.37638975,0.76975357,.01,0.85864419]\n",
    "# freq.imag = [0.16991344, 0.61120319, 0.40170791, 0.78244524,0.09825714,0.87004014]\n",
    "# freq.imag = [.0075,.05,.025,.075,.01,.075]\n",
    "set_eigvalues = np.array([[np.exp(timescales) + np.array(freq)],[np.exp(timescales) - np.array(freq)]]).squeeze().T\n",
    "\n",
    "evals_As=[]\n",
    "from scipy import signal\n",
    "for i in range(len(set_eigvalues)):\n",
    "    true_lds = ssm.LDS(ssm_params['obs_dim'],ssm_params['latent_dim'], emissions=\"gaussian\")\n",
    "    As_lds, bs_lds = [], []\n",
    "    neg = True if i % 2 == 0 else False\n",
    "    desired_eigenvalues = set_eigvalues[i]\n",
    "    # Create an empty A matrix with the same dimensions as desired_eigenvalues\n",
    "    A = np.zeros((len(desired_eigenvalues), len(desired_eigenvalues)))\n",
    "    # Create the system with the desired eigenvalues using pole placement\n",
    "    B = np.random.randn(len(desired_eigenvalues),len(desired_eigenvalues))  \n",
    "    # Calculate the state-space representation\n",
    "    poles = signal.place_poles(A, B,desired_eigenvalues)\n",
    "    A0 = A-B@poles.gain_matrix\n",
    "    print(A0)\n",
    "    b = np.random.randn(ssm_params['latent_dim']) #npr.randn(ssm_params['latent_dim'])\n",
    "    As_lds.append(A0)\n",
    "    bs_lds.append(b)\n",
    "    evals_A0 = np.linalg.eigvals(A0)\n",
    "    evals_As.append(evals_A0)\n",
    "    true_lds.dynamics.As = np.stack(As_lds)\n",
    "    true_lds.dynamics.bs = np.stack(bs_lds)\n",
    "    x, y = true_lds.sample(5000)\n",
    "    x = x/np.max(np.abs(x),axis=0,keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4)) \n",
    "    gs = gridspec.GridSpec(nrows=2, ncols=3) \n",
    "    gs0 = gridspec.GridSpecFromSubplotSpec(1, 3, subplot_spec=gs[0,:], wspace=.4,hspace=.5)\n",
    "    gs1 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs[1,:], wspace=.2,hspace=.5)\n",
    "    axs = np.array([fig.add_subplot(gs0[0]),fig.add_subplot(gs0[1]),fig.add_subplot(gs0[2])])\n",
    "    ax = axs[0]\n",
    "    ax.acorr(x[:,0],maxlags=100)\n",
    "\n",
    "    ax = axs[1]\n",
    "    q = plot_dynamics_2d(A0, \n",
    "                        bias_vector=b,\n",
    "                        mins=[-1,-1],#plot_x.min(axis=0),\n",
    "                        maxs=[1,1],#plot_x.max(axis=0),\n",
    "                        axis=ax)\n",
    "    ax.plot(x[:100,0],x[:100,1], '-k', lw=1)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.scatter(np.real(evals_A0.reshape(-1)),np.imag(evals_A0.reshape(-1)),25,c='g',zorder=5,label='data')\n",
    "    # im = ax.scatter(np.real(evals_Uhat1[:,n]),np.imag(evals_Uhat1[:,n]),10,c=np.arange(evals_Uhat1.shape[0]),zorder=5,label='pred')\n",
    "    # Move left y-axis and bottom x-axis to centre, passing through (0,0)\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "\n",
    "    # Eliminate upper and right axes\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "\n",
    "    # Show ticks in the left and lower axes only\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.set_xticks([-1,1])\n",
    "    ax.set_yticks([-1,1])\n",
    "    circ = plt.Circle((0, 0), radius=1, edgecolor='k', facecolor='None')\n",
    "    ax.add_patch(circ)\n",
    "    ax.axis('square')\n",
    "\n",
    "    axs = np.array([fig.add_subplot(gs1[0])])\n",
    "    ax = axs[0]\n",
    "    ax.plot(x[:500,:], lw=1)\n",
    "    # plt.suptitle('{}'.format(timescales[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.real(np.log(set_eigvalues)),np.log(set_eigvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31597d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "for n in range(len(timescales)):\n",
    "    ax.axvline(x=timescales[n],c=sys_clrs[clr_ind[n]])\n",
    "# ax = plot_hist(timescales,-1.75,.1,.1,ax,'TiDHy',clr=clrs_b[ts])\n",
    "ax.set_xscale('symlog',linthresh=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206657af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_superposition_SLDS(cfg, ssm_params, timescales=None, normalize=False, random_projection=False, partial_sup=False, full_sup=False, seed=0,saved_evals=True,**kwargs):\n",
    "    '''\n",
    "    Create a dataset of LDS systems with partially superimposed inputs.\n",
    "    ssm_params: dict\n",
    "    '''\n",
    "    ##### Fast to slow: 0-7 #####\n",
    "    # set_eigvalues = np.array([np.array([-0.1995+0.930j,  -0.1995-0.930j]),  # 0\n",
    "    #                           np.array([0.85+0.117j,     0.85-0.117j]),     # 3\n",
    "    #                           np.array([0.5275+0.790j,   0.5275-0.790j]),   # 1\n",
    "    #                           np.array([0.949+0.017j,    0.949-0.017j]),    # 5\n",
    "    #                           np.array([0.894+0.322j,    0.894-0.322j]),    # 2\n",
    "    #                           np.array([0.943+0.114j,    0.943-0.114j]),    # 4\n",
    "    #                           np.array([0.995+0.0017j,    0.995-0.0017j]),  # 7\n",
    "    #                           np.array([0.999+0.0005j,    0.999-0.0005j]),  # 8\n",
    "    #                         ])\n",
    "    # set_eigvalues = np.array([np.array([-0.1995+0.930j,  -0.1995-0.930j]),  # 0\n",
    "    #                     np.array([0.894+0.322j,    0.894-0.322j]),    # 2\n",
    "                        \n",
    "    #                     np.array([0.949+0.17j,    0.949-0.17j]),    # 5\n",
    "    #                     np.array([0.999+0.017j,    0.999-0.017j]),  # 7\n",
    "                        \n",
    "    #                     np.array([0.5275+0.790j,   0.5275-0.790j]),   # 1\n",
    "    #                     np.array([-0.85+0.5j,     -0.85-0.5j]),     # 3\n",
    "                        \n",
    "    #                     np.array([0.943+0.114j,    0.943-0.114j]),    # 4\n",
    "    #                     np.array([0.999+0.0005j,    0.999-0.0005j]),  # 8\n",
    "    #                     ])\n",
    "    timescales = [-.025,-.25,-.1,-.5,-.01,-.75]\n",
    "    freq = np.zeros(len(timescales)).astype(complex)\n",
    "    # freq.imag = [.0075,.05,.025,.075,.01,.075]\n",
    "    freq.imag = [0.16991344, 0.61120319, 0.40170791, 0.78244524,.01,0.87004014]\n",
    "    set_eigvalues = np.array([[np.exp(timescales) + 2*np.pi*np.array(freq)],[np.exp(timescales) - 2*np.pi*np.array(freq)]]).squeeze().T\n",
    "\n",
    "    # Set the random seed\n",
    "    # filename = 'SLDS_N{}_zD{}_xD{}_yD{}_seed{}.h5'.format(ssm_params['Nlds'],ssm_params['n_disc_states'],ssm_params['latent_dim'],ssm_params['obs_dim'],ssm_params['seed'])\n",
    "    # if (cfg.paths.data_dir/filename).exists():\n",
    "    #     data_dict = ioh5.load(cfg.paths.data_dir / filename)\n",
    "    #     inputs_train = data_dict['inputs_train']\n",
    "    #     inputs_val = data_dict['inputs_val']\n",
    "    #     inputs_test = data_dict['inputs_test']\n",
    "    #     states_z = data_dict['states_z']\n",
    "    #     states_z_test = data_dict['states_z_test']\n",
    "    #     states_z_val = data_dict['states_z_val']\n",
    "    #     states_x = data_dict['states_x']\n",
    "    #     states_x_val = data_dict['states_x_val']\n",
    "    #     states_x_test = data_dict['states_x_test']\n",
    "    #     As = data_dict['As']\n",
    "    #     bs = data_dict['bs']\n",
    "    #     lds_dict={}\n",
    "    # else:\n",
    "    npr.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # Make an LDS with somewhat interesting dynamics parameters\n",
    "    lds_dict = {}\n",
    "    states_z, states_z_test, states_z_val =  [], [], []\n",
    "    states_x, states_x_test, states_x_val = [], [], []\n",
    "    inputs_train, inputs_test, inputs_val = [], [], []\n",
    "    As,bs = [],[]\n",
    "    p = 0\n",
    "\n",
    "    for i in range(ssm_params['Nlds']):\n",
    "        true_lds = ssm.SLDS(ssm_params['obs_dim'], ssm_params['n_disc_states'],ssm_params['latent_dim'], emissions=\"gaussian\")\n",
    "        As_lds, bs_lds = [], []\n",
    "        for n in range(ssm_params['n_disc_states']):\n",
    "            if saved_evals:\n",
    "                A0 = create_dyn_mat(set_eigvalues[p])\n",
    "            else:\n",
    "                a = 1 if i % 2 == 0 else -1\n",
    "                neg = True if i % 2 == 0 else False\n",
    "                theta = np.pi/2 * npr.rand()/(timescales[p])\n",
    "                A0 = .95 * random_rotation(ssm_params['latent_dim'], theta=theta, neg=neg)\n",
    "            b = npr.uniform(low=-1,high=1,size=ssm_params['latent_dim']) #np.zeros(ssm_params['latent_dim']) #\n",
    "            As_lds.append(A0)\n",
    "            bs_lds.append(b)\n",
    "            p+=1\n",
    "        # Set the dynamics matrix (A) and the \n",
    "        true_lds.dynamics.As = np.stack(As_lds)\n",
    "        true_lds.dynamics.bs = np.stack(bs_lds)\n",
    "        K = ssm_params['n_disc_states']\n",
    "        z_timescale = ssm_params['z_timescale'][i]\n",
    "        Ps = z_timescale * np.eye(K) + (1-z_timescale) #* npr.rand(K, K)\n",
    "        Ps /= Ps.sum(axis=1, keepdims=True)\n",
    "        print(Ps)\n",
    "        true_lds.transitions.log_Ps = np.log(Ps)\n",
    "        p_states_x = 0\n",
    "        z_count = 0\n",
    "        while (np.min(p_states_x) < 0.45):\n",
    "            z, x, y = true_lds.sample(ssm_params['time_bins_train'])\n",
    "            _,counts = np.unique(z,return_counts=True)\n",
    "            p_states_x = counts/np.sum(counts)\n",
    "            z_count += 1\n",
    "            print('Train',z_count, p_states_x)\n",
    "        p_states_x = 0\n",
    "        z_count = 0\n",
    "        while (np.min(p_states_x) < 0.45):\n",
    "            z2, x2, y2 = true_lds.sample(ssm_params['time_bins_test'])\n",
    "            _,counts = np.unique(z2,return_counts=True)\n",
    "            p_states_x = counts/np.sum(counts)\n",
    "            z_count += 1\n",
    "            print('Val',z_count, p_states_x)\n",
    "        p_states_x = 0\n",
    "        z_count = 0\n",
    "        while (np.min(p_states_x) < 0.45):\n",
    "            z3, x3, y3 = true_lds.sample(ssm_params['time_bins_test'])\n",
    "            _,counts = np.unique(z3,return_counts=True)\n",
    "            p_states_x = counts/np.sum(counts)\n",
    "            z_count += 1\n",
    "            print('Test',z_count, p_states_x)\n",
    "            \n",
    "        if normalize:\n",
    "            x = x/np.max(np.abs(x),axis=0,keepdims=True)\n",
    "            x2 = x2/np.max(np.abs(x2),axis=0,keepdims=True)\n",
    "            x3 = x3/np.max(np.abs(x3),axis=0,keepdims=True)\n",
    "        states_x.append(x)\n",
    "        states_x_test.append(x2)\n",
    "        states_z.append(z)\n",
    "        states_z_test.append(z2)\n",
    "        inputs_train.append(y)\n",
    "        inputs_test.append(y2)\n",
    "        inputs_val.append(y3)\n",
    "        states_x_val.append(x3)\n",
    "        states_z_val.append(z3)\n",
    "        As.append(true_lds.dynamics.As)\n",
    "        bs.append(true_lds.dynamics.bs)\n",
    "\n",
    "        lds_dict['{}'.format(i)] = true_lds\n",
    "\n",
    "\n",
    "    states_x = np.stack(states_x)\n",
    "    states_x_test = np.stack(states_x_test)\n",
    "    states_x_val = np.stack(states_x_val)\n",
    "    states_z = np.stack(states_z)\n",
    "    states_z_test = np.stack(states_z_test)\n",
    "    states_z_val = np.stack(states_z_val)\n",
    "    inputs_train = np.stack(inputs_train,axis=1)\n",
    "    inputs_test = np.stack(inputs_test,axis=1)\n",
    "    inputs_val = np.stack(inputs_val,axis=1)\n",
    "    data_dict={'states_z':states_z,'states_z_test':states_z_test, 'states_z_val':states_z_val,\n",
    "                'states_x':states_x,'states_x_test':states_x_test, 'states_x_val':states_x_val,\n",
    "                'inputs_train':inputs_train,'inputs_test':inputs_test, 'inputs_val':inputs_val,\n",
    "                'As':As,'bs':bs,'timescales':timescales}\n",
    "    # ioh5.save(cfg.paths.data_dir/filename,data_dict)\n",
    "    ##### Partial Superposition #####\n",
    "    if partial_sup:\n",
    "        set_seed(42)\n",
    "        ##### Partial Superposition #####\n",
    "        assert ssm_params['overlap'] <= ssm_params['obs_dim'], 'Overlap must be less than or equal to the number of observations'\n",
    "        inputs_train = inputs_train.reshape(ssm_params['time_bins_train'],-1)\n",
    "        inputs_test = inputs_test.reshape(ssm_params['time_bins_test'],-1)\n",
    "        inputs_val = inputs_val.reshape(ssm_params['time_bins_test'],-1)\n",
    "        ##### Constructing the mixing matrix #####\n",
    "        sup_inds = np.stack([np.arange(n,ssm_params['obs_dim']*ssm_params['Nlds'],ssm_params['obs_dim']) for n in range(ssm_params['obs_dim'])])\n",
    "        MixMat = np.zeros((ssm_params['obs_dim']*ssm_params['Nlds'],ssm_params['obs_dim']*ssm_params['Nlds']))\n",
    "        for n in range(sup_inds.shape[0]):\n",
    "            if n < ssm_params['overlap']:\n",
    "                MixMat[sup_inds[n],n] = 1\n",
    "            else:\n",
    "                MixMat[sup_inds[n],sup_inds[n]] = 1\n",
    "        idx = np.argwhere(np.all(MixMat[..., :] == 0, axis=0))\n",
    "        MixMat = np.delete(MixMat, idx, axis=1)\n",
    "        inputs_train = inputs_train@MixMat\n",
    "        inputs_test = inputs_test@MixMat\n",
    "        inputs_val = inputs_val@MixMat\n",
    "    elif full_sup:\n",
    "        inputs_train = np.sum(inputs_train,axis=(-1,-2))[:,np.newaxis]\n",
    "        inputs_test = np.sum(inputs_test,axis=(-1,-2))[:,np.newaxis]\n",
    "        inputs_val = np.sum(inputs_val,axis=(-1,-2))[:,np.newaxis]\n",
    "    else:\n",
    "        inputs_train = inputs_train.reshape(inputs_train.shape[0],-1)\n",
    "        inputs_test = inputs_test.reshape(inputs_test.shape[0],-1)\n",
    "        inputs_val = inputs_val.reshape(inputs_val.shape[0],-1)\n",
    "    if normalize:\n",
    "        inputs_train = inputs_train/np.max(np.abs(inputs_train),axis=0,keepdims=True)\n",
    "        inputs_test = inputs_test/np.max(np.abs(inputs_test),axis=0,keepdims=True)\n",
    "        inputs_val = inputs_val/np.max(np.abs(inputs_val),axis=0,keepdims=True)\n",
    "    if random_projection:\n",
    "        RandMat = np.random.randn(inputs_train.shape[-1],ssm_params['rand_dim'])\n",
    "        inputs_train = inputs_train@RandMat\n",
    "        inputs_test = inputs_test@RandMat\n",
    "        inputs_val = inputs_val@RandMat\n",
    "        if normalize:\n",
    "            inputs_train = inputs_train/np.max(np.abs(inputs_train),axis=0,keepdims=True)\n",
    "            inputs_test = inputs_test/np.max(np.abs(inputs_test),axis=0,keepdims=True)\n",
    "            inputs_val = inputs_val/np.max(np.abs(inputs_val),axis=0,keepdims=True)\n",
    "    if cfg.delay.delay_embed: \n",
    "        inputs_train = delay_embedding(inputs_train,cfg.delay.delay_tau,skipt=cfg.delay.skipt)\n",
    "        inputs_test = delay_embedding(inputs_test,cfg.delay.delay_tau,skipt=cfg.delay.skipt)\n",
    "        inputs_val = delay_embedding(inputs_val,cfg.delay.delay_tau,skipt=cfg.delay.skipt)\n",
    "\n",
    "    data_dict={'states_z':states_z,'states_z_test':states_z_test, 'states_z_val':states_z_val,\n",
    "                'states_x':states_x,'states_x_test':states_x_test, 'states_x_val':states_x_val,\n",
    "                'inputs_train':inputs_train,'inputs_test':inputs_test, 'inputs_val':inputs_val,\n",
    "                'As':As,'bs':bs,'timescales':data_dict['timescales']}\n",
    "    print('x Timescales: {}, z Timescales: {}'.format(data_dict['timescales'],ssm_params['z_timescale']))\n",
    "    return lds_dict,data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ffde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ssm_params = cfg.dataset.ssm_params\n",
    "ssm_params.seed = 11\n",
    "ssm_params.time_bins_train = 50000\n",
    "ssm_params.time_bins_test = 10000\n",
    "\n",
    "ssm_params.partial_sup = False\n",
    "ssm_params.z_timescale = [.99,.99,.99]\n",
    "lds_dict, data_dict = partial_superposition_SLDS(cfg,ssm_params,**ssm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['inputs_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c50152",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(itertools.product([0, 1], repeat=3))\n",
    "lst2 = list(itertools.product(['F', 'S'], repeat=3))\n",
    "full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,data_dict['states_z'])] = n\n",
    "    \n",
    "print(np.unique(full_state_z,return_counts=True))\n",
    "\n",
    "full_state_z = np.zeros(ssm_params['time_bins_test'],dtype=int)\n",
    "# full_state_z = np.zeros(ssm_params['time_bins_train'],dtype=int)\n",
    "for n in range(len(lst)):\n",
    "    full_state_z[np.apply_along_axis(lambda x: np.all(x == lst[n]),0,data_dict['states_z_test'])] = n\n",
    "print(np.unique(full_state_z,return_counts=True))\n",
    "_,zcounts=np.unique(full_state_z,return_counts=True)\n",
    "zcounts/np.sum(zcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97cab74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aba067",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0;dt=1000\n",
    "fig,ax = plt.subplots()\n",
    "cmap2,norm = map_discrete_cbar(cmap_b,len(np.unique(full_state_z)))\n",
    "\n",
    "im = ax.pcolormesh(full_state_z[None,t:t+dt],cmap=cmap2,norm=norm,alpha=.5,rasterized=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac28235",
   "metadata": {},
   "source": [
    "# Neural Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a46a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80321a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "zfish_data = sio.loadmat('/data/users/eabe/hypernets/Zfish/datasets/zArchonData20210305.mat')\n",
    "zfish_data = zfish_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traces = np.stack([zfish_data['trace'][0,n] for n in range(zfish_data.shape[-1])]).squeeze()\n",
    "traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fe89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_x,nan_y = np.where(np.isnan(traces))\n",
    "traces2 = np.delete(traces,nan_x,axis=0)\n",
    "unique_nan = np.unique(nan_x)\n",
    "fish_N = [np.where(zfish_data['fishId']==n)[1] for n in range(1,11)]\n",
    "fish_N = np.array([np.array([fish_N[n][i] for i  in range(len(fish_N[n])) if i not in unique_nan]) for n in range(len(fish_N))],dtype=object)\n",
    "# np.unique(x,return_counts=True),np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef6c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.hstack(fish_N)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(traces2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_id = np.hstack([(n)*np.ones(fish_N[n].shape) for n in range(len(fish_N))])\n",
    "X_pca.shape,fish_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72983067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:,0],X_pca[:,1],s=1,c=fish_id,alpha=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6650f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(traces[500:510,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "zfish_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e4682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf01163",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.hstack(fish_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187610b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(zfish_data['fishId']==1,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "zfish_data['trial'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3735ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = Path('/data/users/eabe/hypernets/WormDataset/datasets/')\n",
    "data_list = sorted(list(data_dir.glob('*.json')))\n",
    "\n",
    "\n",
    "data_worm_all = {}\n",
    "for n in range(len(data_list)):\n",
    "    with open(data_list[n]) as f:\n",
    "        worm_data = json.load(f)\n",
    "    data_worm_all[data_list[n].stem] = worm_data\n",
    "    \n",
    "trace_array_all = []\n",
    "for key1 in data_worm_all.keys():\n",
    "    trace_array_all.append(np.stack(data_worm_all[key1]['trace_array']))\n",
    "# trace_array_all = np.stack(trace_array_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = list(data_worm_all.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = []\n",
    "for key1 in data_worm_all.keys():\n",
    "    temp_label = []\n",
    "    for key2 in data_worm_all[key1]['labeled'].keys():\n",
    "        if (data_worm_all[key1]['labeled'][key2]['label'] in high_labels):\n",
    "            temp_label.append(data_worm_all[key1]['labeled'][key2]['label'])\n",
    "    print(len(temp_label))\n",
    "    labels_all.append(temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(set.intersection(*map(set, labels_all)))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74907ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([data_worm_all[key1]['labeled'][key2]['label'] for key1 in data_worm_all.keys() for key2 in data_worm_all[key1]['labeled'].keys() ])\n",
    "unique_labels, ids, counts = np.unique(labels,return_inverse=True,return_counts=True)\n",
    "\n",
    "low_labels = unique_labels[counts<35]\n",
    "high_labels = unique_labels[counts>35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_labels,high_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19874c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90bf31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(labels,return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(unique_labels[1]==labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels.shape, ids.shape,counts.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = pd.factorize(labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(labels==common_labels[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365c2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
